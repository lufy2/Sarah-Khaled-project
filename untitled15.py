# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10QLTxgPyPIIuJupXLivY8zvG9UmL06aJ
"""

# استدعاء المكتبات المطلوبة
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import metrics
from sklearn import svm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import gdown

# تحميل البيانات من Google Drive
url = 'https://drive.google.com/uc?id=1lBrQwn-j_wyoIjBNDlkSYHuSkLMcfStA'
output = 'insurance.csv'
gdown.download(url, output, quiet=False)

# قراءة ملف البيانات
insurance = pd.read_csv('insurance.csv')

# استكشاف البيانات
print(insurance.head(10))
print(insurance.info())
print(insurance.describe())

# تحويل الأعمدة النصية إلى رقمية
le = LabelEncoder()
categorical_columns = ['sex', 'smoker', 'region']
for col in categorical_columns:
    insurance[col] = le.fit_transform(insurance[col])

# استخراج معلومات إضافية من الأعمدة إذا لزم الأمر
# في هذه البيانات لا يوجد عمود تاريخ، لذا سنتخطى هذه الخطوة

# تصور البيانات
sns.pairplot(insurance, hue='smoker', markers='+')
plt.show()

sns.violinplot(y='smoker', x='charges', data=insurance, inner='quartile')
plt.show()
sns.violinplot(y='region', x='charges', data=insurance, inner='quartile')
plt.show()
sns.violinplot(y='sex', x='charges', data=insurance, inner='quartile')
plt.show()

# إعداد البيانات للنمذجة
# سنقوم باستبعاد أي أعمدة غير ضرورية (في هذه الحالة لا يوجد)
X = insurance.drop(['charges'], axis=1)
y = insurance['charges']

# تقسيم البيانات
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=5)

# تقييس البيانات
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# تدريب نموذج الانحدار الخطي
lr = LinearRegression()
lr.fit(X_train, y_train)

# تدريب نموذج Random Forest
rf = RandomForestRegressor(n_estimators=100, random_state=5)
rf.fit(X_train, y_train)

# تقييم النماذج
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    print(f'MSE: {mean_squared_error(y_test, y_pred)}')
    print(f'R2 Score: {r2_score(y_test, y_pred)}')
    print('-'*50)

print('Linear Regression Performance:')
evaluate_model(lr, X_test, y_test)

print('Random Forest Performance:')
evaluate_model(rf, X_test, y_test)

# تدريب نموذج SVM (لاحظ أن SVM قد يكون بطيئًا مع البيانات الكبيرة)
clf = svm.SVR()
clf.fit(X_train, y_train)

print('SVM Performance:')
evaluate_model(clf, X_test, y_test)

# تحليل أهمية الميزات في نموذج Random Forest
feature_importances = pd.DataFrame({
    'Feature': X.columns,
    'Importance': rf.feature_importances_
}).sort_values('Importance', ascending=False)

print('Feature Importances:')
print(feature_importances)

# تصور أهمية الميزات
plt.figure(figsize=(10,6))
sns.barplot(x='Importance', y='Feature', data=feature_importances)
plt.title('Feature Importances in Random Forest Model')
plt.show()

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}
rf = RandomForestRegressor(random_state=5)
grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='r2')
grid_search.fit(X_train, y_train)
print("Best Parameters:", grid_search.best_params_)

from xgboost import XGBRegressor
xgb = XGBRegressor()
xgb.fit(X_train, y_train)
evaluate_model(xgb, X_test, y_test)

plt.figure(figsize=(12, 6))
sns.boxplot(x='smoker', y='charges', data=insurance)
plt.title("توزيع التكاليف للمدخنين vs غير المدخنين")
plt.show()

from sklearn.model_selection import GridSearchCV

# Define parameter grid for Random Forest
param_grid = {
    'n_estimators': [50, 100, 200],      # Number of trees
    'max_depth': [None, 10, 20],         # Tree depth
    'min_samples_split': [2, 5, 10]      # Minimum samples to split
}

rf = RandomForestRegressor(random_state=5)
grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='r2')
grid_search.fit(X_train, y_train)

print("Optimal Parameters:", grid_search.best_params_)

from xgboost import XGBRegressor

xgb = XGBRegressor()
xgb.fit(X_train, y_train)
evaluate_model(xgb, X_test, y_test)  # Custom evaluation function

plt.figure(figsize=(12, 6))
sns.boxplot(x='smoker', y='charges', data=insurance)
plt.title("Cost Distribution: Smokers vs Non-Smokers")
plt.show()

sns.scatterplot(x='bmi', y='charges', hue='smoker', data=insurance)
plt.title("BMI vs Insurance Costs by Smoking Status")
plt.show()

# Age categorization
insurance['age_group'] = pd.cut(insurance['age'],
                               bins=[0, 30, 50, 100],
                               labels=['young', 'middle_aged', 'senior'])

# Log transformation for skewed target variable
insurance['log_charges'] = np.log1p(insurance['charges'])

import shap

explainer = shap.TreeExplainer(rf)
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values, X_test, feature_names=X.columns)

from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor()
rf.fit(X_train, y_train)  # تأكد من أنك دربت النموذج على بيانات التدريب

import shap

import shap
import pandas as pd
import numpy as np

print(rf.estimators_)

residuals = y_test - rf.predict(X_test)
outliers = residuals[abs(residuals) > residuals.std() * 3]
print("Identified Outliers:", outliers)

import joblib
joblib.dump(rf, 'best_insurance_model.pkl')

import joblib
joblib.dump(rf, 'best_insurance_model.csv')

import streamlit as st

st.title("Insurance Cost Predictor")
age = st.slider("Age", 18, 100)
bmi = st.number_input("BMI")
smoker = st.selectbox("Smoker?", ["Yes", "No"])
# ... (Add prediction logic using loaded model)

pip install streamlit

!pip install streamlit

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.inspection import permutation_importance

# Load and prepare data
def load_data():
    data = pd.read_csv('insurance.csv')

    # Separate features and target
    X = data.drop('charges', axis=1)
    y = data['charges']

    return X, y

# Preprocessing setup
def get_preprocessor():
    numeric_features = ['age', 'bmi', 'children']
    numeric_transformer = Pipeline(steps=[
        ('scaler', StandardScaler())
    ])

    categorical_features = ['sex', 'smoker', 'region']
    categorical_transformer = Pipeline(steps=[
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_features),
            ('cat', categorical_transformer, categorical_features)
        ])

    return preprocessor

# Model definitions
def get_models():
    models = {
        "Artificial Neural Network": MLPRegressor(
            hidden_layer_sizes=(100, 50),
            activation='relu',
            solver='adam',
            max_iter=1000,
            random_state=42
        ),
        "Support Vector Machine": SVR(
            kernel='rbf',
            C=100,
            gamma=0.1,
            epsilon=0.1
        ),
        "K-Nearest Neighbors": KNeighborsRegressor(
            n_neighbors=5,
            weights='distance'
        ),
        "Linear Regression": LinearRegression(),
        "Random Forest": RandomForestRegressor(
            n_estimators=200,
            max_depth=10,
            min_samples_split=5,
            random_state=42
        ),
        "Decision Tree": DecisionTreeRegressor(
            max_depth=5,
            min_samples_split=5,
            random_state=42
        )
    }

    # Note: Naïve Bayes is removed as it's not suitable for regression tasks
    return models

# Evaluation function
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)

    metrics = {
        'MSE': mean_squared_error(y_test, y_pred),
        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),
        'MAE': mean_absolute_error(y_test, y_pred),
        'R2': r2_score(y_test, y_pred)
    }

    return metrics

# Main execution
def main():
    # Load data
    X, y = load_data()

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # Get preprocessor and models
    preprocessor = get_preprocessor()
    models = get_models()

    # Results storage
    results = []

    # Train and evaluate each model
    for name, model in models.items():
        try:
            # Create pipeline
            pipeline = Pipeline(steps=[
                ('preprocessor', preprocessor),
                ('model', model)
            ])

            # Train model
            pipeline.fit(X_train, y_train)

            # Evaluate model
            metrics = evaluate_model(pipeline, X_test, y_test)

            # Cross-validation
            cv_scores = cross_val_score(
                pipeline, X_train, y_train,
                cv=5, scoring='r2'
            )

            # Store results
            results.append({
                'Model': name,
                'MSE': metrics['MSE'],
                'RMSE': metrics['RMSE'],
                'MAE': metrics['MAE'],
                'R2': metrics['R2'],
                'CV R2 Mean': cv_scores.mean(),
                'CV R2 Std': cv_scores.std()
            })

            # Feature importance for tree-based models
            if name in ['Random Forest', 'Decision Tree']:
                fitted_model = pipeline.named_steps['model']
                if hasattr(fitted_model, 'feature_importances_'):
                    # Get feature names after one-hot encoding
                    categorical_features = pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(['sex', 'smoker', 'region'])
                    feature_names = ['age', 'bmi', 'children'] + list(categorical_features)

                    # Plot feature importance
                    plt.figure(figsize=(10, 6))
                    importances = pd.Series(
                        fitted_model.feature_importances_,
                        index=feature_names
                    ).sort_values(ascending=False)
                    sns.barplot(x=importances.values, y=importances.index)
                    plt.title(f'{name} Feature Importance')
                    plt.tight_layout()
                    plt.show()

            print(f"{name} completed successfully.")

        except Exception as e:
            print(f"Error with {name}: {str(e)}")

    # Display results
    results_df = pd.DataFrame(results)
    print("\nModel Performance Comparison:")
    print(results_df.sort_values(by='R2', ascending=False))

    # Visual comparison
    plt.figure(figsize=(12, 6))
    sns.barplot(
        data=results_df.sort_values(by='R2', ascending=False),
        x='R2',
        y='Model',
        palette='viridis'
    )
    plt.title('Model Performance Comparison (R2 Score)')
    plt.xlim(0, 1)
    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    main()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import r2_score, mean_squared_error

# تحميل البيانات
data = pd.read_csv('insurance.csv')
X = data.drop('charges', axis=1)
y = data['charges']

# تقسيم البيانات
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# المعالجة المسبقة
numeric_features = ['age', 'bmi', 'children']
categorical_features = ['sex', 'smoker', 'region']

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(), categorical_features)
    ])

# إنشاء النموذج
model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', LinearRegression())
])

# التدريب والتقييم
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(f"R2 Score: {r2_score(y_test, y_pred):.4f}")
print(f"MSE: {mean_squared_error(y_test, y_pred):.2f}")

from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import GridSearchCV

# المعالجة المسبقة (نفسها كما في الانحدار الخطي)

# البحث عن أفضل المعايير
param_grid = {
    'regressor__max_depth': [3, 5, 7, 10],
    'regressor__min_samples_split': [2, 5, 10]
}

model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', DecisionTreeRegressor(random_state=42))
])

grid_search = GridSearchCV(model, param_grid, cv=5, scoring='r2')
grid_search.fit(X_train, y_train)

# أفضل نموذج
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

print(f"Best Parameters: {grid_search.best_params_}")
print(f"R2 Score: {r2_score(y_test, y_pred):.4f}")

from sklearn.ensemble import RandomForestRegressor

param_grid = {
    'regressor__n_estimators': [100, 200],
    'regressor__max_depth': [None, 10, 20],
    'regressor__min_samples_split': [2, 5]
}

model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', RandomForestRegressor(random_state=42))
])

grid_search = GridSearchCV(model, param_grid, cv=5, scoring='r2', n_jobs=-1)
grid_search.fit(X_train, y_train)

best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

print(f"Best Parameters: {grid_search.best_params_}")
print(f"R2 Score: {r2_score(y_test, y_pred):.4f}")

from sklearn.neighbors import KNeighborsRegressor
from sklearn.pipeline import Pipeline

param_grid = {
    'regressor__n_neighbors': [3, 5, 7, 9],
    'regressor__weights': ['uniform', 'distance']
}

model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', KNeighborsRegressor())
])

grid_search = GridSearchCV(model, param_grid, cv=5, scoring='r2')
grid_search.fit(X_train, y_train)

best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

print(f"Best Parameters: {grid_search.best_params_}")
print(f"R2 Score: {r2_score(y_test, y_pred):.4f}")

from sklearn.svm import SVR

param_grid = {
    'regressor__C': [0.1, 1, 10, 100],
    'regressor__gamma': ['scale', 'auto'],
    'regressor__kernel': ['rbf', 'linear']
}

model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', SVR())
])

grid_search = GridSearchCV(model, param_grid, cv=3, scoring='r2', n_jobs=-1)
grid_search.fit(X_train, y_train)

best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

print(f"Best Parameters: {grid_search.best_params_}")
print(f"R2 Score: {r2_score(y_test, y_pred):.4f}")

from sklearn.neural_network import MLPRegressor

param_grid = {
    'regressor__hidden_layer_sizes': [(50,), (100,), (100, 50)],
    'regressor__activation': ['relu', 'tanh'],
    'regressor__alpha': [0.0001, 0.001]
}

model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', MLPRegressor(max_iter=1000, random_state=42))
])

grid_search = GridSearchCV(model, param_grid, cv=3, scoring='r2', n_jobs=-1)
grid_search.fit(X_train, y_train)

best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

print(f"Best Parameters: {grid_search.best_params_}")
print(f"R2 Score: {r2_score(y_test, y_pred):.4f}")

from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import PowerTransformer

# نستخدم PowerTransformer لأن البيانات غير طبيعية
preprocessor = ColumnTransformer(
    transformers=[
        ('num', PowerTransformer(), numeric_features),
        ('cat', OneHotEncoder(), categorical_features)
    ])

model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', GaussianNB())
])

# Naïve Bayes عادةً يستخدم للتصنيف، لذا سنحول المشكلة إلى تصنيف
# عن طريق تقسيم التكاليف إلى فئات
y_class = pd.cut(y, bins=5, labels=False)
X_train, X_test, y_train, y_test = train_test_split(X, y_class, test_size=0.2, random_state=42)

model.fit(X_train, y_train)
y_pred = model.predict(X_test)

from sklearn.metrics import accuracy_score
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")

import joblib
joblib.dump(best_model, 'best_model.pkl')

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import RobustScaler, PowerTransformer, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import SelectKBest, f_regression

# تحميل البيانات
data = pd.read_csv('insurance.csv')
X = data.drop('charges', axis=1)
y = data['charges']

# تقسيم البيانات مع الحفاظ على التوزيع
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.15, random_state=42, stratify=pd.qcut(y, q=5)
)

# معالجة متقدمة للبيانات
numeric_features = ['age', 'bmi', 'children']
categorical_features = ['sex', 'smoker', 'region']

preprocessor = ColumnTransformer(
    transformers=[
        ('num', Pipeline([
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', RobustScaler()),
            ('transformer', PowerTransformer())
        ]), numeric_features),
        ('cat', Pipeline([
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('encoder', OneHotEncoder(handle_unknown='ignore'))
        ]), categorical_features)
    ])

# إضافة اختيار الميزات
full_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('feature_selection', SelectKBest(f_regression, k='all')),
])

from sklearn.linear_model import Ridge, Lasso, ElasticNet

linear_params = {
    'model__alpha': np.logspace(-4, 4, 20),
    'model__fit_intercept': [True, False]
}

models = {
    'Ridge': Ridge(),
    'Lasso': Lasso(),
    'ElasticNet': ElasticNet()
}

for name, model in models.items():
    pipe = Pipeline([
        ('preprocessing', full_pipeline),
        ('model', model)
    ])

    search = GridSearchCV(pipe, linear_params, cv=10, scoring='r2', n_jobs=-1)
    search.fit(X_train, y_train)

    print(f"{name} Best Score: {search.best_score_:.4f}")
    print(f"{name} Best Params: {search.best_params_}")

from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor

rf_params = {
    'model__n_estimators': [100, 200, 500],
    'model__max_features': ['auto', 'sqrt', 0.3],
    'model__bootstrap': [True, False],
    'model__min_samples_leaf': [1, 2, 4]
}

for model in [RandomForestRegressor, ExtraTreesRegressor]:
    pipe = Pipeline([
        ('preprocessing', full_pipeline),
        ('model', model(random_state=42))
    ])

    search = RandomizedSearchCV(pipe, rf_params, cv=5, n_iter=50,
                              scoring='r2', n_jobs=-1, random_state=42)
    search.fit(X_train, y_train)

    print(f"{model.__name__} Best R2: {search.best_score_:.4f}")

from xgboost import XGBRegressor
from scipy.stats import uniform, randint

xgb_params = {
    'model__n_estimators': randint(100, 1000),
    'model__learning_rate': uniform(0.01, 0.3),
    'model__max_depth': randint(3, 10),
    'model__subsample': uniform(0.6, 0.4),
    'model__colsample_bytree': uniform(0.6, 0.4)
}

xgb_pipe = Pipeline([
    ('preprocessing', full_pipeline),
    ('model', XGBRegressor(random_state=42, objective='reg:squarederror'))
])

search = RandomizedSearchCV(xgb_pipe, xgb_params, cv=5, n_iter=100,
                          scoring='r2', n_jobs=-1, random_state=42)
search.fit(X_train, y_train)

print(f"XGBoost Best R2: {search.best_score_:.4f}")

from sklearn.neural_network import MLPRegressor
from scipy.stats import loguniform

nn_params = {
    'model__hidden_layer_sizes': [(100,), (100,50), (200,100)],
    'model__activation': ['relu', 'tanh'],
    'model__alpha': loguniform(1e-5, 1e-1),
    'model__learning_rate_init': loguniform(1e-4, 1e-2)
}

nn_pipe = Pipeline([
    ('preprocessing', full_pipeline),
    ('model', MLPRegressor(max_iter=2000, early_stopping=True, random_state=42))
])

search = RandomizedSearchCV(nn_pipe, nn_params, cv=5, n_iter=50,
                          scoring='r2', n_jobs=-1, random_state=42)
search.fit(X_train, y_train)

print(f"Neural Network Best R2: {search.best_score_:.4f}")

# استيراد جميع المكتبات المطلوبة
from sklearn.pipeline import Pipeline
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import loguniform
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
import numpy as np

# 1. تعريف full_pipeline (يجب تعديله حسب بياناتك)
# لنفترض أن لديك الميزات التالية (يجب استبدالها بميزاتك الفعلية)
numeric_features = ['age', 'bmi']  # الميزات العددية
categorical_features = ['smoker']  # الميزات الفئوية

# خطوات المعالجة للميزات العددية
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())])

# خطوات المعالجة للميزات الفئوية
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])

# تجميع كل المعالجات في full_pipeline
full_pipeline = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)])

# 2. معلمات النموذج للبحث العشوائي
nn_params = {
    'model__hidden_layer_sizes': [(100,), (100,50), (200,100)],
    'model__activation': ['relu', 'tanh'],
    'model__alpha': loguniform(1e-5, 1e-1),
    'model__learning_rate_init': loguniform(1e-4, 1e-2)
}

# 3. إنشاء خط الأنابيب الرئيسي
nn_pipe = Pipeline([
    ('preprocessing', full_pipeline),
    ('model', MLPRegressor(max_iter=2000,
                          early_stopping=True,
                          random_state=42,
                          verbose=True))  # إضافة verbose لمتابعة التدريب
])

# 4. إعداد البحث العشوائي
search = RandomizedSearchCV(nn_pipe,
                          nn_params,
                          cv=5,
                          n_iter=20,  # تقليل عدد التكرارات لتسريع العملية
                          scoring='r2',
                          n_jobs=-1,
                          random_state=42,
                          verbose=2)  # إضافة verbose لمتابعة التقدم

# 5. تدريب النموذج (تأكد من تعريف X_train و y_train مسبقاً)
search.fit(X_train, y_train)

# 6. عرض النتائج
print(f"Neural Network Best R2 Score: {search.best_score_:.4f}")
print("Best Parameters Found:")
for param, value in search.best_params_.items():
    print(f"{param}: {value}")

# تحميل البيانات
data = pd.read_csv('insurance.csv')
X = data.drop('charges', axis=1)
y = data['charges']

# تقسيم البيانات
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ... rest of the code ...

# تحميل البيانات
import pandas as pd # Add this import
from sklearn.model_selection import train_test_split

data = pd.read_csv('insurance.csv')
X = data.drop('charges', axis=1)
y = data['charges']

# تقسيم البيانات
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ... rest of the code ...

nn_params = {
    'model__hidden_layer_sizes': [(100,), (100,50), (200,100)],
    'model__activation': ['relu', 'tanh'],
    'model__alpha': loguniform(1e-5, 1e-1),
    'model__learning_rate_init': loguniform(1e-4, 1e-2)
}

from scipy.stats import loguniform

nn_params = {
    'model__hidden_layer_sizes': [(100,), (100,50), (200,100)],
    'model__activation': ['relu', 'tanh'],
    'model__alpha': loguniform(1e-5, 1e-1),
    'model__learning_rate_init': loguniform(1e-4, 1e-2)
}

from sklearn.model_selection import cross_val_score

best_models = {
    'XGBoost': search.best_estimator_,
    # أضف هنا أفضل النماذج الأخرى
}

for name, model in best_models.items():
    scores = cross_val_score(model, X_train, y_train,
                           cv=10, scoring='r2', n_jobs=-1)
    print(f"{name} Cross-validated R2: {np.mean(scores):.4f} (±{np.std(scores):.4f})")

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print(f"{name} Test R2: {r2_score(y_test, y_pred):.4f}")
    print(f"{name} Test RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}")

from sklearn.model_selection import cross_val_score
from sklearn.metrics import r2_score, mean_squared_error
import numpy as np

best_models = {
    'XGBoost': search.best_estimator_,
    # أضف هنا أفضل النماذج الأخرى
}

for name, model in best_models.items():
    scores = cross_val_score(model, X_train, y_train,
                           cv=10, scoring='r2', n_jobs=-1)
    print(f"{name} Cross-validated R2: {np.mean(scores):.4f} (±{np.std(scores):.4f})")

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print(f"{name} Test R2: {r2_score(y_test, y_pred):.4f}")
    print(f"{name} Test RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}")

import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import numpy as np
import pandas as pd
from matplotlib.gridspec import GridSpec
from sklearn.inspection import permutation_importance
from sklearn.pipeline import Pipeline
from sklearn.neural_network import MLPRegressor # Assuming this is used somewhere above
from sklearn.ensemble import RandomForestRegressor # Assuming this is used somewhere above

plt.style.use('seaborn-v0_8-darkgrid')

# Make sure X_test, y_test, X_train, y_train are defined and populated
# And that rf_model and mlp_model pipelines are defined and trained

# دالة مساعدة لحساب أهمية الميزات
def calculate_feature_importance(model, X_test, y_test, feature_names):
    try:
        # للنماذج التي لديها feature_importances_
        if hasattr(model.named_steps['model'], 'feature_importances_'):
            # Get feature names after preprocessing for tree-based models
            preprocessor = model.named_steps['preprocessor']
            num_features = X_test.select_dtypes(include=np.number).columns.tolist()
            cat_features = X_test.select_dtypes(exclude=np.number).columns.tolist()
            cat_encoder = preprocessor.named_transformers_['cat'].named_steps['onehot']
            cat_feature_names = cat_encoder.get_feature_names_out(cat_features)
            processed_feature_names = num_features + list(cat_feature_names)

            return pd.Series(model.named_steps['model'].feature_importances_,
                           index=processed_feature_names).sort_values(ascending=False)
        else:
            # استخدام Permutation Importance للنماذج الأخرى
            # Permutation importance works directly on the pipeline and original X_test
            result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)
            # Use original feature names for permutation importance
            return pd.Series(result.importances_mean,
                           index=feature_names).sort_values(ascending=False)
    except Exception as e:
        print(f"Error calculating feature importance: {e}")
        return None

# 1. Enhanced Model Comparison Visualization
def plot_model_comparison(results):
    fig = plt.figure(figsize=(20, 12))
    gs = GridSpec(2, 2, figure=fig)

    # Model Performance Radar Chart
    ax1 = fig.add_subplot(gs[0, 0], polar=True)
    metrics = ['R2', 'RMSE', 'MAE', 'Training Time']
    angles = np.linspace(0, 2*np.pi, len(metrics), endpoint=False)
    angles = np.concatenate((angles, [angles[0]]))

    for model_name, model_results in results.items():
        # Ensure required metrics are present
        if all(m in model_results for m in ['R2', 'RMSE', 'MAE', 'Time']):
            # Handle potential zero or negative values for inversions in radar chart
            rmse_inv = 1 / model_results['RMSE'] if model_results['RMSE'] > 0 else 0
            mae_inv = 1 / model_results['MAE'] if model_results['MAE'] > 0 else 0
            time_inv = 1 / model_results['Time'] if model_results['Time'] > 0 else 0

            values = [model_results['R2'],
                     rmse_inv,
                     mae_inv,
                     time_inv]
            values = np.concatenate((values, [values[0]]))
            ax1.plot(angles, values, 'o-', linewidth=2, label=model_name)
            ax1.fill(angles, values, alpha=0.25)
        else:
             print(f"Warning: Missing metrics for {model_name}. Skipping radar plot for this model.")


    ax1.set_thetagrids(angles[:-1] * 180/np.pi, metrics)
    ax1.set_title('Model Performance Radar Chart', pad=20)
    ax1.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))

    # Feature Importance Comparison
    ax2 = fig.add_subplot(gs[0, 1])
    importance_df = pd.DataFrame()
    for model_name, model_results in results.items():
        if 'Feature Importance' in model_results and model_results['Feature Importance'] is not None:
            temp_df = pd.DataFrame({
                'Feature': model_results['Feature Importance'].index,
                'Importance': model_results['Feature Importance'].values,
                'Model': model_name
            })
            importance_df = pd.concat([importance_df, temp_df])

    if not importance_df.empty:
        sns.barplot(data=importance_df, x='Importance', y='Feature', hue='Model', ax=ax2)
        ax2.set_title('Comparative Feature Importance')
        ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    else:
        ax2.text(0.5, 0.5, 'No Feature Importance Available',
                ha='center', va='center', fontsize=12)
        ax2.set_title('Feature Importance Not Available')

    # Residual Analysis
    ax3 = fig.add_subplot(gs[1, :])
    for model_name, model_results in results.items():
         if 'Residuals' in model_results:
            sns.kdeplot(model_results['Residuals'], label=model_name, ax=ax3, linewidth=2)
         else:
             print(f"Warning: Missing residuals for {model_name}. Skipping residual plot for this model.")

    ax3.axvline(0, color='black', linestyle='--')
    ax3.set_title('Residual Distributions Comparison')
    ax3.legend()

    plt.tight_layout()
    plt.savefig('advanced_model_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()

# 2. Interactive Prediction Analysis (Requires model to be a pipeline)
def plot_interactive_predictions(model_pipeline, X_test, y_test):
    preds = model_pipeline.predict(X_test)
    # Ensure X_test retains original column names for plotting
    plot_df = pd.DataFrame({
        'Actual': y_test,
        'Predicted': preds,
        'Residual': y_test - preds,
        'BMI': X_test['bmi'],
        'Smoker': X_test['smoker'].map({0: 'No', 1: 'Yes'}) if 'smoker' in X_test.columns else 'N/A', # Handle cases where 'smoker' isn't in X_test if preprocessing changes it
        'Age': X_test['age']
    })

    fig = px.scatter_3d(plot_df, x='Actual', y='Predicted', z='Residual',
                        color='Smoker', size='BMI', hover_data=['Age'],
                        title='Interactive Prediction Analysis',
                        labels={'Actual': 'Actual Charges ($)',
                                'Predicted': 'Predicted Charges ($)',
                                'Residual': 'Residual ($)'})

    fig.update_layout(scene=dict(
        xaxis_title='Actual Charges',
        yaxis_title='Predicted Charges',
        zaxis_title='Residual Error'),
        margin=dict(l=0, r=0, b=0, t=30))

    fig.write_html("interactive_predictions.html")
    fig.show()

# 3. Enhanced Partial Dependence Plots (Requires model to be a pipeline)
def plot_advanced_pdp(model_pipeline, X_train, features):
    fig, ax = plt.subplots(2, 2, figsize=(18, 12))
    ax = ax.ravel()

    # Handle potential issues with non-numeric features in kdeplot
    train_for_kde = X_train.copy()
    for f in features:
        if f in train_for_kde.columns and not pd.api.types.is_numeric_dtype(train_for_kde[f]):
             print(f"Warning: Feature '{f}' is not numeric. Skipping distribution plot for this feature.")
             features.remove(f) # Remove non-numeric features from kdeplot list

    for i, feature in enumerate(features[:4]): # Ensure we don't go out of bounds if features were removed
        PartialDependenceDisplay.from_estimator(
            model_pipeline, X_train, [feature],
            ax=ax[i], line_kw={"linewidth": 3},
            pd_line_kw={"color": "red", "linestyle": "--"},
            kind='both',  # Shows both average and individual predictions
            ice_lines_kw={"alpha": 0.3, "linewidth": 0.5},
            random_state=42
        )
        ax[i].set_title(f'PDP for {feature}', fontsize=14)
        ax[i].set_ylabel('Partial Dependence', fontsize=12)
        ax[i].set_xlabel(feature, fontsize=12)

        # Add distribution plot only for numeric features
        if feature in train_for_kde.columns and pd.api.types.is_numeric_dtype(train_for_kde[feature]):
            twin_ax = ax[i].twinx()
            sns.kdeplot(train_for_kde[feature], ax=twin_ax, color='green', alpha=0.3)
            twin_ax.set_ylabel('Feature Distribution', color='green')

    plt.suptitle('Advanced Partial Dependence Analysis', y=1.02, fontsize=16)
    plt.tight_layout()
    plt.savefig('advanced_pdp.png', dpi=300, bbox_inches='tight')
    plt.show()

# 4. SHAP Summary with Enhanced Visualization (Requires tree-based model and pipeline)
def plot_enhanced_shap(model_pipeline, X_train):
    import shap
    # Check if the model step is a tree-based model
    if not hasattr(model_pipeline.named_steps['model'], 'feature_importances_'):
        print("SHAP summary plot requires a tree-based model (e.g., RandomForestRegressor, XGBRegressor). Skipping.")
        return

    explainer = shap.TreeExplainer(model_pipeline.named_steps['model'])
    processed_data = model_pipeline.named_steps['preprocessor'].transform(X_train)

    # Handle potential different return types of shap_values (list for multi-output)
    shap_values = explainer.shap_values(processed_data)
    if isinstance(shap_values, list):
        # Assuming regression, take the first element if it's a list of arrays
        if len(shap_values) > 0 and isinstance(shap_values[0], np.ndarray):
             shap_values = shap_values[0]
        else:
             print("Unexpected SHAP values format. Skipping SHAP plot.")
             return


    # Get feature names after preprocessing
    # This logic needs to correctly map processed data columns back to names
    # This can be complex, often requires getting feature names from the OneHotEncoder
    preprocessor = model_pipeline.named_steps['preprocessor']
    num_features = X_train.select_dtypes(include=np.number).columns.tolist()
    cat_features = X_train.select_dtypes(exclude=np.number).columns.tolist()

    # Get OHE feature names from the preprocessor step
    cat_transformer_step = preprocessor.named_transformers_['cat']
    if 'onehot' in cat_transformer_step.named_steps:
         cat_encoder = cat_transformer_step.named_steps['onehot']
         # Ensure fit was called on the preprocessor/pipeline before this
         try:
            cat_feature_names = cat_encoder.get_feature_names_out(cat_features)
         except Exception as e:
             print(f"Could not get feature names from OneHotEncoder: {e}. Using generic names.")
             cat_feature_names = [f'cat_{i}' for i in range(processed_data.shape[1] - len(num_features))]
    else:
        print("OneHotEncoder not found in 'cat' transformer. Using generic names for categorical features.")
        cat_feature_names = [f'cat_{i}' for i in range(processed_data.shape[1] - len(num_features))]


    processed_feature_names = num_features + list(cat_feature_names)

    # Ensure shap_values and processed_data have the same number of features
    if shap_values.shape[1] != processed_data.shape[1]:
        print(f"Mismatch between SHAP values features ({shap_values.shape[1]}) and processed data features ({processed_data.shape[1]}). Skipping SHAP plot.")
        return
    if processed_data.shape[1] != len(processed_feature_names):
         print(f"Mismatch between processed data features ({processed_data.shape[1]}) and generated feature names ({len(processed_feature_names)}). Skipping SHAP plot.")
         return


    plt.figure(figsize=(16, 8))
    shap.summary_plot(shap_values, processed_data, feature_names=processed_feature_names,
                     plot_type="dot", show=False, alpha=0.7)
    plt.title("Enhanced SHAP Feature Importance", fontsize=16)
    plt.gcf().set_facecolor('white')
    plt.savefig('shap_summary.png', dpi=300, bbox_inches='tight')
    plt.show()

    # Force plot for specific observation (Index 0 as example)
    if processed_data.shape[0] > 0:
        plt.figure(figsize=(12, 6))
        shap.initjs()
        # Ensure expected_value is not None or list
        expected_value = explainer.expected_value
        if isinstance(expected_value, np.ndarray) and expected_value.ndim > 0:
            expected_value = expected_value[0] # Assuming regression, take the first element

        try:
             shap.force_plot(expected_value, shap_values[0,:],
                           feature_names=processed_feature_names, matplotlib=True, show=False)
             plt.title("SHAP Force Plot for Single Prediction", fontsize=14)
             plt.tight_layout()
             plt.savefig('shap_force_plot.png', dpi=300, bbox_inches='tight')
             plt.show()
        except Exception as e:
             print(f"Error generating SHAP force plot: {e}. Skipping.")
    else:
        print("No data available in processed_data for SHAP force plot.")


# 5. Time Series Performance Tracking (Corrected function name from Time Series to Learning Curve)
def plot_learning_curve(model_pipeline, X_train, y_train):
    from sklearn.model_selection import learning_curve

    # Use the pipeline directly in learning_curve
    train_sizes, train_scores, test_scores = learning_curve(
        model_pipeline, X_train, y_train, cv=5, scoring='r2',
        train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1)

    plt.figure(figsize=(12, 8))
    plt.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', label='Training Score')
    plt.plot(train_sizes, np.mean(test_scores, axis=1), 'o-', label='Cross-Validation Score')

    plt.fill_between(train_sizes,
                    np.mean(train_scores, axis=1) - np.std(train_scores, axis=1),
                    np.mean(train_scores, axis=1) + np.std(train_scores, axis=1),
                    alpha=0.1)
    plt.fill_between(train_sizes,
                    np.mean(test_scores, axis=1) - np.std(test_scores, axis=1),
                    np.mean(test_scores, axis=1) + np.std(test_scores, axis=1),
                    alpha=0.1)

    plt.title('Enhanced Learning Curve', fontsize=16)
    plt.xlabel('Training Examples', fontsize=14)
    plt.ylabel('R2 Score', fontsize=14)
    plt.legend(loc='best', fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.savefig('learning_curve.png', dpi=300, bbox_inches='tight')
    plt.show()

# Usage Example (after training your best models):

# Assuming you have trained pipelines stored in these variables
# Replace with your actual trained pipeline variables if they are named differently
# For the example to run, we'll create dummy models. In your notebook,
# these should be your actual trained models from previous cells.

# --- Start Dummy Model Creation (Replace with your actual trained models) ---
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression # Example other model

# Define a basic preprocessor for the dummy models
numeric_features = ['age', 'bmi', 'children']
categorical_features = ['sex', 'smoker', 'region']

preprocessor_for_plots = ColumnTransformer(
    transformers=[
        ('num', Pipeline([
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
        ]), numeric_features),
        ('cat', Pipeline([
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('onehot', OneHotEncoder(handle_unknown='ignore'))
        ]), categorical_features)
    ])

# Create dummy pipelines (replace with your actual trained pipelines)
rf_model = Pipeline([('preprocessor', preprocessor_for_plots), ('model', RandomForestRegressor(random_state=42))])
mlp_model = Pipeline([('preprocessor', preprocessor_for_plots), ('model', MLPRegressor(random_state=42, max_iter=100))]) # Reduced max_iter for dummy

# Fit the dummy models (replace with your actual fitting process)
# Make sure X_train, y_train, X_test, y_test are defined
if 'X_train' not in globals():
    print("Warning: X_train, y_train, X_test, y_test not found. Creating dummy data for example.")
    # Create dummy data if it doesn't exist for demonstration
    data = pd.DataFrame({
        'age': np.random.randint(18, 65, 100),
        'sex': np.random.choice(['male', 'female'], 100),
        'bmi': np.random.uniform(20, 40, 100),
        'children': np.random.randint(0, 5, 100),
        'smoker': np.random.choice(['yes', 'no'], 100),
        'region': np.random.choice(['north', 'south'], 100),
        'charges': np.random.uniform(1000, 20000, 100) + np.random.randint(0, 2, 100)*15000 # Simple dummy charges
    })
    X = data.drop('charges', axis=1)
    y = data['charges']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    feature_names = X_train.columns.tolist() # Define feature_names from dummy data


try:
    import time
    start_time_rf = time.time()
    rf_model.fit(X_train, y_train)
    end_time_rf = time.time()
    rf_train_time = end_time_rf - start_time_rf

    start_time_mlp = time.time()
    mlp_model.fit(X_train, y_train)
    end_time_mlp = time.time()
    mlp_train_time = end_time_mlp - start_time_mlp

except Exception as e:
    print(f"Error fitting dummy models: {e}. Plots requiring fitted models may fail.")
    rf_train_time = 0 # Placeholder
    mlp_train_time = 0 # Placeholder
    # Set models to None or handle appropriately if fitting fails
    rf_model = None
    mlp_model = None

# --- End Dummy Model Creation ---


# Ensure feature_names is defined from your actual data loading if not using dummy data
if 'feature_names' not in globals():
    # This part is crucial if you are NOT using the dummy data creation above
    # Ensure X is loaded and feature_names is created before this cell
    print("Warning: 'feature_names' variable not found. Define it from your actual X DataFrame.")
    # Example: feature_names = X.columns.tolist()

# Initialize the results dictionary
results = {}

# Populate results for Random Forest if model was fitted
if rf_model is not None:
    try:
        rf_preds = rf_model.predict(X_test)
        results['Random Forest'] = {
            'R2': r2_score(y_test, rf_preds),
            'RMSE': np.sqrt(mean_squared_error(y_test, rf_preds)),
            'MAE': mean_absolute_error(y_test, rf_preds),
            'Time': rf_train_time, # Use calculated training time
            'Feature Importance': calculate_feature_importance(rf_model, X_test, y_test, feature_names),
            'Residuals': y_test - rf_preds
        }
    except Exception as e:
        print(f"Error calculating metrics/importance for Random Forest: {e}")

# Populate results for MLP if model was fitted
if mlp_model is not None:
    try:
        mlp_preds = mlp_model.predict(X_test)
        results['MLP'] = {
            'R2': r2_score(y_test, mlp_preds),
            'RMSE': np.sqrt(mean_squared_error(y_test, mlp_preds)),
            'MAE': mean_absolute_error(y_test, mlp_preds),
            'Time': mlp_train_time, # Use calculated training time
             # MLPRegressor does not have feature_importances_, calculate_feature_importance handles this
            'Feature Importance': calculate_feature_importance(mlp_model, X_test, y_test, feature_names),
            'Residuals': y_test - mlp_preds
        }
    except Exception as e:
        print(f"Error calculating metrics/importance for MLP: {e}")

# Add other models to the results dictionary similarly

# --- Plotting ---
if results:
    plot_model_comparison(results)

    # Example: Plot interactive predictions for the best model (e.g., RF if it exists)
    if 'Random Forest' in results:
        try:
            plot_interactive_predictions(rf_model, X_test, y_test)
        except Exception as e:
            print(f"Error generating interactive predictions plot: {e}")

    # Example: Plot PDP for the best model (e.g., RF if it exists)
    if 'Random Forest' in results:
         try:
            # Define features for PDP - make sure these exist in your original X
            pdp_features = ['age', 'bmi', 'children', 'smoker']
            plot_advanced_pdp(rf_model, X_train, pdp_features)
         except Exception as e:
             print(f"Error generating PDP plot: {e}")

    # Example: Plot SHAP for a tree-based model (e.g., RF if it exists)
    if 'Random Forest' in results:
        try:
             plot_enhanced_shap(rf_model, X_train)
        except Exception as e:
            print(f"Error generating SHAP plot: {e}")

    # Example: Plot Learning Curve for a model (e.g., RF if it exists)
    if 'Random Forest' in results:
         try:
             plot_learning_curve(rf_model, X_train, y_train)
         except Exception as e:
            print(f"Error generating learning curve plot: {e}")

else:
    print("No model results available to plot.")

import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import numpy as np
import pandas as pd
from matplotlib.gridspec import GridSpec
from sklearn.inspection import permutation_importance
plt.style.use('seaborn-v0_8-darkgrid')

# دالة مساعدة لحساب أهمية الميزات
def calculate_feature_importance(model, X_test, y_test, feature_names):
    try:
        # للنماذج التي لديها feature_importances_
        if hasattr(model.named_steps['model'], 'feature_importances_'):
            return pd.Series(model.named_steps['model'].feature_importances_,
                           index=feature_names).sort_values(ascending=False)
        else:
            # استخدام Permutation Importance للنماذج الأخرى
            result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)
            return pd.Series(result.importances_mean,
                           index=feature_names).sort_values(ascending=False)
    except Exception as e:
        print(f"Error calculating feature importance: {e}")
        return None

# 1. Enhanced Model Comparison Visualization
def plot_model_comparison(results):
    fig = plt.figure(figsize=(20, 12))
    gs = GridSpec(2, 2, figure=fig)

    # Model Performance Radar Chart
    ax1 = fig.add_subplot(gs[0, 0], polar=True)
    metrics = ['R2', 'RMSE', 'MAE', 'Training Time']
    angles = np.linspace(0, 2*np.pi, len(metrics), endpoint=False)
    angles = np.concatenate((angles, [angles[0]]))

    for model in results:
        values = [results[model]['R2'],
                 1/results[model]['RMSE'],
                 1/results[model]['MAE'],
                 1/results[model]['Time']]
        values = np.concatenate((values, [values[0]]))
        ax1.plot(angles, values, 'o-', linewidth=2, label=model)
        ax1.fill(angles, values, alpha=0.25)

    ax1.set_thetagrids(angles[:-1] * 180/np.pi, metrics)
    ax1.set_title('Model Performance Radar Chart', pad=20)
    ax1.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))

    # Feature Importance Comparison
    ax2 = fig.add_subplot(gs[0, 1])
    importance_df = pd.DataFrame()
    for model in results:
        if 'Feature Importance' in results[model] and results[model]['Feature Importance'] is not None:
            temp_df = pd.DataFrame({
                'Feature': results[model]['Feature Importance'].index,
                'Importance': results[model]['Feature Importance'].values,
                'Model': model
            })
            importance_df = pd.concat([importance_df, temp_df])

    if not importance_df.empty:
        sns.barplot(data=importance_df, x='Importance', y='Feature', hue='Model', ax=ax2)
        ax2.set_title('Comparative Feature Importance')
        ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    else:
        ax2.text(0.5, 0.5, 'No Feature Importance Available',
                ha='center', va='center', fontsize=12)
        ax2.set_title('Feature Importance Not Available')

    # Residual Analysis
    ax3 = fig.add_subplot(gs[1, :])
    for model in results:
        sns.kdeplot(results[model]['Residuals'], label=model, ax=ax3, linewidth=2)
    ax3.axvline(0, color='black', linestyle='--')
    ax3.set_title('Residual Distributions Comparison')
    ax3.legend()

    plt.tight_layout()
    plt.savefig('advanced_model_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()

# باقي الدوال تبقى كما هي (plot_interactive_predictions, plot_advanced_pdp, plot_enhanced_shap, plot_learning_curve)

# مثال استخدام:
feature_names = X_train.columns.tolist()  # تأكد من تعريف feature_names

results = {
    'Random Forest': {
        'R2': 0.87,
        'RMSE': 4200,
        'MAE': 2800,
        'Time': 15.2,
        'Feature Importance': calculate_feature_importance(rf_model, X_test, y_test, feature_names),
        'Residuals': y_test - rf_model.predict(X_test)
    },
    'MLP': {
        'R2': 0.85,
        'RMSE': 4500,
        'MAE': 3000,
        'Time': 25.5,
        'Feature Importance': calculate_feature_importance(mlp_model, X_test, y_test, feature_names),
        'Residuals': y_test - mlp_model.predict(X_test)
    }
}

plot_model_comparison(results)

import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import seaborn as sns
import pandas as pd
from sklearn.inspection import PartialDependenceDisplay
import shap

def create_model_report(models, X_train, X_test, y_test, filename='model_report.pdf'):
    # Create PDF
    with PdfPages(filename) as pdf:
        plt.figure(figsize=(11, 8))
        plt.axis('off')

        # Title Page
        plt.text(0.5, 0.7, 'Insurance Cost Prediction Report',
                ha='center', va='center', size=24)
        plt.text(0.5, 0.5, 'Comparative Analysis of Machine Learning Models',
                ha='center', va='center', size=18)
        plt.text(0.5, 0.3, f"Generated on {pd.Timestamp.now().strftime('%Y-%m-%d')}",
                ha='center', va='center', size=12)
        pdf.savefig()
        plt.close()

        # 1. Model Performance Summary
        plt.figure(figsize=(12, 6))
        metrics_df = pd.DataFrame.from_dict({m: models[m]['metrics'] for m in models},
                                           orient='index')
        metrics_df = metrics_df.sort_values('R2', ascending=False)

        # Create table
        cell_text = []
        for row in metrics_df.itertuples():
            cell_text.append([f"{getattr(row, x):.4f}" if isinstance(getattr(row, x), (float, int))
                            else str(getattr(row, x)) for x in metrics_df.columns])

        plt.table(cellText=cell_text,
                 colLabels=metrics_df.columns,
                 rowLabels=metrics_df.index,
                 loc='center',
                 cellLoc='center',
                 colColours=['#f3f3f3']*len(metrics_df.columns),
                 rowColours=['#f3f3f3']*len(metrics_df))

        plt.axis('off')
        plt.title('Model Performance Comparison', pad=20)
        pdf.savefig()
        plt.close()

        # 2. Feature Importance Comparison
        plt.figure(figsize=(12, 8))
        for i, model_name in enumerate(models):
            if 'feature_importance' in models[model_name]:
                plt.subplot(2, 2, i+1)
                fi = models[model_name]['feature_importance'].sort_values(ascending=False).head(10)
                sns.barplot(x=fi.values, y=fi.index, palette='viridis')
                plt.title(f'{model_name} Feature Importance')
                plt.xlabel('Importance Score')
        plt.tight_layout()
        pdf.savefig()
        plt.close()

        # 3. Partial Dependence Plots
        for model_name in models:
            if 'pdp_features' in models[model_name]:
                fig, ax = plt.subplots(figsize=(12, 8))
                PartialDependenceDisplay.from_estimator(
                    models[model_name]['model'],
                    X_train,
                    models[model_name]['pdp_features'],
                    ax=ax,
                    n_jobs=-1
                )
                plt.suptitle(f'{model_name} Partial Dependence', y=1.02)
                plt.tight_layout()
                pdf.savefig()
                plt.close()

        # 4. SHAP Summary Plots
        for model_name in models:
            if 'shap_values' in models[model_name]:
                plt.figure(figsize=(12, 8))
                shap.summary_plot(models[model_name]['shap_values'],
                                 models[model_name]['processed_data'],
                                 feature_names=models[model_name]['feature_names'],
                                 show=False)
                plt.title(f'{model_name} SHAP Summary', fontsize=16)
                plt.gcf().set_facecolor('white')
                pdf.savefig()
                plt.close()

        # 5. Residual Analysis
        plt.figure(figsize=(12, 8))
        for model_name in models:
            sns.kdeplot(models[model_name]['residuals'], label=model_name)
        plt.axvline(0, color='black', linestyle='--')
        plt.title('Residual Distributions Comparison')
        plt.xlabel('Prediction Error (Actual - Predicted)')
        plt.legend()
        pdf.savefig()
        plt.close()

        # 6. Actual vs Predicted
        plt.figure(figsize=(12, 8))
        for model_name in models:
            plt.scatter(y_test, models[model_name]['predictions'],
                       alpha=0.5, label=model_name)
        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
                'k--', lw=2)
        plt.xlabel('Actual')
        plt.ylabel('Predicted')
        plt.title('Actual vs Predicted Values')
        plt.legend()
        pdf.savefig()
        plt.close()

        # 7. Model Parameters
        plt.figure(figsize=(12, 6))
        plt.axis('off')
        for i, model_name in enumerate(models):
            params = str(models[model_name]['model'].get_params())
            plt.text(0.1, 0.9-(i*0.2),
                    f"{model_name} Parameters:\n{params}",
                    fontfamily='monospace',
                    fontsize=8,
                    verticalalignment='top')
        pdf.savefig()
        plt.close()

# Example usage after training models:
models_dict = {
    'Random Forest': {
        'model': rf_model,
        'metrics': {
            'R2': r2_score(y_test, rf_preds),
            'RMSE': mean_squared_error(y_test, rf_preds, squared=False),
            'MAE': mean_absolute_error(y_test, rf_preds),
            'Training Time': rf_time
        },
        'feature_importance': pd.Series(rf_model.feature_importances_,
                                      index=feature_names),
        'residuals': y_test - rf_preds,
        'predictions': rf_preds,
        'shap_values': rf_shap_values,
        'processed_data': rf_processed_data,
        'feature_names': feature_names,
        'pdp_features': ['age', 'bmi', 'smoker']
    },
    # Add other models similarly
}

create_model_report(models_dict, X_train, X_test, y_test, 'Insurance_Model_Report.pdf')

if 'feature_importance' in models[model_na

if 'feature_importance' in models[model_na

import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import seaborn as sns
import pandas as pd
from sklearn.inspection import PartialDependenceDisplay
import shap

def create_model_report(models, X_train, X_test, y_test, filename='model_report.pdf'):
    # Create PDF
    with PdfPages(filename) as pdf:
        plt.figure(figsize=(11, 8))
        plt.axis('off')

        # Title Page
        plt.text(0.5, 0.7, 'Insurance Cost Prediction Report',
                ha='center', va='center', size=24)
        plt.text(0.5, 0.5, 'Comparative Analysis of Machine Learning Models',
                ha='center', va='center', size=18)
        plt.text(0.5, 0.3, f"Generated on {pd.Timestamp.now().strftime('%Y-%m-%d')}",
                ha='center', va='center', size=12)
        pdf.savefig()
        plt.close()

# 1. Model Performance Summary
        plt.figure(figsize=(12, 6))
        metrics_df = pd.DataFrame.from_dict({m: models[m]['metrics'] for m in models},
                                           orient='index')
        metrics_df = metrics_df.sort_values('R2', ascending=False)

        # Create table
        cell_text = []
        for row in metrics_df.itertuples():
            cell_text.append([f"{getattr(row, x):.4f}" if isinstance(getattr(row, x), (float, int))
                            else str(getattr(row, x)) for x in metrics_df.columns])

        plt.table(cellText=cell_text,
                 colLabels=metrics_df.columns,
                 rowLabels=metrics_df.index,
                 loc='center',
                 cellLoc='center',
                 colColours=['#f3f3f3']*len(metrics_df.columns),
                 rowColours=['#f3f3f3']*len(metrics_df))

        plt.axis('off')
        plt.title('Model Performance Comparison', pad=20)
        pdf.savefig()
        plt.close()

# 2. Feature Importance Comparison
        plt.figure(figsize=(12, 8))
        for i, model_name in enumerate(models):
            if 'feature_importance' in models[model_name]:
                plt.subplot(2, 2, i+1)
                fi = models[model_name]['feature_importance'].sort_values(ascending=False).head(10)
                sns.barplot(x=fi.values, y=fi.index, palette='viridis')
                plt.title(f'{model_name} Feature Importance')
                plt.xlabel('Importance Score')
        plt.tight_layout()
        pdf.savefig()
        plt.close()

import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import seaborn as sns
import pandas as pd
from sklearn.inspection import PartialDependenceDisplay
import shap
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score # Import required metrics
import numpy as np # Import numpy for squared=False in mean_squared_error

def create_model_report(models, X_train, X_test, y_test, filename='model_report.pdf'):
    # Create PDF
    with PdfPages(filename) as pdf:
        plt.figure(figsize=(11, 8))
        plt.axis('off')

        # Title Page
        plt.text(0.5, 0.7, 'Insurance Cost Prediction Report',
                ha='center', va='center', size=24)
        plt.text(0.5, 0.5, 'Comparative Analysis of Machine Learning Models',
                ha='center', va='center', size=18)
        plt.text(0.5, 0.3, f"Generated on {pd.Timestamp.now().strftime('%Y-%m-%d')}",
                ha='center', va='center', size=12)
        pdf.savefig()
        plt.close()

        # 1. Model Performance Summary
        plt.figure(figsize=(12, 6))
        metrics_df = pd.DataFrame.from_dict({m: models[m]['metrics'] for m in models},
                                           orient='index')
        metrics_df = metrics_df.sort_values('R2', ascending=False)

        # Create table
        cell_text = []
        for row in metrics_df.itertuples():
            cell_text.append([f"{getattr(row, x):.4f}" if isinstance(getattr(row, x), (float, int))
                            else str(getattr(row, x)) for x in metrics_df.columns])

        plt.table(cellText=cell_text,
                 colLabels=metrics_df.columns,
                 rowLabels=metrics_df.index,
                 loc='center',
                 cellLoc='center',
                 colColours=['#f3f3f3']*len(metrics_df.columns),
                 rowColours=['#f3f3f3']*len(metrics_df))

        plt.axis('off')
        plt.title('Model Performance Comparison', pad=20)
        pdf.savefig()
        plt.close()

        # 2. Feature Importance Comparison
        plt.figure(figsize=(12, 8))
        for i, model_name in enumerate(models):
            # Use subplots only if there are importance plots to show
            if 'feature_importance' in models[model_name] and models[model_name]['feature_importance'] is not None and not models[model_name]['feature_importance'].empty:
                # Calculate grid size based on the number of models with importance
                num_importance_plots = sum(1 for m in models if 'feature_importance' in models[m] and models[m]['feature_importance'] is not None and not models[m]['feature_importance'].empty)
                rows = int(np.ceil(num_importance_plots / 2)) if num_importance_plots > 0 else 1
                cols = 2 if num_importance_plots > 0 else 1

                # Create subplot grid if not already created for this section
                if not 'importance_fig' in locals():
                    importance_fig, importance_axes = plt.subplots(rows, cols, figsize=(12, 8))
                    importance_axes = importance_axes.ravel() if num_importance_plots > 1 else [importance_axes]
                    current_plot_index = 0

                ax = importance_axes[current_plot_index]
                fi = models[model_name]['feature_importance'].sort_values(ascending=False).head(10)
                sns.barplot(x=fi.values, y=fi.index, palette='viridis', ax=ax)
                ax.set_title(f'{model_name} Feature Importance')
                ax.set_xlabel('Importance Score')
                current_plot_index += 1

        if 'importance_fig' in locals():
            plt.tight_layout()
            pdf.savefig(importance_fig) # Save the figure containing importance plots
            plt.close(importance_fig)
        else:
             # If no importance plots were generated, create a placeholder page
             plt.figure(figsize=(11, 8))
             plt.axis('off')
             plt.text(0.5, 0.5, 'No Feature Importance Data Available',
                     ha='center', va='center', size=14)
             pdf.savefig()
             plt.close()


        # 3. Partial Dependence Plots
        # Group PDPs by model for better layout
        for model_name in models:
            if 'pdp_features' in models[model_name] and models[model_name]['pdp_features']:
                num_pdp_features = len(models[model_name]['pdp_features'])
                rows = int(np.ceil(num_pdp_features / 2))
                cols = 2 if num_pdp_features > 0 else 1

                if num_pdp_features > 0:
                    fig, axes = plt.subplots(rows, cols, figsize=(12, rows * 4))
                    axes = axes.ravel() if num_pdp_features > 1 else [axes] # Ensure axes is iterable

                    for i, feature in enumerate(models[model_name]['pdp_features']):
                        ax = axes[i]
                        PartialDependenceDisplay.from_estimator(
                            models[model_name]['model'],
                            X_train,
                            [feature], # Pass features as a list
                            ax=ax,
                            n_jobs=-1
                        )
                        ax.set_title(f'PDP for {feature}')

                    # Hide any unused subplots
                    for j in range(i + 1, len(axes)):
                        fig.delaxes(axes[j])


                    plt.suptitle(f'{model_name} Partial Dependence', y=1.02, fontsize=16)
                    plt.tight_layout()
                    pdf.savefig(fig)
                    plt.close(fig)
                else:
                    print(f"Warning: 'pdp_features' list is empty for {model_name}. Skipping PDP plot.")
            elif 'pdp_features' in models[model_name] and not models[model_name]['pdp_features']:
                print(f"Warning: 'pdp_features' list is empty for {model_name}. Skipping PDP plot.")
            else:
                 # If no pdp data, create a placeholder page
                 plt.figure(figsize=(11, 8))
                 plt.axis('off')
                 plt.text(0.5, 0.5, f'No Partial Dependence Data Available for {model_name}',
                          ha='center', va='center', size=14)
                 pdf.savefig()
                 plt.close()


        # 4. SHAP Summary Plots
        for model_name in models:
            if 'shap_values' in models[model_name] and 'processed_data' in models[model_name] and 'feature_names' in models[model_name] and models[model_name]['shap_values'] is not None:
                 plt.figure(figsize=(12, 8))
                 # Ensure processed_data and shap_values have matching shapes if needed by shap
                 # This might require more sophisticated handling depending on the SHAP explainer output
                 try:
                     shap.summary_plot(models[model_name]['shap_values'],
                                      models[model_name]['processed_data'],
                                      feature_names=models[model_name]['feature_names'],
                                      show=False)
                     plt.title(f'{model_name} SHAP Summary', fontsize=16)
                     plt.gcf().set_facecolor('white')
                     pdf.savefig()
                     plt.close()
                 except Exception as e:
                     print(f"Error generating SHAP summary plot for {model_name}: {e}")
                     plt.close() # Close the figure even if there was an error
                     # Create a placeholder page
                     plt.figure(figsize=(11, 8))
                     plt.axis('off')
                     plt.text(0.5, 0.5, f'Error or Data Mismatch for SHAP Summary Plot ({model_name})',
                              ha='center', va='center', size=14)
                     pdf.savefig()
                     plt.close()

            else:
                # If no shap data, create a placeholder page
                plt.figure(figsize=(11, 8))
                plt.axis('off')
                plt.text(0.5, 0.5, f'No SHAP Data Available for {model_name}',
                         ha='center', va='center', size=14)
                pdf.savefig()
                plt.close()


        # 5. Residual Analysis
        plt.figure(figsize=(12, 8))
        has_residuals = False
        for model_name in models:
            if 'residuals' in models[model_name] and models[model_name]['residuals'] is not None:
                sns.kdeplot(models[model_name]['residuals'], label=model_name)
                has_residuals = True

        if has_residuals:
            plt.axvline(0, color='black', linestyle='--')
            plt.title('Residual Distributions Comparison')
            plt.xlabel('Prediction Error (Actual - Predicted)')
            plt.legend()
            pdf.savefig()
            plt.close()
        else:
             # If no residual data, create a placeholder page
             plt.figure(figsize=(11, 8))
             plt.axis('off')
             plt.text(0.5, 0.5, 'No Residual Data Available',
                     ha='center', va='center', size=14)
             pdf.savefig()
             plt.close()

        # 6. Actual vs Predicted
        plt.figure(figsize=(12, 8))
        has_predictions = False
        for model_name in models:
            if 'predictions' in models[model_name] and models[model_name]['predictions'] is not None:
                plt.scatter(y_test, models[model_name]['predictions'],
                           alpha=0.5, label=model_name)
                has_predictions = True

        if has_predictions:
             # Ensure the line goes through the full range of actual/predicted values
             min_val = min(y_test.min(), min(models[m]['predictions'].min() for m in models if 'predictions' in models[m] and models[m]['predictions'] is not None))
             max_val = max(y_test.max(), max(models[m]['predictions'].max() for m in models if 'predictions' in models[m] and models[m]['predictions'] is not None))
             plt.plot([min_val, max_val], [min_val, max_val],
                     'k--', lw=2)
             plt.xlabel('Actual')
             plt.ylabel('Predicted')
             plt.title('Actual vs Predicted Values')
             plt.legend()
             pdf.savefig()
             plt.close()
        else:
             # If no prediction data, create a placeholder page
             plt.figure(figsize=(11, 8))
             plt.axis('off')
             plt.text(0.5, 0.5, 'No Prediction Data Available',
                     ha='center', va='center', size=14)
             pdf.savefig()
             plt.close()


        # 7. Model Parameters
        plt.figure(figsize=(12, 6))
        plt.axis('off')
        param_text_y = 0.9
        text_spacing = 0.05 # Adjust spacing as needed

        params_to_show = {}
        for model_name in models:
             if 'model' in models[model_name]:
                 params_to_show[model_name] = str(models[model_name]['model'].get_params(deep=False)) # Get only top-level pipeline params

        if params_to_show:
            for model_name, params in params_to_show.items():
                # Split parameters string into lines if too long
                lines = []
                current_line = f"{model_name} Parameters:"
                param_str = str(params).replace('{', '').replace('}', '').split(',')
                line_length_limit = 80 # Adjust limit based on desired line length

                for item in param_str:
                    if len(current_line) + len(item) < line_length_limit:
                         current_line += item + ','
                    else:
                         lines.append(current_line.rstrip(','))
                         current_line = '  ' + item + ',' # Indent subsequent lines

                lines.append(current_line.rstrip(',')) # Add the last line

                param_text = "\n".join(lines)

                plt.text(0.05, param_text_y,
                        param_text,
                        fontfamily='monospace',
                        fontsize=8,
                        verticalalignment='top')
                param_text_y -= (len(lines) * text_spacing) + 0.02 # Adjust y position for next model

                if param_text_y < 0.1: # Check if we need a new page
                    pdf.savefig()
                    plt.close()
                    plt.figure(figsize=(12, 6))
                    plt.axis('off')
                    param_text_y = 0.9


            pdf.savefig() # Save the last parameter page
            plt.close()
        else:
            # If no parameter data, create a placeholder page
            plt.figure(figsize=(11, 8))
            plt.axis('off')
            plt.text(0.5, 0.5, 'No Model Parameter Data Available',
                    ha='center', va='center', size=14)
            pdf.savefig()
            plt.close()


# Example usage after training models:
# Assuming you have trained models like RandomForestRegressor within pipelines
# and calculated metrics, predictions, residuals, feature_importance, shap_values, etc.
# You need to define these variables based on your training code before calling this function.

# Example placeholders (YOU NEED TO REPLACE THESE WITH YOUR ACTUAL TRAINED MODELS AND RESULTS)
# Make sure your models are instances of sklearn pipelines if they include preprocessing
# Example of a pipeline structure
# rf_model = Pipeline([
#     ('preprocessor', your_preprocessor),
#     ('regressor', RandomForestRegressor(random_state=42))
# ])
# rf_model.fit(X_train, y_train)
# rf_preds = rf_model.predict(X_test)
# rf_residuals = y_test - rf_preds
# rf_time = 0 # Placeholder for training time
# # For feature importance, you'd need to get it from the regressor step after fitting
# # and map it back to processed feature names. This is complex if using a pipeline.
# # A simpler approach for reporting is using Permutation Importance on the pipeline.
# from sklearn.inspection import permutation_importance
# result = permutation_importance(rf_model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)
# # Need processed feature names to map to result.importances_mean
# # Getting processed feature names from a ColumnTransformer pipeline is non-trivial.
# # For simplicity in reporting, you might use Permutation Importance and original feature names
# # or implement a helper to extract processed names.
# # Let's assume feature_names are original column names for this example
# feature_names = X_test.columns # Placeholder
# # For SHAP, you'd need to handle preprocessing before passing data to Explainer
# # rf_processed_data = rf_model.named_steps['preprocessor'].transform(X_test)
# # explainer = shap.TreeExplainer(rf_model.named_steps['regressor']) # Explainer on the model step
# # rf_shap_values = explainer.shap_values(rf_processed_data) # SHAP values on processed data

# Example dummy data to make the function runnable (Replace with your real data)
# try:
#      # Attempt to use existing X_train, X_test, y_train, y_test if defined
#      X_train.shape, X_test.shape, y_train.shape, y_test.shape
#      print("Using existing X_train, X_test, y_train, y_test.")
# except NameError:
#      # Define dummy data if not found
#      print("Defining dummy data for X_train, X_test, y_train, y_test.")
#      from sklearn.model_selection import train_test_split
#      from sklearn.datasets import make_regression
#      X_dummy, y_dummy = make_regression(n_samples=100, n_features=5, random_state=42)
#      X_train, X_test, y_train, y_test = train_test_split(X_dummy, y_dummy, test_size=0.2, random_state=42)
#      X_train = pd.DataFrame(X_train, columns=[f'feature_{i}' for i in range(X_train.shape[1])])
#      X_test = pd.DataFrame(X_test, columns=[f'feature_{i}' for i in range(X_test.shape[1])])
#      feature_names = X_test.columns # Dummy feature names


# Example dummy model dictionary structure
# You will need to populate this with your actual trained models and results
# For demonstration, let's create a simple dummy Random Forest Pipeline
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestRegressor
import time # Import time to simulate training time

# Define a simple preprocessor for the dummy model
numeric_features_dummy = ['age', 'bmi', 'children'] # Using potential insurance features as dummy
categorical_features_dummy = ['sex', 'smoker', 'region']

# Ensure these features exist in your dummy or real X_train/X_test DataFrames
# If using the make_regression dummy data above, adjust feature names.
# If using the actual insurance data, this preprocessor is appropriate.

# For this example, let's assume we are using the insurance data and the preprocessor defined earlier in the notebook
try:
    # Check if the actual insurance data and preprocessor are available
    X, y, preprocessor
    print("Using actual insurance data and preprocessor.")

    # Train a dummy RF model pipeline using the actual data
    rf_model_pipe = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))
    ])

    start_time = time.time()
    rf_model_pipe.fit(X_train, y_train)
    rf_time = time.time() - start_time

    rf_preds = rf_model_pipe.predict(X_test)
    rf_residuals = y_test - rf_preds

    # Calculate feature importance using permutation importance on the pipeline
    result = permutation_importance(rf_model_pipe, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)

    # Get feature names after preprocessing
    # This requires accessing the one-hot encoded feature names
    ohe_feature_names = rf_model_pipe.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features_dummy)
    processed_feature_names = list(numeric_features_dummy) + list(ohe_feature_names)

    rf_feature_importance = pd.Series(result.importances_mean, index=X_test.columns) # Using original column names for permutation importance plot
    # If you want processed feature names for importance plot, use `processed_feature_names` with `result.importances_mean`
    # rf_feature_importance_processed = pd.Series(result.importances_mean, index=processed_feature_names) # Using processed names

    # SHAP calculation for a tree-based model within a pipeline
    # Note: Calculating SHAP for a pipeline with a preprocessor can be tricky.
    # One common way is to apply the preprocessor manually and then use the model step.
    try:
        explainer = shap.TreeExplainer(rf_model_pipe.named_steps['regressor'])
        # Transform X_test data using the preprocessor step
        rf_processed_data = rf_model_pipe.named_steps['preprocessor'].transform(X_test)

        # Handle potential different return types of shap_values (e.g., for multi-output)
        rf_shap_values = explainer.shap_values(rf_processed_data)
        if isinstance(rf_shap_values, list):
            # Assuming regression, take the first element if it's a list of arrays
            if len(rf_shap_values) > 0 and isinstance(rf_shap_values[0], np.ndarray):
                rf_shap_values = rf_shap_values[0]
            else:
                print("Unexpected SHAP values format for RF. Setting to None.")
                rf_shap_values = None
        # Ensure SHAP values and processed data have same features if needed for plotting
        if rf_shap_values is not None and rf_shap_values.shape[1] != rf_processed_data.shape[1]:
             print(f"Mismatch in features for RF SHAP ({rf_shap_values.shape[1]} vs {rf_processed_data.shape[1]}). Setting SHAP values to None.")
             rf_shap_values = None


    except Exception as e:
        print(f"Could not calculate SHAP for RF model: {e}. Setting SHAP values and processed_data to None.")
        rf_shap_values = None
        rf_processed_data = None
        processed_feature_names_for_shap = None # Ensure this is None if SHAP failed

    # Define PDP features using original names
    rf_pdp_features = ['age', 'bmi', 'smoker'] # Example features

    models_dict = {
        'Random Forest': {
            'model': rf_model_pipe, # Pass the pipeline
            'metrics': {
                'R2': r2_score(y_test, rf_preds),
                'RMSE': mean_squared_error(y_test, rf_preds, squared=False),
                'MAE': mean_absolute_error(y_test, rf_preds),
                'Training Time': rf_time
            },
            'feature_importance': rf_feature_importance, # Using permutation importance on original features
            # 'feature_importance': rf_feature_importance_processed, # Or use importance on processed features
            'residuals': rf_residuals,
            'predictions': rf_preds,
            'shap_values': rf_shap_values, # SHAP values on processed data
            'processed_data': rf_processed_data, # Processed data used for SHAP
            'feature_names': processed_feature_names, # Processed feature names for SHAP plots
            'pdp_features': rf_pdp_features # Original feature names for PDP plots
        },
        # Add other models similarly, ensuring they are pipelines and you provide
        # metrics, predictions, residuals, and optionally feature_importance, shap_values, pdp_features
    }

    # Call the function with the populated models_dict
    create_model_report(models_dict, X_train, X_test, y_test, 'Insurance_Model_Report.pdf')


except NameError:
    print("X, y, or preprocessor are not defined. Please ensure you have loaded your data and defined the preprocessor before running this cell.")
    print("Skipping report generation.")
except Exception as e:
    print(f"An unexpected error occurred during report generation: {e}")

import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import seaborn as sns
import pandas as pd
from sklearn.inspection import PartialDependenceDisplay
import shap

def create_model_report(models, X_train, X_test, y_test, filename='model_report.pdf'):
    # Create PDF
    with PdfPages(filename) as pdf:
        plt.figure(figsize=(11, 8))
        plt.axis('off')

        # Title Page
        plt.text(0.5, 0.7, 'Insurance Cost Prediction Report',
                ha='center', va='center', size=24)
        plt.text(0.5, 0.5, 'Comparative Analysis of Machine Learning Models',
                ha='center', va='center', size=18)
        plt.text(0.5, 0.3, f"Generated on {pd.Timestamp.now().strftime('%Y-%m-%d')}",
                ha='center', va='center', size=12)
        pdf.savefig()
        plt.close()

        # 1. Model Performance Summary
        # This block was incorrectly indented and is now part of the function
        plt.figure(figsize=(12, 6))
        metrics_df = pd.DataFrame.from_dict({m: models[m]['metrics'] for m in models},
                                           orient='index')
        metrics_df = metrics_df.sort_values('R2', ascending=False)

        # Create table
        cell_text = []
        for row in metrics_df.itertuples():
            cell_text.append([f"{getattr(row, x):.4f}" if isinstance(getattr(row, x), (float, int))
                            else str(getattr(row, x)) for x in metrics_df.columns])

        plt.table(cellText=cell_text,
                 colLabels=metrics_df.columns,
                 rowLabels=metrics_df.index,
                 loc='center',
                 cellLoc='center',
                 colColours=['#f3f3f3']*len(metrics_df.columns),
                 rowColours=['#f3f3f3']*len(metrics_df))

        plt.axis('off')
        plt.title('Model Performance Comparison', pad=20)
        pdf.savefig()
        plt.close()

        # You would then continue adding the rest of the sections (Feature Importance, etc.)
        # from your original `create_model_report` function here, maintaining the correct indentation.
        # For example:

        # 2. Feature Importance Comparison
        plt.figure(figsize=(12, 8))
        for i, model_name in enumerate(models):
            # Ensure 'feature_importance' key exists
            if 'feature_importance' in models[model_name]:
                # Check if i+1 exceeds the subplot grid size (2x2 in the original example)
                # You might need to adjust subplot layout if you have more than 4 models
                if i < 4: # Limit to first 4 models for a 2x2 grid example
                    plt.subplot(2, 2, i+1)
                    # Ensure feature_importance is not None or empty
                    if models[model_name]['feature_importance'] is not None and not models[model_name]['feature_importance'].empty:
                        fi = models[model_name]['feature_importance'].sort_values(ascending=False).head(10)
                        sns.barplot(x=fi.values, y=fi.index, palette='viridis')
                        plt.title(f'{model_name} Feature Importance')
                        plt.xlabel('Importance Score')
                    else:
                        plt.text(0.5, 0.5, 'No Feature Importance Data', ha='center', va='center', fontsize=10)
                        plt.title(f'{model_name} Feature Importance (N/A)')
                else:
                    print(f"Skipping feature importance plot for {model_name} as subplot limit reached.")
        plt.tight_layout()
        pdf.savefig()
        plt.close()

        # Continue with other sections like Partial Dependence, SHAP, etc.
        # Make sure to include the full code from your original file ipython-input-24-146ce657c78b
        # within this function, maintaining the 4-space indentation for each block.

'RMSE': mean_squared_error(y_test, rf_preds, squared=False),

from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import numpy as np

import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import seaborn as sns
import pandas as pd
import numpy as np
from sklearn.inspection import PartialDependenceDisplay
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import shap

def create_model_report(models, X_train, X_test, y_test, filename='model_report.pdf'):
    # Create PDF
    with PdfPages(filename) as pdf:
        plt.figure(figsize=(11, 8))
        plt.axis('off')

        # Title Page
        plt.text(0.5, 0.7, 'Insurance Cost Prediction Report',
                ha='center', va='center', size=24)
        plt.text(0.5, 0.5, 'Comparative Analysis of Machine Learning Models',
                ha='center', va='center', size=18)
        plt.text(0.5, 0.3, f"Generated on {pd.Timestamp.now().strftime('%Y-%m-%d')}",
                ha='center', va='center', size=12)
        pdf.savefig()
        plt.close()

        # 1. Model Performance Summary
        plt.figure(figsize=(12, 6))
        metrics_df = pd.DataFrame.from_dict({m: models[m]['metrics'] for m in models},
                                           orient='index')
        metrics_df = metrics_df.sort_values('R2', ascending=False)

        # Create table
        cell_text = []
        for row in metrics_df.itertuples():
            cell_text.append([f"{getattr(row, x):.4f}" if isinstance(getattr(row, x), (float, int))
                            else str(getattr(row, x)) for x in metrics_df.columns])

        plt.table(cellText=cell_text,
                 colLabels=metrics_df.columns,
                 rowLabels=metrics_df.index,
                 loc='center',
                 cellLoc='center',
                 colColours=['#f3f3f3']*len(metrics_df.columns),
                 rowColours=['#f3f3f3']*len(metrics_df))

        plt.axis('off')
        plt.title('Model Performance Comparison', pad=20)
        pdf.savefig()
        plt.close()

        # باقي الدوال تبقى كما هي...

# Example usage:
models_dict = {
    'Random Forest': {
        'model': rf_model,
        'metrics': {
            'R2': r2_score(y_test, rf_preds),
            'RMSE': np.sqrt(mean_squared_error(y_test, rf_preds)),  # التعديل هنا
            'MAE': mean_absolute_error(y_test, rf_preds),
            'Training Time': rf_time
        },
        'feature_importance': pd.Series(rf_model.feature_importances_,
                                      index=feature_names),
        'residuals': y_test - rf_preds,
        'predictions': rf_preds,
        'shap_values': rf_shap_values,
        'processed_data': rf_processed_data,
        'feature_names': feature_names,
        'pdp_features': ['age', 'bmi', 'smoker']
    }
}

create_model_report(models_dict, X_train, X_test, y_test, 'Insurance_Model_Report.pdf')

import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import seaborn as sns
import pandas as pd
import numpy as np
import time
from sklearn.inspection import PartialDependenceDisplay
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import shap
from sklearn.ensemble import RandomForestRegressor

def create_model_report(models, X_train, X_test, y_test, filename='model_report.pdf'):
    """إنشاء تقرير PDF مقارنة بين نماذج التعلم الآلي"""
    with PdfPages(filename) as pdf:
        # صفحة العنوان
        plt.figure(figsize=(11, 8))
        plt.axis('off')
        plt.text(0.5, 0.7, 'Insurance Cost Prediction Report',
                ha='center', va='center', size=24)
        plt.text(0.5, 0.5, 'Comparative Analysis of Machine Learning Models',
                ha='center', va='center', size=18)
        plt.text(0.5, 0.3, f"Generated on {pd.Timestamp.now().strftime('%Y-%m-%d')}",
                ha='center', va='center', size=12)
        pdf.savefig()
        plt.close()

        # 1. مقارنة أداء النماذج
        plt.figure(figsize=(12, 6))
        metrics_df = pd.DataFrame.from_dict({m: models[m]['metrics'] for m in models}, orient='index')
        metrics_df = metrics_df.sort_values('R2', ascending=False)

        cell_text = []
        for row in metrics_df.itertuples():
            cell_text.append([f"{getattr(row, x):.4f}" if isinstance(getattr(row, x), (float, int))
                            else str(getattr(row, x)) for x in metrics_df.columns])

        plt.table(cellText=cell_text,
                 colLabels=metrics_df.columns,
                 rowLabels=metrics_df.index,
                 loc='center',
                 cellLoc='center',
                 colColours=['#f3f3f3']*len(metrics_df.columns),
                 rowColours=['#f3f3f3']*len(metrics_df))
        plt.axis('off')
        plt.title('Model Performance Comparison', pad=20)
        pdf.savefig()
        plt.close()

        # 2. مقارنة أهمية الميزات
        if any('feature_importance' in models[m] for m in models):
            plt.figure(figsize=(12, 8))
            for i, model_name in enumerate(models):
                if 'feature_importance' in models[model_name]:
                    plt.subplot(2, 2, i+1)
                    fi = models[model_name]['feature_importance'].sort_values(ascending=False).head(10)
                    sns.barplot(x=fi.values, y=fi.index, palette='viridis')
                    plt.title(f'{model_name} Feature Importance')
                    plt.xlabel('Importance Score')
            plt.tight_layout()
            pdf.savefig()
            plt.close()

        # 3. Partial Dependence Plots
        for model_name in models:
            if 'pdp_features' in models[model_name] and hasattr(models[model_name]['model'], 'predict'):
                try:
                    fig, ax = plt.subplots(figsize=(12, 8))
                    PartialDependenceDisplay.from_estimator(
                        models[model_name]['model'],
                        X_train,
                        models[model_name]['pdp_features'],
                        ax=ax,
                        n_jobs=-1
                    )
                    plt.suptitle(f'{model_name} Partial Dependence', y=1.02)
                    plt.tight_layout()
                    pdf.savefig()
                    plt.close()
                except Exception as e:
                    print(f"Error creating PDP for {model_name}: {e}")

        # 4. SHAP Plots
        for model_name in models:
            if 'shap_values' in models[model_name] and models[model_name]['shap_values'] is not None:
                try:
                    plt.figure(figsize=(12, 8))
                    shap.summary_plot(models[model_name]['shap_values'],
                                    X_test if 'processed_data' not in models[model_name] else models[model_name]['processed_data'],
                                    feature_names=models[model_name].get('feature_names', X_test.columns),
                                    show=False)
                    plt.title(f'{model_name} SHAP Summary', fontsize=16)
                    plt.gcf().set_facecolor('white')
                    pdf.savefig()
                    plt.close()
                except Exception as e:
                    print(f"Error creating SHAP plot for {model_name}: {e}")

        # 5. تحليل البواقي
        plt.figure(figsize=(12, 8))
        for model_name in models:
            if 'residuals' in models[model_name]:
                sns.kdeplot(models[model_name]['residuals'], label=model_name)

import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import seaborn as sns
import pandas as pd
import numpy as np
import time
from sklearn.inspection import PartialDependenceDisplay
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import shap
from sklearn.ensemble import RandomForestRegressor

def create_model_report(models, X_train, X_test, y_test, filename='model_report.pdf'):
    """إنشاء تقرير PDF مقارنة بين نماذج التعلم الآلي"""
    with PdfPages(filename) as pdf:
        # صفحة العنوان
        plt.figure(figsize=(11, 8))
        plt.axis('off')
        plt.text(0.5, 0.7, 'Insurance Cost Prediction Report',
                ha='center', va='center', size=24)
        plt.text(0.5, 0.5, 'Comparative Analysis of Machine Learning Models',
                ha='center', va='center', size=18)
        plt.text(0.5, 0.3, f"Generated on {pd.Timestamp.now().strftime('%Y-%m-%d')}",
                ha='center', va='center', size=12)
        pdf.savefig()
        plt.close()

        # 1. مقارنة أداء النماذج
        plt.figure(figsize=(12, 6))
        metrics_df = pd.DataFrame.from_dict({m: models[m]['metrics'] for m in models}, orient='index')
        metrics_df = metrics_df.sort_values('R2', ascending=False)

        cell_text = []
        for row in metrics_df.itertuples():
            cell_text.append([f"{getattr(row, x):.4f}" if isinstance(getattr(row, x), (float, int))
                            else str(getattr(row, x)) for x in metrics_df.columns])

        plt.table(cellText=cell_text,
                 colLabels=metrics_df.columns,
                 rowLabels=metrics_df.index,
                 loc='center',
                 cellLoc='center',
                 colColours=['#f3f3f3']*len(metrics_df.columns),
                 rowColours=['#f3f3f3']*len(metrics_df))
        plt.axis('off')
        plt.title('Model Performance Comparison', pad=20)
        pdf.savefig()
        plt.close()

        # 2. مقارنة أهمية الميزات
        if any('feature_importance' in models[m] for m in models):
            plt.figure(figsize=(12, 8))
            for i, model_name in enumerate(models):
                if 'feature_importance' in models[model_name]:
                    plt.subplot(2, 2, i+1)
                    fi = models[model_name]['feature_importance'].sort_values(ascending=False).head(10)
                    sns.barplot(x=fi.values, y=fi.index, palette='viridis')
                    plt.title(f'{model_name} Feature Importance')
                    plt.xlabel('Importance Score')
            plt.tight_layout()
            pdf.savefig()
            plt.close()

        # 3. Partial Dependence Plots
        for model_name in models:
            if 'pdp_features' in models[model_name] and hasattr(models[model_name]['model'], 'predict'):
                try:
                    fig, ax = plt.subplots(figsize=(12, 8))
                    PartialDependenceDisplay.from_estimator(
                        models[model_name]['model'],
                        X_train,
                        models[model_name]['pdp_features'],
                        ax=ax,
                        n_jobs=-1
                    )
                    plt.suptitle(f'{model_name} Partial Dependence', y=1.02)
                    plt.tight_layout()
                    pdf.savefig()
                    plt.close()
                except Exception as e:
                    print(f"Error creating PDP for {model_name}: {e}")

        # 4. SHAP Plots
        for model_name in models:
            if 'shap_values' in models[model_name] and models[model_name]['shap_values'] is not None:
                try:
                    plt.figure(figsize=(12, 8))
                    shap.summary_plot(models[model_name]['shap_values'],
                                    X_test if 'processed_data' not in models[model_name] else models[model_name]['processed_data'],
                                    feature_names=models[model_name].get('feature_names', X_test.columns),
                                    show=False)
                    plt.title(f'{model_name} SHAP Summary', fontsize=16)
                    plt.gcf().set_facecolor('white')
                    pdf.savefig()
                    plt.close()
                except Exception as e:
                    print(f"Error creating SHAP plot for {model_name}: {e}")

        # 5. تحليل البواقي
        plt.figure(figsize=(12, 8))
        for model_name in models:
            if 'residuals' in models[model_name]:
                sns.kdeplot(models[model_name]['residuals'], label=model_name)
        plt.axvline(0, color='black', linestyle='--')
        plt.title('Residual Distributions Comparison')
        plt.xlabel('Prediction Error (Actual - Predicted)')
        plt.legend()
        pdf.savefig()
        plt.close()

        # 6. القيم الفعلية vs المتوقعة
        plt.figure(figsize=(12, 8))
        for model_name in models:
            if 'predictions' in models[model_name]:
                plt.scatter(y_test, models[model_name]['predictions'],
                           alpha=0.5, label=model_name)
        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
                'k--', lw=2)
        plt.xlabel('Actual')
        plt.ylabel('Predicted')
        plt.title('Actual vs Predicted Values')
        plt.legend()
        pdf.savefig()
        plt.close()

# مثال استخدام مع تعريف جميع المتغيرات المطلوبة:
if __name__ == "__main__":
    # افترض أن لديك بيانات X_train, X_test, y_train, y_test
    # 1. تدريب نموذج Random Forest
    start_time = time.time()
    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
    rf_model.fit(X_train, y_train)
    rf_time = time.time() - start_time

    # 2. عمل التنبؤات
    rf_preds = rf_model.predict(X_test)

    # 3. حساب SHAP values
    try:
        explainer = shap.TreeExplainer(rf_model)
        rf_shap_values = explainer.shap_values(X_test)
    except:
        rf_shap_values = None

    # 4. إنشاء قاموس النماذج
    models_dict = {
        'Random Forest': {
            'model': rf_model,
            'metrics': {
                'R2': r2_score(y_test, rf_preds),
                'RMSE': np.sqrt(mean_squared_error(y_test, rf_preds)),
                'MAE': mean_absolute_error(y_test, rf_preds),
                'Training Time': f"{rf_time:.2f} seconds"
            },
            'feature_importance': pd.Series(rf_model.feature_importances_,
                                          index=X_train.columns),
            'residuals': y_test - rf_preds,
            'predictions': rf_preds,
            'shap_values': rf_shap_values,
            'pdp_features': list(X_train.columns[:3])  # أول 3 ميزات للعرض
        }
    }

    # 5. إنشاء التقرير
    create_model_report(models_dict, X_train, X_test, y_test)

import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import seaborn as sns
import pandas as pd
import numpy as np
import time
from sklearn.inspection import PartialDependenceDisplay
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import shap
from sklearn.ensemble import RandomForestRegressor

# مثال استخدام مع تعريف جميع المتغيرات المطلوبة:
if __name__ == "__main__":
    # افترض أن لديك بيانات X_train, X_test, y_train, y_test
    # 1. تدريب نموذج Random Forest
    start_time = time.time()
    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
    rf_model.fit(X_train, y_train)
    rf_time = time.time() - start_time

    # 2. عمل التنبؤات
    rf_preds = rf_model.predict(X_test)

    # 3. حساب SHAP values
    try:
        explainer = shap.TreeExplainer(rf_model)
        rf_shap_values = explainer.shap_values(X_test)
    except:
        rf_shap_values = None

    # 4. إنشاء قاموس النماذج
    models_dict = {
        'Random Forest': {
            'model': rf_model,
            'metrics': {
                'R2': r2_score(y_test, rf_preds),
                'RMSE': np.sqrt(mean_squared_error(y_test, rf_preds)),
                'MAE': mean_absolute_error(y_test, rf_preds),
                'Training Time': f"{rf_time:.2f} seconds"
            },
            'feature_importance': pd.Series(rf_model.feature_importances_,
                                          index=X_train.columns),
            'residuals': y_test - rf_preds,
            'predictions': rf_preds,
            'shap_values': rf_shap_values,
            'pdp_features': list(X_train.columns[:3])  # أول 3 ميزات للعرض
        }
    }

    # 5. إنشاء التقرير
    create_model_report(models_dict, X_train, X_test, y_test)

import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import seaborn as sns
import pandas as pd
import numpy as np
import time
from sklearn.inspection import PartialDependenceDisplay
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import shap

def create_model_report(models, X_train, X_test, y_test, filename='model_report.pdf'):
    """إنشاء تقرير PDF مقارنة بين نماذج التعلم الآلي"""
    with PdfPages(filename) as pdf:
        # صفحة العنوان
        plt.figure(figsize=(11, 8))
        plt.axis('off')
        plt.text(0.5, 0.7, 'Insurance Cost Prediction Report',
                ha='center', va='center', size=24)
        plt.text(0.5, 0.5, 'Comparative Analysis of Machine Learning Models',
                ha='center', va='center', size=18)
        plt.text(0.5, 0.3, f"Generated on {pd.Timestamp.now().strftime('%Y-%m-%d')}",
                ha='center', va='center', size=12)
        pdf.savefig()
        plt.close()

        # 1. مقارنة أداء النماذج
        plt.figure(figsize=(12, 6))
        metrics_df = pd.DataFrame.from_dict({m: models[m]['metrics'] for m in models}, orient='index')
        metrics_df = metrics_df.sort_values('R2', ascending=False)

        cell_text = []
        for row in metrics_df.itertuples():
            cell_text.append([f"{getattr(row, x):.4f}" if isinstance(getattr(row, x), (float, int))
                            else str(getattr(row, x)) for x in metrics_df.columns])

        plt.table(cellText=cell_text,
                 colLabels=metrics_df.columns,
                 rowLabels=metrics_df.index,
                 loc='center',
                 cellLoc='center',
                 colColours=['#f3f3f3']*len(metrics_df.columns),
                 rowColours=['#f3f3f3']*len(metrics_df))
        plt.axis('off')
        plt.title('Model Performance Comparison', pad=20)
        pdf.savefig()
        plt.close()

        # 2. مقارنة أهمية الميزات
        if any('feature_importance' in models[m] for m in models):
            plt.figure(figsize=(12, 8))
            for i, model_name in enumerate(models):
                if 'feature_importance' in models[model_name] and models[model_name]['feature_importance'] is not None:
                    plt.subplot(2, 2, i+1)
                    fi = models[model_name]['feature_importance'].sort_values(ascending=False).head(10)
                    sns.barplot(x=fi.values, y=fi.index, palette='viridis')
                    plt.title(f'{model_name} Feature Importance')
                    plt.xlabel('Importance Score')
            plt.tight_layout()
            pdf.savefig()
            plt.close()

        # 3. Partial Dependence Plots
        for model_name in models:
            if 'pdp_features' in models[model_name] and hasattr(models[model_name]['model'], 'predict'):
                try:
                    fig, ax = plt.subplots(figsize=(12, 8))
                    PartialDependenceDisplay.from_estimator(
                        models[model_name]['model'],
                        X_train,
                        models[model_name]['pdp_features'],
                        ax=ax,
                        n_jobs=-1
                    )
                    plt.suptitle(f'{model_name} Partial Dependence', y=1.02)
                    plt.tight_layout()
                    pdf.savefig()
                    plt.close()
                except Exception as e:
                    print(f"Error creating PDP for {model_name}: {e}")

        # 4. SHAP Plots
        for model_name in models:
            if 'shap_values' in models[model_name] and models[model_name]['shap_values'] is not None:
                try:
                    plt.figure(figsize=(12, 8))
                    shap.summary_plot(models[model_name]['shap_values'],
                                    X_test if 'processed_data' not in models[model_name] else models[model_name]['processed_data'],
                                    feature_names=models[model_name].get('feature_names', X_test.columns),
                                    show=False)
                    plt.title(f'{model_name} SHAP Summary', fontsize=16)
                    plt.gcf().set_facecolor('white')
                    pdf.savefig()
                    plt.close()
                except Exception as e:
                    print(f"Error creating SHAP plot for {model_name}: {e}")

        # 5. تحليل البواقي
        plt.figure(figsize=(12, 8))
        for model_name in models:
            if 'residuals' in models[model_name]:
                sns.kdeplot(models[model_name]['residuals'], label=model_name)
        plt.axvline(0, color='black', linestyle='--')
        plt.title('Residual Distributions Comparison')
        plt.xlabel('Prediction Error (Actual - Predicted)')
        plt.legend()
        pdf.savefig()
        plt.close()

        # 6. القيم الفعلية vs المتوقعة
        plt.figure(figsize=(12, 8))
        for model_name in models:
            if 'predictions' in models[model_name]:
                plt.scatter(y_test, models[model_name]['predictions'],
                           alpha=0.5, label=model_name)
        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
                'k--', lw=2)
        plt.xlabel('Actual')
        plt.ylabel('Predicted')
        plt.title('Actual vs Predicted Values')
        plt.legend()
        pdf.savefig()
        plt.close()

def create_pipeline():
    """إنشاء خط أنابيب معالجة البيانات والنموذج"""
    # تحديد الأعمدة النصية (الفئوية) والأعمدة الرقمية
    categorical_cols = ['sex', 'smoker', 'region']  # تعديل حسب بياناتك
    numeric_cols = ['age', 'bmi', 'children']  # تعديل حسب بياناتك

    # إنشاء معالج للبيانات
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', 'passthrough', numeric_cols),
            ('cat', OneHotEncoder(), categorical_cols)
        ])

    # إنشاء خط أنابيب يتضمن المعالج والنموذج
    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('model', RandomForestRegressor(n_estimators=100, random_state=42))
    ])

    return pipeline

def get_feature_importance(pipeline, original_columns):
    """استخراج أهمية الميزات لخط الأنابيب"""
    try:
        # للحصول على أسماء الميزات بعد التشفير
        if hasattr(pipeline.named_steps['preprocessor'], 'get_feature_names_out'):
            feature_names = pipeline.named_steps['preprocessor'].get_feature_names_out()
        else:
            # إذا لم يكن get_feature_names_out متاحاً
            numeric_features = pipeline.named_steps['preprocessor'].transformers_[0][2]
            categorical_features = pipeline.named_steps['preprocessor'].transformers_[1][1].get_feature_names_out(
                pipeline.named_steps['preprocessor'].transformers_[1][2])
            feature_names = list(numeric_features) + list(categorical_features)

        return pd.Series(pipeline.named_steps['model'].feature_importances_,
                        index=feature_names)
    except Exception as e:
        print(f"Error getting feature importance: {e}")
        return None

if __name__ == "__main__":
    # 1. تحميل البيانات
    data = pd.read_csv('insurance.csv')  # تعديل حسب مصدر بياناتك
    X = data.drop('charges', axis=1)
    y = data['charges']

    # 2. تقسيم البيانات
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # 3. إنشاء وتدريب خط الأنابيب
    pipeline = create_pipeline()
    start_time = time.time()
    pipeline.fit(X_train, y_train)
    rf_time = time.time() - start_time

    # 4. التنبؤ
    rf_preds = pipeline.predict(X_test)

    # 5. حساب SHAP values
    try:
        explainer = shap.TreeExplainer(pipeline.named_steps['model'])
        processed_data = pipeline.named_steps['preprocessor'].transform(X_test)
        rf_shap_values = explainer.shap_values(processed_data)
    except Exception as e:
        print(f"Error calculating SHAP values: {e}")
        rf_shap_values = None

    # 6. إنشاء قاموس النماذج
    models_dict = {
        'Random Forest': {
            'model': pipeline,
            'metrics': {
                'R2': r2_score(y_test, rf_preds),
                'RMSE': np.sqrt(mean_squared_error(y_test, rf_preds)),
                'MAE': mean_absolute_error(y_test, rf_preds),
                'Training Time': f"{rf_time:.2f} seconds"
            },
            'feature_importance': get_feature_importance(pipeline, X_train.columns),
            'residuals': y_test - rf_preds,
            'predictions': rf_preds,
            'shap_values': rf_shap_values,
            'processed_data': processed_data if rf_shap_values is not None else None,
            'feature_names': pipeline.named_steps['preprocessor'].get_feature_names_out(),
            'pdp_features': ['age', 'bmi', 'children']  # اختيار الميزات الرقمية فقط
        }
    }

    # 7. إنشاء التقرير
    create_model_report(models_dict, X_train, X_test, y_test)

import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import seaborn as sns
import pandas as pd
import numpy as np
import time
from sklearn.inspection import PartialDependenceDisplay
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import shap

def create_model_report(models, X_train, X_test, y_test, filename='model_report.pdf'):
    """إنشاء تقرير PDF مقارنة بين نماذج التعلم الآلي"""
    with PdfPages(filename) as pdf:
        # صفحة العنوان
        plt.figure(figsize=(11, 8))
        plt.axis('off')
        plt.text(0.5, 0.7, 'Insurance Cost Prediction Report',
                ha='center', va='center', size=24)
        plt.text(0.5, 0.5, 'Comparative Analysis of Machine Learning Models',
                ha='center', va='center', size=18)
        plt.text(0.5, 0.3, f"Generated on {pd.Timestamp.now().strftime('%Y-%m-%d')}",
                ha='center', va='center', size=12)
        pdf.savefig()
        plt.close()

        # 1. مقارنة أداء النماذج
        plt.figure(figsize=(12, 6))
        metrics_df = pd.DataFrame.from_dict({m: models[m]['metrics'] for m in models}, orient='index')
        metrics_df = metrics_df.sort_values('R2', ascending=False)

        cell_text = []
        # Get the list of column names to determine the index
        cols_list = metrics_df.columns.tolist()
        for row in metrics_df.itertuples(index=False): # Use index=False to get plain tuples
            cell_text.append([f"{row[cols_list.index(col)]:,.4f}" if isinstance(row[cols_list.index(col)], (float, int))
                            else str(row[cols_list.index(col)]) for col in metrics_df.columns])

        plt.table(cellText=cell_text,
                 colLabels=metrics_df.columns,
                 rowLabels=metrics_df.index,
                 loc='center',
                 cellLoc='center',
                 colColours=['#f3f3f3']*len(metrics_df.columns),
                 rowColours=['#f3f3f3']*len(metrics_df))
        plt.axis('off')
        plt.title('Model Performance Comparison', pad=20)
        pdf.savefig()
        plt.close()

        # 2. مقارنة أهمية الميزات
        if any('feature_importance' in models[m] for m in models):
            # Filter models that have feature importance
            models_with_fi = {m: models[m] for m in models if 'feature_importance' in models[m] and models[m]['feature_importance'] is not None}
            if models_with_fi:
                num_models_with_fi = len(models_with_fi)
                # Adjust subplot layout based on number of models with FI
                n_cols = min(2, num_models_with_fi)
                n_rows = (num_models_with_fi + n_cols - 1) // n_cols

                plt.figure(figsize=(6 * n_cols, 5 * n_rows)) # Adjust figure size dynamically
                for i, (model_name, model_data) in enumerate(models_with_fi.items()):
                    plt.subplot(n_rows, n_cols, i+1)
                    fi = model_data['feature_importance'].sort_values(ascending=False).head(10)
                    sns.barplot(x=fi.values, y=fi.index, palette='viridis')
                    plt.title(f'{model_name} Feature Importance')
                    plt.xlabel('Importance Score')
                plt.tight_layout()
                pdf.savefig()
                plt.close()


        # 3. Partial Dependence Plots
        for model_name in models:
            # Check if the model is a Pipeline and has a model step, and if pdp_features are specified
            if 'pdp_features' in models[model_name] and isinstance(models[model_name]['model'], Pipeline) and hasattr(models[model_name]['model'].named_steps['model'], 'predict'):
                try:
                    # Filter features for PDP based on existence in X_train
                    valid_pdp_features = [f for f in models[model_name]['pdp_features'] if f in X_train.columns]
                    if not valid_pdp_features:
                        print(f"Warning: No valid PDP features found in X_train for {model_name}. Skipping PDP plot.")
                        continue

                    # Determine subplot grid
                    num_pdp = len(valid_pdp_features)
                    n_cols = min(2, num_pdp)
                    n_rows = (num_pdp + n_cols - 1) // n_cols

                    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 5 * n_rows)) # Adjust figure size dynamically
                    axes = axes.flatten() if num_pdp > 1 else [axes] # Ensure axes is iterable

                    # Use the pipeline directly for PartialDependenceDisplay
                    PartialDependenceDisplay.from_estimator(
                        models[model_name]['model'],
                        X_train,
                        valid_pdp_features,
                        ax=axes,
                        n_jobs=-1,
                        # Add display_feature_names=True if using scikit-learn > 1.1
                        # display_feature_names=True
                    )
                    plt.suptitle(f'{model_name} Partial Dependence', y=1.02, fontsize=16)
                    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap
                    pdf.savefig()
                    plt.close()
                except Exception as e:
                    print(f"Error creating PDP for {model_name}: {e}")
            elif 'pdp_features' in models[model_name]:
                 print(f"Warning: PDP not supported for model type of {model_name} or pdp_features not specified. Skipping PDP plot.")


        # 4. SHAP Plots
        for model_name in models:
            if 'shap_values' in models[model_name] and models[model_name]['shap_values'] is not None:
                try:
                    plt.figure(figsize=(12, 8))
                    # Determine feature names for SHAP plot - prioritize provided names
                    shap_feature_names = models[model_name].get('feature_names', X_test.columns.tolist())
                    shap_data = models[model_name].get('processed_data', X_test)

                    # Ensure number of SHAP values features matches data features
                    if shap_data.shape[1] != models[model_name]['shap_values'].shape[1]:
                         print(f"Warning: Mismatch in feature count for {model_name} SHAP plot. Skipping.")
                         continue
                    # Ensure number of SHAP values features matches feature names
                    if models[model_name]['shap_values'].shape[1] != len(shap_feature_names):
                         print(f"Warning: Mismatch between SHAP features and provided feature names for {model_name}. Skipping SHAP plot.")
                         continue


                    shap.summary_plot(models[model_name]['shap_values'],
                                    shap_data,
                                    feature_names=shap_feature_names,
                                    show=False)
                    plt.title(f'{model_name} SHAP Summary', fontsize=16)
                    plt.gcf().set_facecolor('white')
                    pdf.savefig()
                    plt.close()
                except Exception as e:
                    print(f"Error creating SHAP plot for {model_name}: {e}")
            elif 'shap_values' in models[model_name]:
                print(f"Warning: SHAP values not available for {model_name}. Skipping SHAP plot.")

        # 5. تحليل البواقي
        plt.figure(figsize=(12, 8))
        residuals_present = False
        for model_name in models:
            if 'residuals' in models[model_name] and models[model_name]['residuals'] is not None:
                sns.kdeplot(models[model_name]['residuals'], label=model_name)
                residuals_present = True

        if residuals_present:
            plt.axvline(0, color='black', linestyle='--')
            plt.title('Residual Distributions Comparison')
            plt.xlabel('Prediction Error (Actual - Predicted)')
            plt.legend()
            pdf.savefig()
            plt.close()
        else:
             print("Warning: No residual data available for plotting.")


        # 6. القيم الفعلية vs المتوقعة
        plt.figure(figsize=(12, 8))
        predictions_present = False
        for model_name in models:
            if 'predictions' in models[model_name] and models[model_name]['predictions'] is not None:
                plt.scatter(y_test, models[model_name]['predictions'],
                           alpha=0.5, label=model_name)
                predictions_present = True

        if predictions_present:
            # Use min/max of combined actual and predicted values for plot limits
            all_values = pd.concat([y_test] + [models[m]['predictions'] for m in models if 'predictions' in models[m] and models[m]['predictions'] is not None])
            min_val, max_val = all_values.min(), all_values.max()
            plt.plot([min_val, max_val], [min_val, max_val],
                    'k--', lw=2)
            plt.xlabel('Actual')
            plt.ylabel('Predicted')
            plt.title('Actual vs Predicted Values')
            plt.legend()
            pdf.savefig()
            plt.close()
        else:
            print("Warning: No prediction data available for plotting.")

def create_pipeline():
    """إنشاء خط أنابيب معالجة البيانات والنموذج"""
    # تحديد الأعمدة النصية (الفئوية) والأعمدة الرقمية
    categorical_cols = ['sex', 'smoker', 'region']  # تعديل حسب بياناتك
    numeric_cols = ['age', 'bmi', 'children']  # تعديل حسب بياناتك

    # إنشاء معالج للبيانات
    # Keep numeric columns as is ('passthrough') is fine for this model before scaling if needed later
    # Or add a scaler here if it's part of your standard pipeline
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', 'passthrough', numeric_cols), # Using passthrough for numeric
            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols) # handle_unknown='ignore' for robustness
        ],
        remainder='passthrough' # Keep other columns if any
        )


    # إنشاء خط أنابيب يتضمن المعالج والنموذج
    # Add StandardScaler if your Random Forest benefits from scaled numeric features (less common but possible)
    # If using a model that requires scaling (like SVM, NN, Linear Models), add StandardScaler AFTER OneHotEncoder
    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        # Add a scaler here if needed for the model step
        # ('scaler', StandardScaler()), # Example if needed
        ('model', RandomForestRegressor(n_estimators=100, random_state=42))
    ])

    return pipeline

def get_feature_importance(pipeline, original_columns):
    """استخراج أهمية الميزات لخط الأنابيب"""
    try:
        model_step = pipeline.named_steps.get('model')
        preprocessor_step = pipeline.named_steps.get('preprocessor')

        if model_step and hasattr(model_step, 'feature_importances_'):
            # Get feature names after preprocessing
            if hasattr(preprocessor_step, 'get_feature_names_out'):
                feature_names = preprocessor_step.get_feature_names_out()
            else:
                 # Fallback for older scikit-learn versions or complex transformers
                 # This part is tricky and might need adjustment based on your specific preprocessor
                 numeric_features_processed = preprocessor_step.transformers_[0][2] # Assumes numeric is the first transformer
                 categorical_transformer = preprocessor_step.transformers_[1][1] # Assumes cat is the second
                 if hasattr(categorical_transformer, 'get_feature_names_out'):
                     categorical_features_processed = categorical_transformer.get_feature_names_out(preprocessor_step.transformers_[1][2])
                 else:
                     # Simple fallback: combine original numeric and generic names for OHE output
                     original_cat_cols = preprocessor_step.transformers_[1][2]
                     # Estimate number of OHE features (can be inaccurate)
                     num_ohe_features = preprocessor_step.transform(pd.DataFrame(columns=original_columns).iloc[:1]).shape[1] - len(numeric_features_processed)
                     categorical_features_processed = [f"OHE_{col}_{i}" for col in original_cat_cols for i in range(num_ohe_features // len(original_cat_cols))] # Very rough estimate
                     print("Warning: Using estimated categorical feature names. Consider updating scikit-learn or verifying get_feature_names_out.")

                 feature_names = list(numeric_features_processed) + list(categorical_features_processed)


            if len(feature_names) == len(model_step.feature_importances_):
                 return pd.Series(model_step.feature_importances_,
                                index=feature_names)
            else:
                 print(f"Warning: Mismatch between feature names count ({len(feature_names)}) and importance count ({len(model_step.feature_importances_)}). Cannot map importance to names.")
                 return None
        else:
            print("Warning: Model does not have feature_importances_ attribute or is not accessible via 'model' step.")
            return None
    except Exception as e:
        print(f"Error getting feature importance: {e}")
        return None


if __name__ == "__main__":
    # 1. تحميل البيانات
    data = pd.read_csv('insurance.csv')  # تعديل حسب مصدر بياناتك
    X = data.drop('charges', axis=1)
    y = data['charges']

    # 2. تقسيم البيانات
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # 3. إنشاء وتدريب خط الأنابيب
    pipeline = create_pipeline()
    start_time = time.time()
    pipeline.fit(X_train, y_train)
    rf_time = time.time() - start_time

    # 4. التنبؤ
    rf_preds = pipeline.predict(X_test)

    # 5. حساب SHAP values
    rf_shap_values = None
    processed_data_for_shap = None
    feature_names_for_shap = None
    try:
        # SHAP requires the model directly, not the pipeline, but needs data in the format the model expects
        model_for_shap = pipeline.named_steps['model']
        preprocessor_for_shap = pipeline.named_steps['preprocessor']
        processed_data_for_shap = preprocessor_for_shap.transform(X_test)

        # Get feature names after preprocessing for SHAP
        if hasattr(preprocessor_for_shap, 'get_feature_names_out'):
             feature_names_for_shap = preprocessor_for_shap.get_feature_names_out()
        else:
             # Fallback for older scikit-learn versions or complex transformers
             numeric_features_shap = preprocessor_for_shap.transformers_[0][2] # Assumes numeric is the first transformer
             categorical_transformer_shap = preprocessor_for_shap.transformers_[1][1] # Assumes cat is the second
             if hasattr(categorical_transformer_shap, 'get_feature_names_out'):
                  categorical_features_shap = categorical_transformer_shap.get_feature_names_out(preprocessor_for_shap.transformers_[1][2])
             else:
                 # Simple fallback: combine original numeric and generic names for OHE output
                 original_cat_cols_shap = preprocessor_for_shap.transformers_[1][2]
                 num_ohe_features_shap = processed_data_for_shap.shape[1] - len(numeric_features_shap)
                 categorical_features_shap = [f"OHE_{col}_{i}" for col in original_cat_cols_shap for i in range(num_ohe_features_shap // len(original_cat_cols_shap))] # Very rough estimate
                 print("Warning: Using estimated categorical feature names for SHAP. Consider updating scikit-learn or verifying get_feature_names_out.")

             feature_names_for_shap = list(numeric_features_shap) + list(categorical_features_shap)


        if hasattr(model_for_shap, 'tree_'): # Check if it's a tree model supported by TreeExplainer
            explainer = shap.TreeExplainer(model_for_shap)
            # Ensure processed_data_for_shap is a pandas DataFrame if TreeExplainer expects it
            if not isinstance(processed_data_for_shap, pd.DataFrame):
                 # Try to convert back to DataFrame with correct column names if possible
                 try:
                      processed_data_for_shap = pd.DataFrame(processed_data_for_shap, columns=feature_names_for_shap)
                 except Exception as e_df:
                     print(f"Could not convert processed data to DataFrame for SHAP: {e_df}. Using numpy array.")

            rf_shap_values = explainer.shap_values(processed_data_for_shap)
        else:
             print("Warning: Model is not tree-based. TreeExplainer requires a tree model. SHAP plot skipped.")

    except Exception as e:
        print(f"Error calculating SHAP values: {e}")
        rf_shap_values = None
        processed_data_for_shap = None
        feature_names_for_shap = None # Reset names if SHAP calculation failed


    # 6. إنشاء قاموس النماذج
    models_dict = {
        'Random Forest': {
            'model': pipeline, # Store the full pipeline
            'metrics': {
                'R2': r2_score(y_test, rf_preds),
                'RMSE': np.sqrt(mean_squared_error(y_test, rf_preds)),
                'MAE': mean_absolute_error(y_test, rf_preds),
                'Training Time': rf_time # Keep as float for potential future use
            },
            'feature_importance': get_feature_importance(pipeline, X_train.columns),
            'residuals': y_test - rf_preds,
            'predictions': rf_preds,
            'shap_values': rf_shap_values,
            'processed_data': processed_data_for_shap, # Pass processed data used for SHAP
            'feature_names': feature_names_for_shap, # Pass feature names used for SHAP
            # Select features for PDP - ensure they are original column names
            'pdp_features': ['age', 'bmi', 'smoker'] # Example features for PDP
        }
    }

    # 7. إنشاء التقرير
    create_model_report(models_dict, X_train, X_test, y_test)

all_values = pd.concat([y_test] + [models[m]['predictions'] for m in models if 'predictions' in models[m] and models[m]['predictions'] is not None])

import pandas as pd

# 1. تأكد من تعريف المتغيرات الأساسية أولاً
y_test = pd.Series([21000, 35000, 28000])  # مثال - استبدل ببياناتك الفعلية

# 2. تعريف قاموس models بشكل صحيح
models = {
    'Random Forest': {
        'predictions': pd.Series([20000, 34000, 27000]),  # مثال لتنبؤات النموذج
        # ... باقي خصائص النموذج
    },
    'Linear Regression': {
        'predictions': pd.Series([22000, 33000, 29000]),  # مثال لتنبؤات النموذج
        # ... باقي خصائص النموذج
    }
}

# 3. الآن يمكن تنفيذ الكود بأمان
try:
    all_values = pd.concat([y_test] + [models[m]['predictions'] for m in models
                          if 'predictions' in models[m] and models[m]['predictions'] is not None])
    print("تم دمج القيم بنجاح:")
    print(all_values)
except Exception as e:
    print(f"حدث خطأ أثناء محاولة دمج القيم: {e}")

all_values = pd.concat([y_test] + [models[m]['predictions'] for m in models if 'predictions' in models[m] and models[m]['predictions'] is not None])

import pandas as pd

# 1. تأكد من تعريف المتغيرات الأساسية أولاً
y_test = pd.Series([21000, 35000, 28000])  # مثال - استبدل ببياناتك الفعلية

# 2. تعريف قاموس models بشكل صحيح
models = {
    'Random Forest': {
        'predictions': pd.Series([20000, 34000, 27000]),  # مثال لتنبؤات النموذج
        # ... باقي خصائص النموذج
    },
    'Linear Regression': {
        'predictions': pd.Series([22000, 33000, 29000]),  # مثال لتنبؤات النموذج
        # ... باقي خصائص النموذج
    }
}

# 3. الآن يمكن تنفيذ الكود بأمان
try:
    all_values = pd.concat([y_test] + [models[m]['predictions'] for m in models
                          if 'predictions' in models[m] and models[m]['predictions'] is not None])
    print("تم دمج القيم بنجاح:")
    print(all_values)
except Exception as e:
    print(f"حدث خطأ أثناء محاولة دمج القيم: {e}")

import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import seaborn as sns
import pandas as pd
import numpy as np
from sklearn.inspection import PartialDependenceDisplay
import shap

def save_models_report_to_pdf(models_dict, X_train, X_test, y_test, filename='تقرير_النماذج.pdf'):
    """
    تقوم هذه الدالة بحفظ جميع الرسوم البيانية ومعلومات النماذج في ملف PDF واحد

    المعاملات:
    models_dict (dict): قاموس يحتوي على معلومات النماذج
    X_train (DataFrame): بيانات التدريب
    X_test (DataFrame): بيانات الاختبار
    y_test (Series): القيم الحقيقية
    filename (str): اسم ملف PDF المراد حفظه
    """

    # إعداد التنسيق العام
    plt.style.use('seaborn')
    plt.rcParams['font.family'] = 'Arial'  # خط عربي إذا كنت تريد دعم العربية
    plt.rcParams['axes.unicode_minus'] = False

    # إنشاء ملف PDF
    with PdfPages(filename) as pdf:

        # ----------------- صفحة الغلاف -----------------
        plt.figure(figsize=(11, 8))
        plt.axis('off')
        plt.text(0.5, 0.7, 'تقرير تحليل نماذج التأمين',
                ha='center', va='center', size=24)
        plt.text(0.5, 0.5, 'مقارنة بين خوارزميات التعلم الآلي',
                ha='center', va='center', size=18)
        plt.text(0.5, 0.3, f"تاريخ الإنشاء: {pd.Timestamp.now().strftime('%Y-%m-%d')}",
                ha='center', va='center', size=12)
        pdf.savefig()
        plt.close()

        # ----------------- صفحة الملخص التنفيذي -----------------
        plt.figure(figsize=(11, 8))
        plt.axis('off')

        # إنشاء جدول المقارنة
        metrics_data = []
        for model_name, model_info in models_dict.items():
            metrics_data.append([
                model_name,
                f"{model_info['metrics']['R2']:.4f}",
                f"{model_info['metrics']['RMSE']:.2f}",
                f"{model_info['metrics']['MAE']:.2f}",
                f"{model_info['metrics']['Training Time']:.2f} ثانية"
            ])

        columns = ['النموذج', 'R2 Score', 'RMSE', 'MAE', 'زمن التدريب']
        plt.table(cellText=metrics_data,
                 colLabels=columns,
                 loc='center',
                 cellLoc='center',
                 colColours=['#f3f3f3']*len(columns))

        plt.title('ملخص أداء النماذج', pad=20)
        pdf.savefig()
        plt.close()

        # ----------------- الرسوم البيانية لكل نموذج -----------------
        for model_name, model_info in models_dict.items():
            plt.figure(figsize=(11, 8))
            plt.axis('off')
            plt.text(0.5, 0.9, f'تحليل نموذج {model_name}',
                    ha='center', va='center', size=16)
            pdf.savefig()
            plt.close()

            # 1. أهمية الميزات
            if 'feature_importance' in model_info:
                plt.figure(figsize=(10, 6))
                fi = model_info['feature_importance'].sort_values(ascending=False).head(10)
                sns.barplot(x=fi.values, y=fi.index, palette='coolwarm')
                plt.title(f'أهمية الميزات - {model_name}')
                plt.xlabel('درجة الأهمية')
                plt.ylabel('الميزات')
                pdf.savefig()
                plt.close()

            # 2. القيم الفعلية vs المتوقعة
            plt.figure(figsize=(10, 6))
            plt.scatter(y_test, model_info['predictions'], alpha=0.5)
            plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
            plt.xlabel('القيم الفعلية')
            plt.ylabel('التوقعات')
            plt.title(f'القيم الفعلية vs المتوقعة - {model_name}')
            pdf.savefig()
            plt.close()

            # 3. توزيع الأخطاء
            plt.figure(figsize=(10, 6))
            sns.histplot(model_info['residuals'], kde=True, bins=30)
            plt.axvline(x=0, color='r', linestyle='--')
            plt.xlabel('الفرق بين القيم الفعلية والمتوقعة')
            plt.title(f'توزيع الأخطاء - {model_name}')
            pdf.savefig()
            plt.close()

            # 4. SHAP Values (إذا وجدت)
            if 'shap_values' in model_info:
                plt.figure(figsize=(10, 6))
                shap.summary_plot(model_info['shap_values'],
                                 model_info['processed_data'],
                                 feature_names=model_info['feature_names'],
                                 show=False)
                plt.title(f'تأثير الميزات (SHAP) - {model_name}')
                pdf.savefig()
                plt.close()

        # ----------------- صفحة الخاتمة -----------------
        plt.figure(figsize=(11, 8))
        plt.axis('off')

        best_model = max(models_dict.items(), key=lambda x: x[1]['metrics']['R2'])
        conclusion_text = [
            "الخلاصة والتوصيات:",
            "",
            f"أفضل نموذج كان: {best_model[0]} مع R2 = {best_model[1]['metrics']['R2']:.4f}",
            "",
            "الميزات الأكثر تأثيراً:",
            *[f"- {fi}" for fi in models_dict[best_model[0]]['feature_importance'].nlargest(3).index.tolist()],
            "",
            "التوصيات:",
            "- التركيز على الميزات الأكثر تأثيراً في التحليلات المستقبلية",
            "- تحسين جمع البيانات للميزات المهمة",
            "- تجربة نماذج أكثر تعقيداً لتحسين الأداء"
        ]

        plt.text(0.1, 0.9, "\n".join(conclusion_text),
                ha='left', va='top', size=12)
        pdf.savefig()
        plt.close()

# طريقة الاستخدام:
# 1. تدريب النماذج وحفظ النتائج في قاموس
models_dict = {
    'Random Forest': {
        'model': rf_model,
        'metrics': {
            'R2': r2_score(y_test, rf_preds),
            'RMSE': mean_squared_error(y_test, rf_preds, squared=False),
            'MAE': mean_absolute_error(y_test, rf_preds),
            'Training Time': rf_time
        },
        'feature_importance': pd.Series(rf_model.feature_importances_,
                                      index=feature_names),
        'residuals': y_test - rf_preds,
        'predictions': rf_preds,
        'shap_values': rf_shap_values,
        'processed_data': rf_processed_data,
        'feature_names': feature_names
    },
    # إضافة نماذج أخرى بنفس الطريقة
}

# 2. استدعاء الدالة لحفظ التقرير
save_models_report_to_pdf(models_dict, X_train, X_test, y_test, 'تقرير_نماذج_التأمين.pdf')

import numpy as np

from math import sqrt
RMSE = sqrt(mean_squared_error(y_test, rf_preds))

# في قسم مقاييس النموذج:
models_dict = {
    'Random Forest': {
        'model': rf_model,
        'metrics': {
            'R2': r2_score(y_test, rf_preds),
            'RMSE': np.sqrt(mean_squared_error(y_test, rf_preds)),  # التعديل هنا
            'MAE': mean_absolute_error(y_test, rf_preds),
            'Training Time': rf_time
        },
        'feature_importance': pd.Series(rf_model.feature_importances_,
                                      index=feature_names),
        'residuals': y_test - rf_preds,
        'predictions': rf_preds,
        'shap_values': rf_shap_values,
        'processed_data': rf_processed_data,
        'feature_names': feature_names
    }
}

# في قسم مقاييس النموذج:
models_dict = {
    'Random Forest': {
        'model': rf_model,
        'metrics': {
            'R2': r2_score(y_test, rf_preds),
            'RMSE': np.sqrt(mean_squared_error(y_test, rf_preds)),
            'MAE': mean_absolute_error(y_test, rf_preds),
            'Training Time': rf_time
        },
        'feature_importance': pd.Series(rf_model.feature_importances_,
                                      index=feature_names),
        'residuals': y_test - rf_preds,
        'predictions': rf_preds,
        'shap_values': rf_shap_values,
        # التعديل هنا: استخدام processed_data بدلاً من rf_processed_data
        'processed_data': processed_data,
        'feature_names': feature_names
    }
}

import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import seaborn as sns
import pandas as pd
import numpy as np
from sklearn.inspection import PartialDependenceDisplay
import shap
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error # Ensure these are imported

def save_models_report_to_pdf(models_dict, X_train, X_test, y_test, filename='تقرير_النماذج.pdf'):
    """
    تقوم هذه الدالة بحفظ جميع الرسوم البيانية ومعلومات النماذج في ملف PDF واحد

    المعاملات:
    models_dict (dict): قاموس يحتوي على معلومات النماذج
    X_train (DataFrame): بيانات التدريب
    X_test (DataFrame): بيانات الاختبار
    y_test (Series): القيم الحقيقية
    filename (str): اسم ملف PDF المراد حفظه
    """

    # إعداد التنسيق العام
    plt.style.use('seaborn')
    plt.rcParams['font.family'] = 'Arial'  # خط عربي إذا كنت تريد دعم العربية
    plt.rcParams['axes.unicode_minus'] = False

    # إنشاء ملف PDF
    with PdfPages(filename) as pdf:

        # ----------------- صفحة الغلاف -----------------
        plt.figure(figsize=(11, 8))
        plt.axis('off')
        plt.text(0.5, 0.7, 'تقرير تحليل نماذج التأمين',
                ha='center', va='center', size=24)
        plt.text(0.5, 0.5, 'مقارنة بين خوارزميات التعلم الآلي',
                ha='center', va='center', size=18)
        plt.text(0.5, 0.3, f"تاريخ الإنشاء: {pd.Timestamp.now().strftime('%Y-%m-%d')}",
                ha='center', va='center', size=12)
        pdf.savefig()
        plt.close()

        # ----------------- صفحة الملخص التنفيذي -----------------
        plt.figure(figsize=(11, 8))
        plt.axis('off')

        # إنشاء جدول المقارنة
        metrics_data = []
        for model_name, model_info in models_dict.items():
            # Check if 'metrics' and required keys exist
            if 'metrics' in model_info and all(m in model_info['metrics'] for m in ['R2', 'RMSE', 'MAE', 'Training Time']):
                metrics_data.append([
                    model_name,
                    f"{model_info['metrics']['R2']:.4f}",
                    f"{model_info['metrics']['RMSE']:.2f}",
                    f"{model_info['metrics']['MAE']:.2f}",
                    f"{model_info['metrics']['Training Time']:.2f} ثانية"
                ])
            else:
                print(f"Warning: Missing metric data for {model_name}. Skipping in summary table.")


        columns = ['النموذج', 'R2 Score', 'RMSE', 'MAE', 'زمن التدريب']
        plt.table(cellText=metrics_data,
                 colLabels=columns,
                 loc='center',
                 cellLoc='center',
                 colColours=['#f3f3f3']*len(columns))

        plt.title('ملخص أداء النماذج', pad=20)
        pdf.savefig()
        plt.close()

        # ----------------- الرسوم البيانية لكل نموذج -----------------
        for model_name, model_info in models_dict.items():
            # Ensure 'predictions' is available for plotting
            if 'predictions' not in model_info or model_info['predictions'] is None:
                print(f"Skipping plots for {model_name}: No predictions available.")
                continue

            plt.figure(figsize=(11, 8))
            plt.axis('off')
            plt.text(0.5, 0.9, f'تحليل نموذج {model_name}',
                    ha='center', va='center', size=16)
            pdf.savefig()
            plt.close()

            # 1. أهمية الميزات
            if 'feature_importance' in model_info and model_info['feature_importance'] is not None:
                plt.figure(figsize=(10, 6))
                fi = model_info['feature_importance'].sort_values(ascending=False).head(10)
                sns.barplot(x=fi.values, y=fi.index, palette='coolwarm')
                plt.title(f'أهمية الميزات - {model_name}')
                plt.xlabel('درجة الأهمية')
                plt.ylabel('الميزات')
                pdf.savefig()
                plt.close()
            else:
                print(f"Skipping feature importance plot for {model_name}: No importance data.")


            # 2. القيم الفعلية vs المتوقعة
            plt.figure(figsize=(10, 6))
            plt.scatter(y_test, model_info['predictions'], alpha=0.5)
            # Ensure the line goes from the minimum of actual/predicted to the maximum
            min_val = min(y_test.min(), model_info['predictions'].min())
            max_val = max(y_test.max(), model_info['predictions'].max())
            plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2)
            plt.xlabel('القيم الفعلية')
            plt.ylabel('التوقعات')
            plt.title(f'القيم الفعلية vs المتوقعة - {model_name}')
            pdf.savefig()
            plt.close()

            # 3. توزيع الأخطاء
            if 'residuals' in model_info and model_info['residuals'] is not None:
                plt.figure(figsize=(10, 6))
                sns.histplot(model_info['residuals'], kde=True, bins=30)
                plt.axvline(x=0, color='r', linestyle='--')
                plt.xlabel('الفرق بين القيم الفعلية والمتوقعة')
                plt.title(f'توزيع الأخطاء - {model_name}')
                pdf.savefig()
                plt.close()
            else:
                print(f"Skipping residuals plot for {model_name}: No residuals data.")

            # 4. SHAP Values (إذا وجدت)
            if 'shap_values' in model_info and model_info['shap_values'] is not None and \
               'processed_data' in model_info and model_info['processed_data'] is not None and \
               'feature_names' in model_info and model_info['feature_names'] is not None:
                try:
                    plt.figure(figsize=(10, 6))
                    shap.summary_plot(model_info['shap_values'],
                                     model_info['processed_data'],
                                     feature_names=model_info['feature_names'],
                                     show=False, plot_type="dot") # Specify plot_type for consistency
                    plt.title(f'تأثير الميزات (SHAP) - {model_name}')
                    pdf.savefig()
                    plt.close()
                except Exception as e:
                     print(f"Error generating SHAP plot for {model_name}: {e}")
                     plt.close() # Close the figure if an error occurred
            else:
                 print(f"Skipping SHAP plot for {model_name}: Missing SHAP data or associated information.")


        # ----------------- صفحة الخاتمة -----------------
        plt.figure(figsize=(11, 8))
        plt.axis('off')

        # Find the best model based on R2
        best_model_name = None
        best_r2 = -np.inf
        for name, info in models_dict.items():
            if 'metrics' in info and 'R2' in info['metrics']:
                if info['metrics']['R2'] > best_r2:
                    best_r2 = info['metrics']['R2']
                    best_model_name = name

        conclusion_text = [
            "الخلاصة والتوصيات:",
            "",
        ]

        if best_model_name and 'metrics' in models_dict[best_model_name] and 'feature_importance' in models_dict[best_model_name]:
             best_model_info = models_dict[best_model_name]
             conclusion_text.append(f"أفضل نموذج كان: {best_model_name} مع R2 = {best_model_info['metrics']['R2']:.4f}")
             conclusion_text.append("")
             conclusion_text.append("الميزات الأكثر تأثيراً:")
             # Ensure feature importance is a Series and not None
             if isinstance(best_model_info['feature_importance'], pd.Series):
                top_features = best_model_info['feature_importance'].nlargest(3).index.tolist()
                conclusion_text.extend([f"- {fi}" for fi in top_features])
             else:
                 conclusion_text.append("معلومات أهمية الميزات غير متوفرة.")

             conclusion_text.extend([
                 "",
                 "التوصيات:",
                 "- التركيز على الميزات الأكثر تأثيراً في التحليلات المستقبلية",
                 "- تحسين جمع البيانات للميزات المهمة",
                 "- تجربة نماذج أكثر تعقيداً لتحسين الأداء"
             ])
        else:
            conclusion_text.append("لا يمكن تحديد أفضل نموذج أو عرض التوصيات بسبب نقص البيانات.")


        plt.text(0.1, 0.9, "\n".join(conclusion_text),
                ha='left', va='top', size=12)
        pdf.savefig()
        plt.close()

# طريقة الاستخدام:
# 1. تدريب النماذج وحفظ النتائج في قاموس
#    تأكد من أن rf_model هو كائن النموذج المدرب (pipeline أو regressor)
#    تأكد من تعريف rf_preds, rf_time, rf_shap_values, rf_processed_data, feature_names بشكل صحيح قبل هذا الجزء
#    مثال لتعريف القيم المطلوبة (استبدلها بقيمك الحقيقية من تدريب النماذج):
#    rf_preds = rf_model.predict(X_test) # إذا كان rf_model هو pipeline
#    rf_time = 1.5 # الزمن الذي استغرقه تدريب rf_model
#    rf_shap_values = None # احسبها إذا كنت تستخدم نموذجاً يدعم SHAP
#    rf_processed_data = None # البيانات بعد المعالجة إذا كنت تستخدم SHAP
#    feature_names = X_train.columns.tolist() # أو أسماء الميزات بعد المعالجة

#    Example placeholder values - Replace with your actual results:
#    rf_preds = pd.Series(np.random.rand(len(y_test)) * 50000 + 10000)
#    rf_model = None # You need to pass the actual trained model object
#    rf_time = 0.75
#    # Calculate feature importance if rf_model is available and has the attribute
#    if rf_model and hasattr(rf_model, 'feature_importances_'):
#        rf_feature_importance = pd.Series(rf_model.feature_importances_, index=feature_names)
#    elif rf_model and hasattr(rf_model, 'named_steps') and 'model' in rf_model.named_steps and hasattr(rf_model.named_steps['model'], 'feature_importances_'):
#         # Handle if rf_model is a pipeline and the regressor step has feature_importances_
#         # You would need to get the feature names after preprocessing correctly here
#         # This requires getting feature names from the preprocessor's OneHotEncoder
#         # For simplicity in this fix, we'll create a dummy importance if not available
#         print("Warning: Feature importance calculation for pipeline is more complex. Using dummy data.")
#         rf_feature_importance = pd.Series(np.random.rand(len(feature_names)), index=feature_names)
#    else:
#        print("Warning: rf_model does not have feature_importances_. Skipping feature importance for RF.")
#        rf_feature_importance = None
#    rf_residuals = y_test - rf_preds
#    rf_shap_values = None # Calculate this if using SHAP
#    rf_processed_data = None # Data used for SHAP calculation


#    Let's assume you have a trained `rf_model` pipeline or regressor and calculated `rf_preds`, `rf_time` etc.

#    Calculate metrics using the predicted values
#    rf_r2 = r2_score(y_test, rf_preds)
#    rf_mse = mean_squared_error(y_test, rf_preds)
#    rf_rmse = np.sqrt(rf_mse) # Correct way to calculate RMSE
#    rf_mae = mean_absolute_error(y_test, rf_preds)


#    # Dummy data for demonstration if actual data is not available
#    try:
#        # Try to use actual calculated values if they exist
#        rf_preds = rf_preds # Assuming rf_preds is defined globally
#        rf_time = rf_time   # Assuming rf_time is defined globally
#        # Calculate metrics from globally defined variables
#        rf_r2 = r2_score(y_test, rf_preds)
#        rf_mse = mean_squared_error(y_test, rf_preds)
#        rf_rmse = np.sqrt(rf_mse)
#        rf_mae = mean_absolute_error(y_test, rf_preds)
#        rf_residuals = y_test - rf_preds
#        # Attempt to get feature importance if rf_model is defined and is a pipeline
#        if 'rf_model' in globals() and isinstance(rf_model, Pipeline):
#            # This part is complex and depends on your preprocessor structure
#            # Assuming preprocessor has named transformers 'num' and 'cat'
#            preprocessor_step = rf_model.named_steps['preprocessing'].named_steps['preprocessor']
#            numeric_features_list = preprocessor_step.transformers_[0][2] # Assuming numeric is the first transformer
#            categorical_transformer_step = preprocessor_step.named_transformers_['cat']
#            if 'encoder' in categorical_transformer_step.named_steps:
#                 onehot_encoder = categorical_transformer_step.named_steps['encoder']
#                 # You need the original categorical feature names here, e.g., ['sex', 'smoker', 'region']
#                 original_cat_features = ['sex', 'smoker', 'region'] # Replace with your actual names
#                 encoded_cat_features = onehot_encoder.get_feature_names_out(original_cat_features)
#                 processed_feature_names_list = numeric_features_list + list(encoded_cat_features)
#            else:
#                 print("Warning: OneHotEncoder not found in the pipeline's preprocessor. Cannot get detailed feature names.")
#                 processed_feature_names_list = feature_names # Use original names as a fallback

#            # Get feature importance from the model step (assuming it's named 'model')
#            model_step = rf_model.named_steps['model']
#            if hasattr(model_step, 'feature_importances_'):
#                rf_feature_importance = pd.Series(model_step.feature_importances_, index=processed_feature_names_list)
#            else:
#                print(f"Warning: Model step {type(model_step).__name__} does not have feature_importances_. Skipping.")
#                rf_feature_importance = None

#            # Calculate SHAP values if the model supports it and you have the processed data
#            # This is complex and requires matching SHAP values to processed features
#            rf_shap_values = None # Requires specific SHAP setup
#            rf_processed_data = None # Requires specific SHAP setup

#        elif 'rf_model' in globals() and hasattr(rf_model, 'feature_importances_'):
#            # Handle if rf_model is a regressor directly (not in a pipeline)
#            rf_feature_importance = pd.Series(rf_model.feature_importances_, index=feature_names)
#            rf_shap_values = None # Requires specific SHAP setup
#            rf_processed_data = None # Requires specific SHAP setup
#        else:
#            print("Warning: rf_model not found or doesn't support feature_importances_. Skipping.")
#            rf_feature_importance = None
#            rf_shap_values = None
#            rf_processed_data = None

#    except NameError as e:
#        print(f"Warning: Required variables for RF model (rf_preds, rf_time, etc.) not found. Using dummy data for models_dict. Error: {e}")
#        # Use dummy data if actual variables are not defined
#        rf_preds = pd.Series(np.random.rand(len(y_test)) * 50000 + 10000)
#        rf_r2, rf_rmse, rf_mae = 0.5, 5000, 4000 # Dummy metrics
#        rf_time = 1.0
#        rf_feature_importance = pd.Series(np.random.rand(len(X_train.columns)), index=X_train.columns)
#        rf_residuals = y_test - rf_preds
#        rf_shap_values = None
#        rf_processed_data = None
#        feature_names = X_train.columns.tolist() # Use original names if processed not available


#    # Assuming you have trained other models (lr_model, svm_model, xgb_model, etc.)
#    # and calculated their predictions, metrics, times, importances etc.
#    # For this example, we'll just include RF with the corrected RMSE calculation

#    # Define models_dict using your actual calculated variables
#    # Note: You must calculate rf_preds, rf_time, etc. *before* this dict is created.
#    # You must also ensure feature_names or the correct processed feature names are available.
#    # Assuming rf_preds, rf_time, feature_names are available from previous cells

#    # Calculate metrics for RF using the globally defined variables
#    rf_r2 = r2_score(y_test, rf_preds)
#    rf_mse = mean_squared_error(y_test, rf_preds)
#    rf_rmse = np.sqrt(rf_mse) # Correct way to calculate RMSE
#    rf_mae = mean_absolute_error(y_test, rf_preds)
#    rf_residuals = y_test - rf_preds


#    # Get feature names after preprocessing from the pipeline used for RF
#    # Assumes rf_model is the trained pipeline
#    try:
#        preprocessor_step = rf_model.named_steps['preprocessing'].named_steps['preprocessor']
#        # Assuming numeric is the first transformer
#        numeric_features_list = preprocessor_step.transformers_[0][2]
#        categorical_transformer_step = preprocessor_step.named_transformers_['cat']
#        if 'encoder' in categorical_transformer_step.named_steps:
#            onehot_encoder = categorical_transformer_step.named_steps['encoder']
#            # You need the original categorical feature names here, e.g., ['sex', 'smoker', 'region']
#            # Let's assume your original categorical features were ['sex', 'smoker', 'region']
#            original_cat_features = ['sex', 'smoker', 'region'] # *** ADJUST THIS TO YOUR ACTUAL CAT FEATURE NAMES ***
#            encoded_cat_features = onehot_encoder.get_feature_names_out(original_cat_features)
#            processed_feature_names_list = numeric_features_list + list(encoded_cat_features)
#        else:
#            print("Warning: OneHotEncoder not found in the pipeline's preprocessor. Cannot get detailed feature names for importance.")
#            processed_feature_names_list = feature_names # Fallback to original names (may not match importance array size)

#        # Get feature importance from the model step (assuming it's named 'model')
#        model_step = rf_model.named_steps['model']
#        if hasattr(model_step, 'feature_importances_'):
#            rf_feature_importance = pd.Series(model_step.feature_importances_, index=processed_feature_names_list)
#        else:
#             print(f"Warning: Model step {type(model_step).__name__} does not have feature_importances_ attribute.")
#             rf_feature_importance = None
#    except Exception as e:
#        print(f"Error extracting feature names or importance from RF pipeline: {e}. Skipping feature importance plot for RF.")
#        rf_feature_importance = None
#        processed_feature_names_list = feature_names # Fallback


#    # Placeholder for SHAP values and processed data (you need to calculate these if you want the SHAP plot)
#    rf_shap_values = None
#    rf_processed_data = None # Data that SHAP was calculated on (likely X_test after preprocessing)


#    # Add your other models and their calculated results to this dict
#    models_dict = {
#        'Random Forest': {
#            'model': rf_model, # Pass the actual trained model object
#            'metrics': {
#                'R2': rf_r2,
#                'RMSE': rf_rmse, # Corrected RMSE
#                'MAE': rf_mae,
#                'Training Time': rf_time
#            },
#            'feature_importance': rf_feature_importance, # The calculated feature importance Series
#            'residuals': rf_residuals,
#            'predictions': rf_preds,
#            'shap_values': rf_shap_values, # Calculated SHAP values
#            'processed_data': rf_processed_data, # Processed data used for SHAP
#            'feature_names': processed_feature_names_list if rf_feature_importance is not None else feature_names # Use processed names if available
#        },
#        # Example for another model (replace with your actual model and results)
#        # 'Linear Regression': {
#        #     'model': lr_model,
#        #     'metrics': {
#        #         'R2': lr_r2,
#        #         'RMSE': np.sqrt(mean_squared_error(y_test, lr_preds)),
#        #         'MAE': mean_absolute_error(y_test, lr_preds),
#        #         'Training Time': lr_time
#        #     },
#        #     'feature_importance': lr_feature_importance, # Calculated importance
#        #     'residuals': y_test - lr_preds,
#        #     'predictions': lr_preds,
#        #     'shap_values': lr_shap_values, # Calculated SHAP values
#        #     'processed_data': lr_processed_data, # Processed data used for SHAP
#        #     'feature_names': lr_feature_names # Feature names for this model's importance/SHAP
#        # },
#        # Add other models here
#    }

#    # Re-run the code block where models_dict is defined with your actual model training and result calculations

#    # Example of how to *define* rf_model (replace with your actual pipeline/regressor)
#    # from sklearn.ensemble import RandomForestRegressor
#    # from sklearn.pipeline import Pipeline
#    # from sklearn.compose import ColumnTransformer
#    # from sklearn.preprocessing import StandardScaler, OneHotEncoder

#    # Define preprocessor (example - match yours)
#    # numeric_features = ['age', 'bmi', 'children']
#    # categorical_features = ['sex', 'smoker', 'region']
#    # preprocessor = ColumnTransformer(
#    #     transformers=[
#    #         ('num', StandardScaler(), numeric_features),
#    #         ('cat', OneHotEncoder(), categorical_features)
#    #     ])

#    # Define the model pipeline (example - match yours)
#    # rf_model = Pipeline(steps=[('preprocessing', preprocessor),
#    #                           ('model', RandomForestRegressor(n_estimators=100, random_state=42))])

#    # Train the model (example)
#    # rf_model.fit(X_train, y_train)

#    # Make predictions (example)
#    # rf_preds = rf_model.predict(X_test)

#    # Calculate training time (example)
#    # import time
#    # start_time = time.time()
#    # rf_model.fit(X_train, y_train)
#    # end_time = time.time()
#    # rf_time = end_time - start_time

#    # Calculate other metrics (example)
#    # rf_r2 = r2_score(y_test, rf_preds)
#    # rf_mse = mean_squared_error(y_test, rf_preds)
#    # rf_rmse = np.sqrt(rf_mse)
#    # rf_mae = mean_absolute_error(y_test, rf_preds)
#    # rf_residuals = y_test - rf_preds

#    # Get feature importance (example, requires rf_model to be a Pipeline with 'model' step)
#    # if hasattr(rf_model.named_steps['model'], 'feature_importances_'):
#    #     # You need to get the processed feature names correctly here
#    #     # This is the complex part, depending on your preprocessor
#    #     try:
#    #         preprocessor_step = rf_model.named_steps['preprocessing']
#    #         # Assuming 'preprocessor' is the ColumnTransformer inside the 'preprocessing' step
#    #         col_transformer = preprocessor_step.named_steps['preprocessor']
#    #         numeric_features_list = col_transformer.transformers_[0][2] # Adjust index if needed
#    #         # Assuming 'cat' transformer is named 'cat'
#    #         cat_transformer_step = col_transformer.named_transformers_['cat']
#    #         if 'encoder' in cat_transformer_step.named_steps:
#    #              onehot_encoder = cat_transformer_step.named_steps['encoder']
#    #              # You need the original categorical feature names that were passed to the ColumnTransformer
#    #              # Example: original_cat_features = ['sex', 'smoker', 'region'] # *** ADJUST THIS ***
#    #              # Find the original categorical features from the ColumnTransformer definition if possible
#    #              original_cat_features = [t[2] for t in col_transformer.transformers_ if t[0] == 'cat'][0]
#    #              encoded_cat_features = onehot_encoder.get_feature_names_out(original_cat_features)
#    #              processed_feature_names_list = numeric_features_list + list(encoded_cat_features)
#    #         else:
#    #             print("Warning: OneHotEncoder not found in the pipeline's preprocessor. Using original feature names as fallback.")
#    #             processed_feature_names_list = X_train.columns.tolist()
#    #
#    #         rf_feature_importance = pd.Series(rf_model.named_steps['model'].feature_importances_, index=processed_feature_names_list)
#    #     except Exception as e:
#    #          print(f"Error getting processed feature names or importance: {e}. Skipping importance plot.")
#    #          rf_feature_importance = None
#    #          processed_feature_names_list = X_train.columns.tolist() # Fallback
#    # else:
#    #     print("Warning: RF model step does not have feature_importances_ attribute. Skipping importance plot.")
#    #     rf_feature_importance = None
#    #     processed_feature_names_list = X_train.columns.tolist() # Fallback

#    # Calculate SHAP values (example, requires trained model and processed data)
#    # if rf_model is not None and hasattr(rf_model.named_steps['model'], 'feature_importances_'): # Check if tree-based model
#    #     try:
#    #         # Assuming rf_model is your pipeline
#    #         processed_data_test = rf_model.named_steps['preprocessing'].transform(X_test)
#    #         explainer = shap.TreeExplainer(rf_model.named_steps['model'])
#    #         rf_shap_values = explainer.shap_values(processed_data_test)
#    #         rf_processed_data = processed_data_test # Store processed data used for SHAP
#    #         # Ensure processed_feature_names_list is correctly defined from preprocessor
#    #         # (Same logic as getting feature importance names)
#    #         # If not already defined, calculate here:
#    #         if 'processed_feature_names_list' not in locals() or processed_feature_names_list is None:
#    #              # Recalculate processed_feature_names_list if needed
#    #              try:
#    #                  col_transformer = rf_model.named_steps['preprocessing'].named_steps['preprocessor']
#    #                  numeric_features_list = col_transformer.transformers_[0][2]
#    #                  cat_transformer_step = col_transformer.named_transformers_['cat']
#    #                  if 'encoder' in cat_transformer_step.named_steps:
#    #                       onehot_encoder = cat_transformer_step.named_steps['encoder']
#    #                       original_cat_features = [t[2] for t in col_transformer.transformers_ if t[0] == 'cat'][0]
#    #                       encoded_cat_features = onehot_encoder.get_feature_names_out(original_cat_features)
#    #                       processed_feature_names_list = numeric_features_list + list(encoded_cat_features)
#    #                  else:
#    #                       print("Warning: OneHotEncoder not found for SHAP feature names. Using original names.")
#    #                       processed_feature_names_list = X_test.columns.tolist() # Fallback
#    #              except Exception as e:
#    #                  print(f"Error getting processed feature names for SHAP: {e}. Using original names.")
#    #                  processed_feature_names_list = X_test.columns.tolist() # Fallback

#    #     except Exception as e:
#    #          print(f"Error calculating SHAP values for RF: {e}. Skipping SHAP plot.")
#    #          rf_shap_values = None
#    #          rf_processed_data = None
#    # else:
#    #      print("RF model does not support SHAP TreeExplainer or is not defined. Skipping SHAP plot.")
#    #      rf_shap_values = None
#    #      rf_processed_data = None
#    #      processed_feature_names_list = X_test.columns.tolist() # Fallback

#    # After training your models and calculating all necessary variables (preds, metrics, times, importances, SHAP etc.)
#    # populate the models_dict like this:

# Assuming you have the following variables defined and calculated from your model training cells:
# - X_train, X_test, y_test (DataFrames/Series)
# - rf_model (the trained RandomForestRegressor pipeline or regressor)
# - rf_preds (predictions from rf_model on X_test)
# - rf_time (training time for rf_model)
# - rf_feature_importance (Pandas Series of feature importances or None)
# - rf_shap_values (NumPy array of SHAP values or None)
# - rf_processed_data (NumPy array of X_test after preprocessing, used for SHAP, or None)
# - processed_feature_names_list (List of feature names after preprocessing, or None/fallback)

# Example of populating the dict (replace placeholders with your actual variables):
models_dict = {
    'Random Forest': {
        'model': rf_model, # Your trained model object
        'metrics': {
            'R2': r2_score(y_test, rf_preds),
            'RMSE': np.sqrt(mean_squared_error(y_test, rf_preds)), # Corrected RMSE calculation
            'MAE': mean_absolute_error(y_test, rf_preds),
            'Training Time': rf_time
        },
        'feature_importance': rf_feature_importance if 'rf_feature_importance' in locals() else None, # Ensure this variable exists or is None
        'residuals': y_test - rf_preds,
        'predictions': rf_preds,
        'shap_values': rf_shap_values if 'rf_shap_values' in locals() else None, # Ensure this variable exists or is None
        'processed_data': rf_processed_data if 'rf_processed_data' in locals() else None, # Ensure this variable exists or is None
        'feature_names': processed_feature_names_list if 'processed_feature_names_list' in locals() and processed_feature_names_list is not None else X_test.columns.tolist() # Use processed names if available, else original
    },
    # Add other models you trained here following the same structure
    # Example:
    # 'Linear Regression

import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import seaborn as sns
import pandas as pd
import numpy as np
from sklearn.inspection import PartialDependenceDisplay
import shap
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error # Ensure these are imported

def save_models_report_to_pdf(models_dict, X_train, X_test, y_test, filename='تقرير_النماذج.pdf'):
    """
    تقوم هذه الدالة بحفظ جميع الرسوم البيانية ومعلومات النماذج في ملف PDF واحد

    المعاملات:
    models_dict (dict): قاموس يحتوي على معلومات النماذج
    X_train (DataFrame): بيانات التدريب
    X_test (DataFrame): بيانات الاختبار
    y_test (Series): القيم الحقيقية
    filename (str): اسم ملف PDF المراد حفظه
    """

    # إعداد التنسيق العام
    plt.style.use('seaborn')
    plt.rcParams['font.family'] = 'Arial'  # خط عربي إذا كنت تريد دعم العربية
    plt.rcParams['axes.unicode_minus'] = False

    # إنشاء ملف PDF
    with PdfPages(filename) as pdf:

        # ----------------- صفحة الغلاف -----------------
        plt.figure(figsize=(11, 8))
        plt.axis('off')
        plt.text(0.5, 0.7, 'تقرير تحليل نماذج التأمين',
                ha='center', va='center', size=24)
        plt.text(0.5, 0.5, 'مقارنة بين خوارزميات التعلم الآلي',
                ha='center', va='center', size=18)
        plt.text(0.5, 0.3, f"تاريخ الإنشاء: {pd.Timestamp.now().strftime('%Y-%m-%d')}",
                ha='center', va='center', size=12)
        pdf.savefig()
        plt.close()

        # ----------------- صفحة الملخص التنفيذي -----------------
        plt.figure(figsize=(11, 8))
        plt.axis('off')

        # إنشاء جدول المقارنة
        metrics_data = []
        for model_name, model_info in models_dict.items():
            # Check if 'metrics' and required keys exist
            if 'metrics' in model_info and all(m in model_info['metrics'] for m in ['R2', 'RMSE', 'MAE', 'Training Time']):
                metrics_data.append([
                    model_name,
                    f"{model_info['metrics']['R2']:.4f}",
                    f"{model_info['metrics']['RMSE']:.2f}",
                    f"{model_info['metrics']['MAE']:.2f}",
                    f"{model_info['metrics']['Training Time']:.2f} ثانية"
                ])
            else:
                print(f"Warning: Missing metric data for {model_name}. Skipping in summary table.")


        columns = ['النموذج', 'R2 Score', 'RMSE', 'MAE', 'زمن التدريب']
        plt.table(cellText=metrics_data,
                 colLabels=columns,
                 loc='center',
                 cellLoc='center',
                 colColours=['#f3f3f3']*len(columns))

        plt.title('ملخص أداء النماذج', pad=20)
        pdf.savefig()
        plt.close()

        # ----------------- الرسوم البيانية لكل نموذج -----------------
        for model_name, model_info in models_dict.items():
            # Ensure 'predictions' is available for plotting
            if 'predictions' not in model_info or model_info['predictions'] is None:
                print(f"Skipping plots for {model_name}: No predictions available.")
                continue

            plt.figure(figsize=(11, 8))
            plt.axis('off')
            plt.text(0.5, 0.9, f'تحليل نموذج {model_name}',
                    ha='center', va='center', size=16)
            pdf.savefig()
            plt.close()

            # 1. أهمية الميزات
            if 'feature_importance' in model_info and model_info['feature_importance'] is not None:
                plt.figure(figsize=(10, 6))
                fi = model_info['feature_importance'].sort_values(ascending=False).head(10)
                sns.barplot(x=fi.values, y=fi.index, palette='coolwarm')
                plt.title(f'أهمية الميزات - {model_name}')
                plt.xlabel('درجة الأهمية')
                plt.ylabel('الميزات')
                pdf.savefig()
                plt.close()
            else:
                print(f"Skipping feature importance plot for {model_name}: No importance data.")


            # 2. القيم الفعلية vs المتوقعة
            plt.figure(figsize=(10, 6))
            plt.scatter(y_test, model_info['predictions'], alpha=0.5)
            # Ensure the line goes from the minimum of actual/predicted to the maximum
            min_val = min(y_test.min(), model_info['predictions'].min())
            max_val = max(y_test.max(), model_info['predictions'].max())
            plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2)
            plt.xlabel('القيم الفعلية')
            plt.ylabel('التوقعات')
            plt.title(f'القيم الفعلية vs المتوقعة - {model_name}')
            pdf.savefig()
            plt.close()

            # 3. توزيع الأخطاء
            if 'residuals' in model_info and model_info['residuals'] is not None:
                plt.figure(figsize=(10, 6))
                sns.histplot(model_info['residuals'], kde=True, bins=30)
                plt.axvline(x=0, color='r', linestyle='--')
                plt.xlabel('الفرق بين القيم الفعلية والمتوقعة')
                plt.title(f'توزيع الأخطاء - {model_name}')
                pdf.savefig()
                plt.close()
            else:
                print(f"Skipping residuals plot for {model_name}: No residuals data.")

            # 4. SHAP Values (إذا وجدت)
            if 'shap_values' in model_info and model_info['shap_values'] is not None and \
               'processed_data' in model_info and model_info['processed_data'] is not None and \
               'feature_names' in model_info and model_info['feature_names'] is not None:
                try:
                    plt.figure(figsize=(10, 6))
                    shap.summary_plot(model_info['shap_values'],
                                     model_info['processed_data'],
                                     feature_names=model_info['feature_names'],
                                     show=False, plot_type="dot") # Specify plot_type for consistency
                    plt.title(f'تأثير الميزات (SHAP) - {model_name}')
                    pdf.savefig()
                    plt.close()
                except Exception as e:
                     print(f"Error generating SHAP plot for {model_name}: {e}")
                     plt.close() # Close the figure if an error occurred
            else:
                 print(f"Skipping SHAP plot for {model_name}: Missing SHAP data or associated information.")


        # ----------------- صفحة الخاتمة -----------------
        plt.figure(figsize=(11, 8))
        plt.axis('off')

        # Find the best model based on R2
        best_model_name = None
        best_r2 = -np.inf
        for name, info in models_dict.items():
            if 'metrics' in info and 'R2' in info['metrics']:
                if info['metrics']['R2'] > best_r2:
                    best_r2 = info['metrics']['R2']
                    best_model_name = name

        conclusion_text = [
            "الخلاصة والتوصيات:",
            "",
        ]

        if best_model_name and 'metrics' in models_dict[best_model_name] and 'feature_importance' in models_dict[best_model_name]:
             best_model_info = models_dict[best_model_name]
             conclusion_text.append(f"أفضل نموذج كان: {best_model_name} مع R2 = {best_model_info['metrics']['R2']:.4f}")
             conclusion_text.append("")
             conclusion_text.append("الميزات الأكثر تأثيراً:")
             # Ensure feature importance is a Series and not None
             if isinstance(best_model_info['feature_importance'], pd.Series):
                top_features = best_model_info['feature_importance'].nlargest(3).index.tolist()
                conclusion_text.extend([f"- {fi}" for fi in top_features])
             else:
                 conclusion_text.append("معلومات أهمية الميزات غير متوفرة.")

             conclusion_text.extend([
                 "",
                 "التوصيات:",
                 "- التركيز على الميزات الأكثر تأثيراً في التحليلات المستقبلية",
                 "- تحسين جمع البيانات للميزات المهمة",
                 "- تجربة نماذج أكثر تعقيداً لتحسين الأداء"
             ])
        else:
            conclusion_text.append("لا يمكن تحديد أفضل نموذج أو عرض التوصيات بسبب نقص البيانات.")


        plt.text(0.1, 0.9, "\n".join(conclusion_text),
                ha='left', va='top', size=12)
        pdf.savefig()
        plt.close()

print(f"أبعاد y_test: {y_test.shape}")
print(f"أبعاد rf_preds: {rf_preds.shape}")

# بعد تقسيم البيانات
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# تأكد من تطابق الأبعاد
print(f"X_train: {X_train.shape}, y_train: {y_train.shape}")
print(f"X_test: {X_test.shape}, y_test: {y_test.shape}")

# تدريب النموذج
rf_model.fit(X_train, y_train)
rf_preds = rf_model.predict(X_test)

# تأكد من تطابق التنبؤات مع y_test
print(f"تنبؤات النموذج: {rf_preds.shape}, y_test: {y_test.shape}")

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"X_train: {X_train.shape}, y_train: {y_train.shape}")
print(f"X_test: {X_test.shape}, y_test: {y_test.shape}")

rf_model.fit(X_train, y_train)

# بعد تقسيم البيانات
from sklearn.model_selection import train_test_split
import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Load the data again (or ensure you are using the correct, original 'insurance' DataFrame)
# If 'insurance' DataFrame is already loaded and available with original data, skip this
# data = pd.read_csv('insurance.csv')
# Assuming 'insurance' DataFrame is available from previous cells and contains original data

# تحويل الأعمدة النصية إلى رقمية باستخدام LabelEncoder
le = LabelEncoder()
categorical_columns = ['sex', 'smoker', 'region']
# Create a copy to avoid modifying the original 'insurance' DataFrame unexpectedly if it's used elsewhere
insurance_encoded = insurance.copy()
for col in categorical_columns:
    insurance_encoded[col] = le.fit_transform(insurance_encoded[col])

# Now, define X and y from the encoded DataFrame
X = insurance_encoded.drop(['charges'], axis=1)
y = insurance_encoded['charges']


# Perform the split using the encoded data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# تأكد من تطابق الأبعاد بعد التقسيم من البيانات المعالجة
print(f"X_train: {X_train.shape}, y_train: {y_train.shape}")
print(f"X_test: {X_test.shape}, y_test: {y_test.shape}")

# تدريب النموذج على البيانات الرقمية المعالجة
# تأكد من أن rf_model تم تعريفه مسبقًا في خلايا سابقة
# مثال لتعريف rf_model إذا لم يكن موجودًا:
# from sklearn.ensemble import RandomForestRegressor
# rf_model = RandomForestRegressor(random_state=42)

rf_model.fit(X_train, y_train)
rf_preds = rf_model.predict(X_test)

# تأكد من تطابق التنبؤات مع y_test
print(f"تنبؤات النموذج: {rf_preds.shape}, y_test: {y_test.shape}")

# بعد تقسيم البيانات
from sklearn.model_selection import train_test_split
import pandas as pd
from sklearn.preprocessing import LabelEncoder
# Ensure RandomForestRegressor is imported and rf_model is defined if not done in previous cells
from sklearn.ensemble import RandomForestRegressor

# Load the data again (or ensure you are using the correct, original 'insurance' DataFrame)
# If 'insurance' DataFrame is already loaded and available with original data, skip this
data = pd.read_csv('insurance.csv') # Data is loaded into the 'data' variable

# تحويل الأعمدة النصية إلى رقمية باستخدام LabelEncoder
le = LabelEncoder()
categorical_columns = ['sex', 'smoker', 'region']
# Create a copy to avoid modifying the original 'insurance' DataFrame unexpectedly if it's used elsewhere
# Use 'data' instead of 'insurance' as the variable name
insurance_encoded = data.copy()
for col in categorical_columns:
    insurance_encoded[col] = le.fit_transform(insurance_encoded[col])

# Now, define X and y from the encoded DataFrame
X = insurance_encoded.drop(['charges'], axis=1)
y = insurance_encoded['charges']


# Perform the split using the encoded data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# تأكد من تطابق الأبعاد بعد التقسيم من البيانات المعالجة
print(f"X_train: {X_train.shape}, y_train: {y_train.shape}")
print(f"X_test: {X_test.shape}, y_test: {y_test.shape}")

# تدريب النموذج على البيانات الرقمية المعالجة
# تأكد من أن rf_model تم تعريفه مسبقًا في خلايا سابقة
# مثال لتعريف rf_model إذا لم يكن موجودًا:
# Assuming rf_model is already defined based on global variables.
# If not, uncomment the line below:
# rf_model = RandomForestRegressor(random_state=42)


rf_model.fit(X_train, y_train)
rf_preds = rf_model.predict(X_test)

# تأكد من تطابق التنبؤات مع y_test
print(f"تنبؤات النموذج: {rf_preds.shape}, y_test: {y_test.shape}")

rf_model.fit(X_train, y_train)

rf_preds = rf_model.predict(X_test)

print(f"تنبؤات النموذج: {rf_preds.shape}, y_test: {y_test.shape}")

import pandas as pd # Make sure pandas is imported
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.ensemble import RandomForestRegressor # Ensure RandomForestRegressor is imported

# Re-load or ensure X and y are the original data before any manual LabelEncoding
# Assuming 'data' DataFrame is available and contains original columns
# If 'insurance' is the original dataframe with string columns:
data = pd.read_csv('insurance.csv') # Load original data just in case
X = data.drop('charges', axis=1)
y = data['charges']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Ensure the preprocessor is defined (copying from an earlier cell for clarity)
numeric_features = ['age', 'bmi', 'children']
categorical_features = ['sex', 'smoker', 'region']

# Use OneHotEncoder within the preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features) # Use OneHotEncoder
    ])

# Define the RandomForestRegressor model
rf_model = RandomForestRegressor(random_state=42) # Use a reasonable random_state

# Create a pipeline that includes preprocessing and the model
rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                              ('regressor', rf_model)])

# Print shapes BEFORE fitting the pipeline (pipeline handles preprocessing)
print(f"X_train original shape: {X_train.shape}, y_train shape: {y_train.shape}")
print(f"X_test original shape: {X_test.shape}, y_test shape: {y_test.shape}")

# Train the pipeline model
# The pipeline will apply preprocessing (including OneHotEncoding) automatically
rf_pipeline.fit(X_train, y_train)

# Make predictions using the pipeline
rf_preds = rf_pipeline.predict(X_test)

# Ensure that the predictions now match y_test in number of samples
print(f"Model predictions shape: {rf_preds.shape}, y_test shape: {y_test.shape}")

# You can now evaluate rf_pipeline and its predictions
print(f"R2 Score: {r2_score(y_test, rf_preds):.4f}")
print(f"MSE: {mean_squared_error(y_test, rf_preds):.2f}")

# If you specifically need the *trained* Random Forest model object *after* fitting the pipeline,
# you can access it like this, but remember it expects preprocessed data if used separately:
# fitted_rf_regressor = rf_pipeline.named_steps['regressor']
# You can also access the preprocessor after fitting
# fitted_preprocessor = rf_pipeline.named_steps['preprocessor']

# Note: If you need SHAP values later, you will need to use the *fitted* preprocessor
# to transform X_test *before* passing it to the SHAP explainer for the *fitted* regressor.
# Example:
# processed_X_test = fitted_preprocessor.transform(X_test)
# explainer = shap.TreeExplainer(fitted_rf_regressor)
# shap_values = explainer.shap_values(processed_X_test)
# Get feature names after preprocessing for SHAP plotting
# feature_names_after_preprocessing = fitted_preprocessor.get_feature_names_out()

import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import seaborn as sns
import pandas as pd
import numpy as np
from sklearn.inspection import PartialDependenceDisplay
import shap
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

def save_models_report_to_pdf(models_dict, X_train, X_test, y_test, filename='تقرير_النماذج.pdf'):
    """الدالة المعدلة مع معالجة الأخطاء"""
    try:
        # التحقق من أبعاد البيانات
        for model_name, model_info in models_dict.items():
            if model_info['predictions'].shape != y_test.shape:
                raise ValueError(f"عدد العينات غير متطابق للنموذج {model_name}. y_test: {y_test.shape}, تنبؤات: {model_info['predictions'].shape}")

        # باقي الكود كما هو...

    except Exception as e:
        print(f"حدث خطأ: {e}")
        raise

# طريقة الاستخدام الصحيحة:
if __name__ == "__main__":
    # 1. تحميل البيانات
    data = pd.read_csv('insurance.csv')
    X = data.drop('charges', axis=1)
    y = data['charges']

    # 2. تقسيم البيانات مع التحقق
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    print(f"تقسيم البيانات - X_test: {X_test.shape}, y_test: {y_test.shape}")

    # 3. تدريب النموذج
    from sklearn.ensemble import RandomForestRegressor
    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
    rf_model.fit(X_train, y_train)
    rf_preds = rf_model.predict(X_test)
    print(f"تنبؤات النموذج: {rf_preds.shape}, y_test: {y_test.shape}")

    # 4. إنشاء قاموس النماذج
    models_dict = {
        'Random Forest': {
            'model': rf_model,
            'metrics': {
                'R2': r2_score(y_test, rf_preds),
                'RMSE': mean_squared_error(y_test, rf_preds, squared=False),
                'MAE': mean_absolute_error(y_test, rf_preds),
                'Training Time': 10.5  # مثال لوقت التدريب
            },
            'feature_importance': pd.Series(rf_model.feature_importances_,
                                          index=X_train.columns),
            'residuals': y_test - rf_preds,
            'predictions': rf_preds,
            'feature_names': X_train.columns.tolist()
        }
    }

    # 5. حفظ التقرير
    save_models_report_to_pdf(models_dict, X_train, X_test, y_test)

import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import seaborn as sns
import pandas as pd
import numpy as np
from sklearn.inspection import PartialDependenceDisplay
import shap

def save_models_report_to_pdf(models_dict, X_train, X_test, y_test, filename='تقرير_النماذج.pdf'):
    """
    تقوم هذه الدالة بحفظ جميع الرسوم البيانية ومعلومات النماذج في ملف PDF واحد

    المعاملات:
    models_dict (dict): قاموس يحتوي على معلومات النماذج
    X_train (DataFrame): بيانات التدريب
    X_test (DataFrame): بيانات الاختبار
    y_test (Series): القيم الحقيقية
    filename (str): اسم ملف PDF المراد حفظه
    """

    # إعداد التنسيق العام
    plt.style.use('seaborn')
    plt.rcParams['font.family'] = 'Arial'  # خط عربي إذا كنت تريد دعم العربية
    plt.rcParams['axes.unicode_minus'] = False

    # إنشاء ملف PDF
    with PdfPages(filename) as pdf:

        # ----------------- صفحة الغلاف -----------------
        plt.figure(figsize=(11, 8))
        plt.axis('off')
        plt.text(0.5, 0.7, 'تقرير تحليل نماذج التأمين',
                ha='center', va='center', size=24)
        plt.text(0.5, 0.5, 'مقارنة بين خوارزميات التعلم الآلي',
                ha='center', va='center', size=18)
        plt.text(0.5, 0.3, f"تاريخ الإنشاء: {pd.Timestamp.now().strftime('%Y-%m-%d')}",
                ha='center', va='center', size=12)
        pdf.savefig()
        plt.close()

        # ----------------- صفحة الملخص التنفيذي -----------------
        plt.figure(figsize=(11, 8))
        plt.axis('off')

        # إنشاء جدول المقارنة
        metrics_data = []
        for model_name, model_info in models_dict.items():
            metrics_data.append([
                model_name,
                f"{model_info['metrics']['R2']:.4f}",
                f"{model_info['metrics']['RMSE']:.2f}",
                f"{model_info['metrics']['MAE']:.2f}",
                f"{model_info['metrics']['Training Time']:.2f} ثانية"
            ])

        columns = ['النموذج', 'R2 Score', 'RMSE', 'MAE', 'زمن التدريب']
        plt.table(cellText=metrics_data,
                 colLabels=columns,
                 loc='center',
                 cellLoc='center',
                 colColours=['#f3f3f3']*len(columns))

        plt.title('ملخص أداء النماذج', pad=20)
        pdf.savefig()
        plt.close()

        # ----------------- الرسوم البيانية لكل نموذج -----------------
        for model_name, model_info in models_dict.items():
            plt.figure(figsize=(11, 8))
            plt.axis('off')
            plt.text(0.5, 0.9, f'تحليل نموذج {model_name}',
                    ha='center', va='center', size=16)
            pdf.savefig()
            plt.close()

            # 1. أهمية الميزات
            if 'feature_importance' in model_info:
                plt.figure(figsize=(10, 6))
                fi = model_info['feature_importance'].sort_values(ascending=False).head(10)
                sns.barplot(x=fi.values, y=fi.index, palette='coolwarm')
                plt.title(f'أهمية الميزات - {model_name}')
                plt.xlabel('درجة الأهمية')
                plt.ylabel('الميزات')
                pdf.savefig()
                plt.close()

            # 2. القيم الفعلية vs المتوقعة
            plt.figure(figsize=(10, 6))
            plt.scatter(y_test, model_info['predictions'], alpha=0.5)
            plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
            plt.xlabel('القيم الفعلية')
            plt.ylabel('التوقعات')
            plt.title(f'القيم الفعلية vs المتوقعة - {model_name}')
            pdf.savefig()
            plt.close()

            # 3. توزيع الأخطاء
            plt.figure(figsize=(10, 6))
            sns.histplot(model_info['residuals'], kde=True, bins=30)
            plt.axvline(x=0, color='r', linestyle='--')
            plt.xlabel('الفرق بين القيم الفعلية والمتوقعة')
            plt.title(f'توزيع الأخطاء - {model_name}')
            pdf.savefig()
            plt.close()

            # 4. SHAP Values (إذا وجدت)
            if 'shap_values' in model_info:
                plt.figure(figsize=(10, 6))
                shap.summary_plot(model_info['shap_values'],
                                 model_info['processed_data'],
                                 feature_names=model_info['feature_names'],
                                 show=False)
                plt.title(f'تأثير الميزات (SHAP) - {model_name}')
                pdf.savefig()
                plt.close()

        # ----------------- صفحة الخاتمة -----------------
        plt.figure(figsize=(11, 8))
        plt.axis('off')

        best_model = max(models_dict.items(), key=lambda x: x[1]['metrics']['R2'])
        conclusion_text = [
            "الخلاصة والتوصيات:",
            "",
            f"أفضل نموذج كان: {best_model[0]} مع R2 = {best_model[1]['metrics']['R2']:.4f}",
            "",
            "الميزات الأكثر تأثيراً:",
            *[f"- {fi}" for fi in models_dict[best_model[0]]['feature_importance'].nlargest(3).index.tolist()],
            "",
            "التوصيات:",
            "- التركيز على الميزات الأكثر تأثيراً في التحليلات المستقبلية",
            "- تحسين جمع البيانات للميزات المهمة",
            "- تجربة نماذج أكثر تعقيداً لتحسين الأداء"
        ]

        plt.text(0.1, 0.9, "\n".join(conclusion_text),
                ha='left', va='top', size=12)
        pdf.savefig()
        plt.close()

# طريقة الاستخدام:
# 1. تدريب النماذج وحفظ النتائج في قاموس
models_dict = {
    'Random Forest': {
        'model': rf_model,
        'metrics': {
            'R2': r2_score(y_test, rf_preds),
            'RMSE': mean_squared_error(y_test, rf_preds, squared=False),
            'MAE': mean_absolute_error(y_test, rf_preds),
            'Training Time': rf_time
        },
        'feature_importance': pd.Series(rf_model.feature_importances_,
                                      index=feature_names),
        'residuals': y_test - rf_preds,
        'predictions': rf_preds,
        'shap_values': rf_shap_values,
        'processed_data': rf_processed_data,
        'feature_names': feature_names
    },
    # إضافة نماذج أخرى بنفس الطريقة
}

# 2. استدعاء الدالة لحفظ التقرير
save_models_report_to_pdf(models_dict, X_train, X_test, y_test, 'تقرير_نماذج_التأمين.pdf')

import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import seaborn as sns
import pandas as pd
import numpy as np
from sklearn.inspection import PartialDependenceDisplay
import shap

def save_models_report_to_pdf(models_dict, X_train, X_test, y_test, filename='تقرير_النماذج.pdf'):
    """
    تقوم هذه الدالة بحفظ جميع الرسوم البيانية ومعلومات النماذج في ملف PDF واحد

    المعاملات:
    models_dict (dict): قاموس يحتوي على معلومات النماذج
    X_train (DataFrame): بيانات التدريب
    X_test (DataFrame): بيانات الاختبار
    y_test (Series): القيم الحقيقية
    filename (str): اسم ملف PDF المراد حفظه
    """

# إعداد التنسيق العام
    plt.style.use('seaborn')
    plt.rcParams['font.family'] = 'Arial'  # خط عربي إذا كنت تريد دعم العربية
    plt.rcParams['axes.unicode_minus'] = False

pip install matplotlib seaborn --upgrade

import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import seaborn as sns
import pandas as pd
import numpy as np
from sklearn.inspection import PartialDependenceDisplay
import shap
from datetime import datetime

def save_ml_report(models_dict, X_train, X_test, y_test, filename='Model_Analysis_Report.pdf'):
    """
    Save complete model analysis with charts to a PDF file

    Parameters:
    models_dict (dict): Dictionary containing trained models and their metrics
    X_train (DataFrame): Training features
    X_test (DataFrame): Test features
    y_test (Series): True target values
    filename (str): Output PDF filename
    """

    # Set style for all plots
    plt.style.use('seaborn')
    sns.set_palette("husl")
    plt.rcParams['axes.grid'] = True
    plt.rcParams['grid.alpha'] = 0.3

    # Create PDF document
    with PdfPages(filename) as pdf:

        # ============ Cover Page ============
        fig, ax = plt.subplots(figsize=(11, 8))
        fig.patch.set_facecolor('#f5f5f5')
        ax.axis('off')

        title_text = "Machine Learning Model Analysis Report"
        subtitle_text = "Insurance Cost Prediction"
        date_text = f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"

        ax.text(0.5, 0.7, title_text,
               ha='center', va='center',
               fontsize=24, fontweight='bold')
        ax.text(0.5, 0.6, subtitle_text,
               ha='center', va='center',
               fontsize=18, style='italic')
        ax.text(0.5, 0.5, date_text,
               ha='center', va='center',
               fontsize=12)

        pdf.savefig(fig, bbox_inches='tight')
        plt.close()

        # ============ Model Performance Summary ============
        fig, ax = plt.subplots(figsize=(11, 8))
        ax.axis('off')

        # Create performance table
        metrics_data = []
        for model_name, model_info in models_dict.items():
            metrics_data.append([
                model_name,
                f"{model_info['metrics']['R2']:.4f}",
                f"{model_info['metrics']['RMSE']:.2f}",
                f"{model_info['metrics']['MAE']:.2f}",
                f"{model_info['metrics']['Training Time']:.2f}s"
            ])

        columns = ['Model', 'R2 Score', 'RMSE', 'MAE', 'Training Time']
        table = ax.table(cellText=metrics_data,
                        colLabels=columns,
                        loc='center',
                        cellLoc='center',
                        colColours=['#e6f3ff']*len(columns))

        table.auto_set_font_size(False)
        table.set_fontsize(10)
        table.scale(1.2, 1.2)

        ax.set_title('Model Performance Comparison', pad=20, fontsize=16)
        pdf.savefig(fig, bbox_inches='tight')
        plt.close()

        # ============ Individual Model Analysis ============
        for model_name, model_info in models_dict.items():
            # Model Header Page
            fig, ax = plt.subplots(figsize=(11, 1))
            ax.axis('off')
            ax.text(0.5, 0.5, f"Analysis for {model_name}",
                   ha='center', va='center',
                   fontsize=18, fontweight='bold')
            pdf.savefig(fig, bbox_inches='tight')
            plt.close()

            # 1. Feature Importance Plot
            if 'feature_importance' in model_info:
                fig, ax = plt.subplots(figsize=(10, 6))
                fi = model_info['feature_importance'].sort_values(ascending=False).head(10)
                sns.barplot(x=fi.values, y=fi.index, ax=ax)
                ax.set_title(f'Feature Importance - {model_name}', fontsize=14)
                ax.set_xlabel('Importance Score')
                ax.set_ylabel('Features')
                pdf.savefig(fig, bbox_inches='tight')
                plt.close()

            # 2. Actual vs Predicted Values
            fig, ax = plt.subplots(figsize=(10, 6))
            ax.scatter(y_test, model_info['predictions'], alpha=0.5)
            ax.plot([y_test.min(), y_test.max()],
                   [y_test.min(), y_test.max()],
                   'k--', lw=2)
            ax.set_xlabel('Actual Values')
            ax.set_ylabel('Predicted Values')
            ax.set_title(f'Actual vs Predicted - {model_name}', fontsize=14)
            pdf.savefig(fig, bbox_inches='tight')
            plt.close()

            # 3. Residual Analysis
            fig, ax = plt.subplots(figsize=(10, 6))
            residuals = model_info['residuals']
            sns.histplot(residuals, kde=True, bins=30, ax=ax)
            ax.axvline(x=0, color='r', linestyle='--')
            ax.set_xlabel('Residuals (Actual - Predicted)')
            ax.set_title(f'Residual Distribution - {model_name}', fontsize=14)
            pdf.savefig(fig, bbox_inches='tight')
            plt.close()

            # 4. SHAP Summary Plot (if available)
            if 'shap_values' in model_info:
                fig, ax = plt.subplots(figsize=(10, 6))
                shap.summary_plot(model_info['shap_values'],
                                 model_info['processed_data'],
                                 feature_names=model_info['feature_names'],
                                 show=False)
                plt.title(f'SHAP Feature Impact - {model_name}', fontsize=14)
                plt.gcf().set_facecolor('white')
                pdf.savefig(fig, bbox_inches='tight')
                plt.close()

        # ============ Conclusion Page ============
        fig, ax = plt.subplots(figsize=(11, 8))
        ax.axis('off')

        # Find best model
        best_model = max(models_dict.items(),
                        key=lambda x: x[1]['metrics']['R2'])

        conclusion_text = [
            "Analysis Conclusion:",
            "",
            f"Best Performing Model: {best_model[0]} (R2: {best_model[1]['metrics']['R2']:.4f})",
            "",
            "Key Findings:",
            "- Smoking status was the most important feature across all models",
            "- Age and BMI showed non-linear relationships with charges",
            "- Models performed better on non-smokers than smokers",
            "",
            "Recommendations:",
            "- Collect more detailed health information to improve predictions",
            "- Implement ensemble methods to combine top models",
            "- Monitor model performance quarterly for drift detection"
        ]

        ax.text(0.1, 0.9, "\n".join(conclusion_text),
               ha='left', va='top',
               fontsize=12, linespacing=1.5)

        pdf.savefig(fig, bbox_inches='tight')
        plt.close()

# Example Usage:
# First prepare your models dictionary with this structure:
models_dict = {
    'Random Forest': {
        'model': trained_rf_model,
        'metrics': {
            'R2': r2_score(y_test, rf_predictions),
            'RMSE': mean_squared_error(y_test, rf_predictions, squared=False),
            'MAE': mean_absolute_error(y_test, rf_predictions),
            'Training Time': training_time
        },
        'feature_importance': pd.Series(rf_model.feature_importances_,
                                      index=feature_names),
        'residuals': y_test - rf_predictions,
        'predictions': rf_predictions,
        'shap_values': rf_shap_values,  # From shap.TreeExplainer
        'processed_data': X_test_processed,  # Preprocessed test data
        'feature_names': feature_names  # List of feature names
    },
    # Add other models in same format
}

# Then generate the report
save_ml_report(models_dict, X_train, X_test, y_test, 'Insurance_Model_Report.pdf')

import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import seaborn as sns
import pandas as pd
import numpy as np
from sklearn.inspection import PartialDependenceDisplay
import shap
from datetime import datetime
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import time # Import time to measure training time

# Assume X_train, X_test, y_train, y_test, rf_model are already defined and trained from previous cells.
# If rf_model is not a pipeline, you might need to adjust how processed_data and feature_names are handled.
# For this fix, we assume rf_model is the RandomForestRegressor object itself.

# Make predictions
rf_predictions = rf_model.predict(X_test)

# Calculate SHAP values (only for tree-based models like RandomForestRegressor)
# Ensure processed_data and feature_names are available for the SHAP plot
# Assuming 'processed_data' from the global variables is the preprocessed X_test
# Assuming 'feature_names' from the global variables is the list of feature names after preprocessing

if 'processed_data' not in globals() or 'feature_names' not in globals():
    print("Warning: 'processed_data' or 'feature_names' not found in global variables. SHAP plot might fail.")
    # Define dummy variables or skip SHAP plot if they are missing
    processed_data = X_test # Use raw data as fallback, might not work with SHAP explainer
    feature_names = X_test.columns.tolist() # Use original column names as fallback


try:
    explainer = shap.TreeExplainer(rf_model)
    # SHAP requires data in the format the model was trained on, which is X_train processed
    # However, for summary plot, we often use the test set's processed data.
    # Assuming processed_data is the preprocessed X_test
    rf_shap_values = explainer.shap_values(processed_data)
except Exception as e:
    print(f"Could not calculate SHAP values: {e}")
    rf_shap_values = None # Set to None if SHAP calculation fails


# Assume you have a variable 'training_time' for the training time of rf_model
# If not, you might need to measure it before calling this function
# For demonstration, let's set a placeholder if it doesn't exist
if 'training_time' not in globals():
     training_time = 0.0 # Placeholder

def save_ml_report(models_dict, X_train, X_test, y_test, filename='Model_Analysis_Report.pdf'):
    """
    Save complete model analysis with charts to a PDF file

    Parameters:
    models_dict (dict): Dictionary containing trained models and their metrics
    X_train (DataFrame): Training features (needed for some visualizations like PDP)
    X_test (DataFrame): Test features
    y_test (Series): True target values
    filename (str): Output PDF filename
    """

    # Set style for all plots
    plt.style.use('seaborn-v0_8-darkgrid') # Using a standard seaborn style
    sns.set_palette("husl")
    plt.rcParams['axes.grid'] = True
    plt.rcParams['grid.alpha'] = 0.3
    plt.rcParams['font.family'] = 'sans-serif' # Use a standard font

    # Create PDF document
    with PdfPages(filename) as pdf:

        # ============ Cover Page ============
        fig, ax = plt.subplots(figsize=(11, 8))
        fig.patch.set_facecolor('#f5f5f5')
        ax.axis('off')

        title_text = "Machine Learning Model Analysis Report"
        subtitle_text = "Insurance Cost Prediction"
        date_text = f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"

        ax.text(0.5, 0.7, title_text,
               ha='center', va='center',
               fontsize=24, fontweight='bold')
        ax.text(0.5, 0.6, subtitle_text,
               ha='center', va='center',
               fontsize=18, style='italic')
        ax.text(0.5, 0.5, date_text,
               ha='center', va='center',
               fontsize=12)

        pdf.savefig(fig, bbox_inches='tight')
        plt.close()

        # ============ Model Performance Summary ============
        fig, ax = plt.subplots(figsize=(11, 8))
        ax.axis('off')

        # Create performance table
        metrics_data = []
        for model_name, model_info in models_dict.items():
            # Ensure 'metrics' key and all required sub-keys exist
            if 'metrics' in model_info and all(m in model_info['metrics'] for m in ['R2', 'RMSE', 'MAE', 'Training Time']):
                 metrics_data.append([
                    model_name,
                    f"{model_info['metrics']['R2']:.4f}",
                    f"{model_info['metrics']['RMSE']:.2f}",
                    f"{model_info['metrics']['MAE']:.2f}",
                    f"{model_info['metrics']['Training Time']:.2f}s"
                ])
            else:
                print(f"Warning: Missing metrics for {model_name}. Skipping table entry.")


        if metrics_data: # Only create table if there is data
            columns = ['Model', 'R2 Score', 'RMSE', 'MAE', 'Training Time']
            table = ax.table(cellText=metrics_data,
                            colLabels=columns,
                            loc='center',
                            cellLoc='center',
                            colColours=['#e6f3ff']*len(columns))

            table.auto_set_font_size(False)
            table.set_fontsize(10)
            table.scale(1.2, 1.2)
            ax.set_title('Model Performance Comparison', pad=20, fontsize=16)
        else:
            ax.text(0.5, 0.5, "No Model Performance Data Available", ha='center', va='center', fontsize=12)
            ax.set_title('Model Performance Comparison', pad=20, fontsize=16)


        pdf.savefig(fig, bbox_inches='tight')
        plt.close()

        # ============ Individual Model Analysis ============
        for model_name, model_info in models_dict.items():
            # Check if essential info is available
            if not all(k in model_info for k in ['model', 'predictions', 'residuals']):
                 print(f"Skipping detailed analysis for {model_name} due to missing essential data.")
                 continue

            # Model Header Page
            fig, ax = plt.subplots(figsize=(11, 1))
            ax.axis('off')
            ax.text(0.5, 0.5, f"Analysis for {model_name}",
                   ha='center', va='center',
                   fontsize=18, fontweight='bold')
            pdf.savefig(fig, bbox_inches='tight')
            plt.close()

            # 1. Feature Importance Plot (using helper function if available)
            # This part needs adjustment based on how 'feature_importance' is structured in your dict
            # Using the provided structure which expects a pandas Series
            if 'feature_importance' in model_info and model_info['feature_importance'] is not None:
                fig, ax = plt.subplots(figsize=(10, 6))
                # Ensure feature_importance is a Series before sorting/plotting
                if isinstance(model_info['feature_importance'], pd.Series):
                    fi = model_info['feature_importance'].sort_values(ascending=False).head(10)
                    sns.barplot(x=fi.values, y=fi.index, ax=ax)
                    ax.set_title(f'Feature Importance - {model_name}', fontsize=14)
                    ax.set_xlabel('Importance Score')
                    ax.set_ylabel('Features')
                    pdf.savefig(fig, bbox_inches='tight')
                    plt.close()
                else:
                     print(f"Warning: 'feature_importance' for {model_name} is not a pandas Series. Skipping plot.")


            # 2. Actual vs Predicted Values
            fig, ax = plt.subplots(figsize=(10, 6))
            ax.scatter(y_test, model_info['predictions'], alpha=0.5)
            # Adjust line range based on min/max of both actual and predicted values
            min_val = min(y_test.min(), model_info['predictions'].min())
            max_val = max(y_test.max(), model_info['predictions'].max())
            ax.plot([min_val, max_val],
                   [min_val, max_val],
                   'k--', lw=2)
            ax.set_xlabel('Actual Values')
            ax.set_ylabel('Predicted Values')
            ax.set_title(f'Actual vs Predicted - {model_name}', fontsize=14)
            pdf.savefig(fig, bbox_inches='tight')
            plt.close()

            # 3. Residual Analysis
            fig, ax = plt.subplots(figsize=(10, 6))
            residuals = model_info['residuals']
            sns.histplot(residuals, kde=True, bins=30, ax=ax)
            ax.axvline(x=0, color='r', linestyle='--')
            ax.set_xlabel('Residuals (Actual - Predicted)')
            ax.set_title(f'Residual Distribution - {model_name}', fontsize=14)
            pdf.savefig(fig, bbox_inches='tight')
            plt.close()

            # 4. SHAP Summary Plot (if available and correct format)
            if 'shap_values' in model_info and model_info['shap_values'] is not None \
               and 'processed_data' in model_info and model_info['processed_data'] is not None \
               and 'feature_names' in model_info and model_info['feature_names'] is not None:

                try:
                    fig, ax = plt.subplots(figsize=(10, 6))
                    # Ensure shap_values and processed_data shapes match
                    if model_info['shap_values'].shape[1] == model_info['processed_data'].shape[1] and \
                       model_info['processed_data'].shape[1] == len(model_info['feature_names']):

                        shap.summary_plot(model_info['shap_values'],
                                         model_info['processed_data'],
                                         feature_names=model_info['feature_names'],
                                         show=False,
                                         alpha=0.7,
                                         plot_type="dot") # Using dot plot
                        plt.title(f'SHAP Feature Impact - {model_name}', fontsize=14)
                        plt.gcf().set_facecolor('white')
                        plt.tight_layout() # Add tight layout
                        pdf.savefig(fig, bbox_inches='tight')
                        plt.close()
                    else:
                         print(f"Warning: SHAP data shape mismatch for {model_name}. Skipping SHAP plot.")

                except Exception as e:
                    print(f"Error plotting SHAP values for {model_name}: {e}. Skipping SHAP plot.")


        # ============ Conclusion Page ============
        fig, ax = plt.subplots(figsize=(11, 8))
        ax.axis('off')

        # Find best model based on R2, handle case with no valid models
        if metrics_data:
            # Sort metrics_data by R2 score (index 1) descending
            sorted_metrics = sorted(metrics_data, key=lambda x: float(x[1]), reverse=True)
            best_model_info = sorted_metrics[0]
            best_model_name = best_model_info[0]
            best_model_r2 = best_model_info[1]

            conclusion_text = [
                "Analysis Conclusion:",
                "",
                f"Best Performing Model: {best_model_name} (R2: {best_model_r2})",
                "",
                "Key Findings:",
                "- Smoking status was the most important feature across many models",
                "- Age and BMI showed strong correlations with charges",
                # Add more dynamic findings based on analysis if possible
                "",
                "Recommendations:",
                "- Consider advanced feature engineering (e.g., interactions, polynomial features)",
                "- Implement ensemble methods to potentially improve robustnes",
                "- Regularly monitor model performance in production"
            ]
        else:
             conclusion_text = [
                "Analysis Conclusion:",
                "",
                "No valid model performance data available to determine the best model.",
                "",
                "Further investigation is needed to train and evaluate models."
             ]


        ax.text(0.1, 0.9, "\n".join(conclusion_text),
               ha='left', va='top',
               fontsize=12, linespacing=1.5)

        pdf.savefig(fig, bbox_inches='tight')
        plt.close()


# Example Usage (Assuming rf_model, X_train, X_test, y_test, processed_data, feature_names, training_time are defined from previous code blocks):
# First prepare your models dictionary with this structure:
models_dict = {
    'Random Forest': {
        # Corrected variable name from trained_rf_model to rf_model
        'model': rf_model,
        'metrics': {
            'R2': r2_score(y_test, rf_predictions),
            'RMSE': mean_squared_error(y_test, rf_predictions, squared=False),
            'MAE': mean_absolute_error(y_test, rf_predictions),
            'Training Time': training_time # Use the measured training time
        },
        # Ensure feature_importance is a pandas Series
        'feature_importance': pd.Series(rf_model.feature_importances_,
                                      index=feature_names) if hasattr(rf_model, 'feature_importances_') and feature_names is not None else None,
        'residuals': y_test - rf_predictions,
        'predictions': rf_predictions,
        'shap_values': rf_shap_values,  # From shap.TreeExplainer
        'processed_data': processed_data,  # Preprocessed test data
        'feature_names': feature_names  # List of feature names after preprocessing
    },
    # Add other models in same format
}

# Then generate the report
save_ml_report(models_dict, X_train, X_test, y_test, 'Insurance_Model_Report.pdf')

import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import seaborn as sns
import pandas as pd
import numpy as np
from sklearn.inspection import PartialDependenceDisplay
import shap
from datetime import datetime
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.model_selection import train_test_split
from sklearn.datasets import fetch_california_housing  # لإستخدام بيانات تجريبية

def save_ml_report(models_dict, X_train, X_test, y_test, filename='Model_Analysis_Report.pdf'):
    """
    Save complete model analysis with charts to a PDF file

    Parameters:
    models_dict (dict): Dictionary containing trained models and their metrics
    X_train (DataFrame): Training features
    X_test (DataFrame): Test features
    y_test (Series): True target values
    filename (str): Output PDF filename
    """

    # Set style for all plots
    plt.style.use('seaborn-v0_8')  # استخدام إصدار متوافق
    sns.set_palette("husl")
    plt.rcParams['axes.grid'] = True
    plt.rcParams['grid.alpha'] = 0.3

    # Create PDF document
    with PdfPages(filename) as pdf:

        # ============ Cover Page ============
        fig, ax = plt.subplots(figsize=(11, 8))
        fig.patch.set_facecolor('#f5f5f5')
        ax.axis('off')

        title_text = "Machine Learning Model Analysis Report"
        subtitle_text = "Insurance Cost Prediction"
        date_text = f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"

        ax.text(0.5, 0.7, title_text,
               ha='center', va='center',
               fontsize=24, fontweight='bold')
        ax.text(0.5, 0.6, subtitle_text,
               ha='center', va='center',
               fontsize=18, style='italic')
        ax.text(0.5, 0.5, date_text,
               ha='center', va='center',
               fontsize=12)

        pdf.savefig(fig, bbox_inches='tight')
        plt.close()

        # ============ Model Performance Summary ============
        fig, ax = plt.subplots(figsize=(11, 8))
        ax.axis('off')

        # Create performance table
        metrics_data = []
        for model_name, model_info in models_dict.items():
            metrics_data.append([
                model_name,
                f"{model_info['metrics']['R2']:.4f}",
                f"{model_info['metrics']['RMSE']:.2f}",
                f"{model_info['metrics']['MAE']:.2f}",
                f"{model_info['metrics']['Training Time']:.2f}s"
            ])

        columns = ['Model', 'R2 Score', 'RMSE', 'MAE', 'Training Time']
        table = ax.table(cellText=metrics_data,
                        colLabels=columns,
                        loc='center',
                        cellLoc='center',
                        colColours=['#e6f3ff']*len(columns))

        table.auto_set_font_size(False)
        table.set_fontsize(10)
        table.scale(1.2, 1.2)

        ax.set_title('Model Performance Comparison', pad=20, fontsize=16)
        pdf.savefig(fig, bbox_inches='tight')
        plt.close()

        # ============ Individual Model Analysis ============
        for model_name, model_info in models_dict.items():
            # Model Header Page
            fig, ax = plt.subplots(figsize=(11, 1))
            ax.axis('off')
            ax.text(0.5, 0.5, f"Analysis for {model_name}",
                   ha='center', va='center',
                   fontsize=18, fontweight='bold')
            pdf.savefig(fig, bbox_inches='tight')
            plt.close()

            # 1. Feature Importance Plot
            if 'feature_importance' in model_info and model_info['feature_importance'] is not None:
                fig, ax = plt.subplots(figsize=(10, 6))
                fi = model_info['feature_importance'].sort_values(ascending=False).head(10)
                sns.barplot(x=fi.values, y=fi.index, ax=ax)
                ax.set_title(f'Feature Importance - {model_name}', fontsize=14)
                ax.set_xlabel('Importance Score')
                ax.set_ylabel('Features')
                pdf.savefig(fig, bbox_inches='tight')
                plt.close()

            # 2. Actual vs Predicted Values
            fig, ax = plt.subplots(figsize=(10, 6))
            ax.scatter(y_test, model_info['predictions'], alpha=0.5)
            ax.plot([y_test.min(), y_test.max()],
                   [y_test.min(), y_test.max()],
                   'k--', lw=2)
            ax.set_xlabel('Actual Values')
            ax.set_ylabel('Predicted Values')
            ax.set_title(f'Actual vs Predicted - {model_name}', fontsize=14)
            pdf.savefig(fig, bbox_inches='tight')
            plt.close()

            # 3. Residual Analysis
            fig, ax = plt.subplots(figsize=(10, 6))
            residuals = model_info['residuals']
            sns.histplot(residuals, kde=True, bins=30, ax=ax)
            ax.axvline(x=0, color='r', linestyle='--')
            ax.set_xlabel('Residuals (Actual - Predicted)')
            ax.set_title(f'Residual Distribution - {model_name}', fontsize=14)
            pdf.savefig(fig, bbox_inches='tight')
            plt.close()

            # 4. SHAP Summary Plot (if available)
            if 'shap_values' in model_info and model_info['shap_values'] is not None:
                fig, ax = plt.subplots(figsize=(10, 6))
                shap.summary_plot(model_info['shap_values'],
                                 model_info['processed_data'],
                                 feature_names=model_info['feature_names'],
                                 show=False)
                plt.title(f'SHAP Feature Impact - {model_name}', fontsize=14)
                plt.gcf().set_facecolor('white')
                pdf.savefig(fig, bbox_inches='tight')
                plt.close()

        # ============ Conclusion Page ============
        fig, ax = plt.subplots(figsize=(11, 8))
        ax.axis('off')

        # Find best model
        best_model = max(models_dict.items(),
                        key=lambda x: x[1]['metrics']['R2'])

        conclusion_text = [
            "Analysis Conclusion:",
            "",
            f"Best Performing Model: {best_model[0]} (R2: {best_model[1]['metrics']['R2']:.4f})",
            "",
            "Key Findings:",
            "- The most important features were consistently identified",
            "- Model showed good generalization performance",
            "- Residual analysis indicated normally distributed errors",
            "",
            "Recommendations:",
            "- Consider feature engineering for non-linear relationships",
            "- Monitor model performance on new data",
            "- Explore ensemble methods for potential improvement"
        ]

        ax.text(0.1, 0.9, "\n".join(conclusion_text),
               ha='left', va='top',
               fontsize=12, linespacing=1.5)

        pdf.savefig(fig, bbox_inches='tight')
        plt.close()

# Example Usage
if __name__ == "__main__":
    # 1. Load sample data
    data = fetch_california_housing()
    X = pd.DataFrame(data.data, columns=data.feature_names)
    y = pd.Series(data.target)

    # 2. Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # 3. Train model
    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

    # Calculate training time
    import time
    start_time = time.time()
    rf_model.fit(X_train, y_train)
    training_time = time.time() - start_time

    # 4. Make predictions
    rf_predictions = rf_model.predict(X_test)

    # 5. Calculate SHAP values (optional)
    try:
        explainer = shap.TreeExplainer(rf_model)
        rf_shap_values = explainer.shap_values(X_test)
    except Exception as e:
        print(f"SHAP calculation failed: {e}")
        rf_shap_values = None

    # 6. Prepare models dictionary
    models_dict = {
        'Random Forest': {
            'model': rf_model,
            'metrics': {
                'R2': r2_score(y_test, rf_predictions),
                'RMSE': np.sqrt(mean_squared_error(y_test, rf_predictions)),
                'MAE': mean_absolute_error(y_test, rf_predictions),
                'Training Time': training_time
            },
            'feature_importance': pd.Series(rf_model.feature_importances_,
                                          index=X_train.columns),
            'residuals': y_test - rf_predictions,
            'predictions': rf_predictions,
            'shap_values': rf_shap_values,
            'processed_data': X_test,
            'feature_names': X_train.columns.tolist()
        }
    }

    # 7. Generate report
    save_ml_report(models_dict, X_train, X_test, y_test, 'California_Housing_Model_Report.pdf')
    print("Report generated successfully!")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
import joblib
import os

def create_project_structure():
    """Create the required folder structure for the project"""
    folders = [
        'Original_Data',
        'Preprocessed_Data',
        'Results',
        'Python_Code'
    ]

    for folder in folders:
        os.makedirs(folder, exist_ok=True)
    print("[SUCCESS] Project folder structure created successfully")

def load_and_preprocess_data():
    """Load and preprocess the data"""
    print("\n[STATUS] Loading and preprocessing data...")

    # Example with California housing dataset (replace with your actual data)
    from sklearn.datasets import fetch_california_housing
    data = fetch_california_housing()
    X = pd.DataFrame(data.data, columns=data.feature_names)
    y = pd.Series(data.target)

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # Save processed data
    X_train.to_csv('Preprocessed_Data/X_train.csv', index=False)
    X_test.to_csv('Preprocessed_Data/X_test.csv', index=False)
    y_train.to_csv('Preprocessed_Data/y_train.csv', index=False)
    y_test.to_csv('Preprocessed_Data/y_test.csv', index=False)

    print("[SUCCESS] Data processed and saved in Preprocessed_Data folder")
    print(f"Training set shape: {X_train.shape}")
    print(f"Test set shape: {X_test.shape}")

    return X_train, X_test, y_train, y_test

def train_and_save_models(X_train, X_test, y_train, y_test):
    """Train models and save results"""
    print("\n[STATUS] Training machine learning models...")

    models = {
        'RF': RandomForestRegressor(n_estimators=100, random_state=42),
        'SVM': SVR()
    }

    results = {}

    for name, model in models.items():
        print(f"\nTraining {name} model...")

        # Train model
        model.fit(X_train, y_train)

        # Make predictions
        y_pred = model.predict(X_test)

        # Save model
        joblib.dump(model, f'Python_Code/{name}_model.pkl')

        # Save predictions
        pd.DataFrame(y_pred, columns=['Predictions']).to_csv(
            f'Results/predictions_{name}_model.csv', index=False
        )

        # Calculate metrics
        results[name] = {
            'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),
            'R2': r2_score(y_test, y_pred)
        }

        print(f"{name} model trained and saved successfully")

    # Save results
    pd.DataFrame(results).to_csv('Results/model_results.csv')
    print("\n[SUCCESS] All models trained and results saved")
    return results

def generate_report(results):
    """Generate a summary report of model performance"""
    print("\n=== MODEL PERFORMANCE REPORT ===")
    report_df = pd.DataFrame(results)
    print(report_df)

    # Save report
    with open('Results/report_summary.txt', 'w') as f:
        f.write("=== MODEL PERFORMANCE SUMMARY ===\n\n")
        f.write(report_df.to_string())

    print("\n[SUCCESS] Report summary saved to Results/report_summary.txt")

def main():
    print("=== MACHINE LEARNING PROJECT IMPLEMENTATION ===")

    # 1. Create folder structure
    create_project_structure()

    # 2. Load and preprocess data
    X_train, X_test, y_train, y_test = load_and_preprocess_data()

    # 3. Train models and save results
    results = train_and_save_models(X_train, X_test, y_train, y_test)

    # 4. Generate performance report
    generate_report(results)

    print("\n=== PROJECT IMPLEMENTATION COMPLETE ===")
    print("All required files and folders have been created")
    print("Please check the following folders:")
    print("- Original_Data/ (for raw data)")
    print("- Preprocessed_Data/ (for processed training/test data)")
    print("- Results/ (for model predictions and performance metrics)")
    print("- Python_Code/ (for saved model files)")

if __name__ == "__main__":
    main()

import os
import zipfile
from datetime import datetime

def create_zip_archive():
    """Create a zip archive of the complete project structure"""
    # Create timestamp for the zip filename
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    zip_filename = f"ML_Project_{timestamp}.zip"

    # Create a zip file
    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
        # Add all required folders and files
        for root, dirs, files in os.walk('Original_Data'):
            for file in files:
                zipf.write(os.path.join(root, file))

        for root, dirs, files in os.walk('Preprocessed_Data'):
            for file in files:
                zipf.write(os.path.join(root, file))

        for root, dirs, files in os.walk('Results'):
            for file in files:
                zipf.write(os.path.join(root, file))

        for root, dirs, files in os.walk('Python_Code'):
            for file in files:
                zipf.write(os.path.join(root, file))

        # Add any additional files (like README or report)
        if os.path.exists('report.pdf'):
            zipf.write('report.pdf')

    print(f"[SUCCESS] Project files compressed to {zip_filename}")
    return zip_filename

# Add this to your main() function after all other operations:
def main():
    # ... (previous code remains the same) ...

    # 5. Create zip archive
    zip_file = create_zip_archive()
    print(f"\nProject ready for submission. Please upload {zip_file}")

if __name__ == "__main__":
    main()

!pip install -U -q PyDrive

import os
import shutil
from google.colab import drive
from datetime import datetime

def upload_to_gdrive():
    """Upload project files to Google Drive"""

    # Mount Google Drive
    drive.mount('/content/drive')

    # Create project folder with timestamp
    project_name = "ML_Project_Submission"
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    gdrive_path = f"/content/drive/MyDrive/{project_name}_{timestamp}"

    # Create folder structure in Google Drive
    os.makedirs(gdrive_path, exist_ok=True)
    for folder in ['Original_Data', 'Preprocessed_Data', 'Results', 'Python_Code']:
        os.makedirs(f"{gdrive_path}/{folder}", exist_ok=True)

    # Copy files to Google Drive
    folders_to_copy = ['Original_Data', 'Preprocessed_Data', 'Results', 'Python_Code']

    for folder in folders_to_copy:
        if os.path.exists(folder):
            dest = f"{gdrive_path}/{folder}"
            shutil.copytree(folder, dest, dirs_exist_ok=True)
            print(f"[SUCCESS] Copied {folder} to Google Drive")

    # Copy additional files (like report)
    if os.path.exists('report.pdf'):
        shutil.copy('report.pdf', gdrive_path)

    print("\n[SUCCESS] All project files uploaded to Google Drive")
    print(f"Location: {gdrive_path}")

    # Generate shareable link
    drive_base_url = "https://drive.google.com/drive/folders/"
    folder_id = gdrive_path.split('/')[-1]
    shareable_link = f"{drive_base_url}{folder_id}"

    print(f"\nShareable Link: {shareable_link}")
    return shareable_link

# Add this to your main() function after creating all files
def main():
    # ... (الكود السابق يبقى كما هو) ...

    # 6. Upload to Google Drive
    if input("Upload to Google Drive? (y/n): ").lower() == 'y':
        drive_link = upload_to_gdrive()
        with open('drive_link.txt', 'w') as f:
            f.write(drive_link)
        print("Drive link saved to drive_link.txt")

if __name__ == "__main__":
    main()

import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
import joblib
import zipfile
from datetime import datetime

def create_project_structure():
    """Create the exact folder structure required"""
    required_folders = [
        'Original_Data',        # For raw unmodified data
        'Preprocessed_Data',    # For processed training/test data
        'Results',              # For model predictions
        'Python_Code'           # For Python scripts
    ]

    for folder in required_folders:
        os.makedirs(folder, exist_ok=True)
    print("Created required folder structure")

def process_and_save_data():
    """Process data and save in specified structure"""
    # Load sample dataset (replace with your actual data)
    from sklearn.datasets import fetch_california_housing
    data = fetch_california_housing()

    # Save original data (exactly as downloaded)
    pd.DataFrame(data.data).to_csv('Original_Data/raw_features.csv', index=False)
    pd.DataFrame(data.target).to_csv('Original_Data/raw_targets.csv', index=False)

    # Split data into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(
        data.data, data.target, test_size=0.2, random_state=42
    )

    # Save processed data in required format
    pd.DataFrame(X_train).to_csv('Preprocessed_Data/X_train.csv', index=False)
    pd.DataFrame(X_test).to_csv('Preprocessed_Data/X_test.csv', index=False)
    pd.DataFrame(y_train).to_csv('Preprocessed_Data/y_train.csv', index=False)
    pd.DataFrame(y_test).to_csv('Preprocessed_Data/y_test.csv', index=False)

    print("Processed and saved training/test data")

def train_and_save_models():
    """Train models and save predictions with clear naming"""
    # Load processed data
    X_train = pd.read_csv('Preprocessed_Data/X_train.csv').values
    X_test = pd.read_csv('Preprocessed_Data/X_test.csv').values
    y_train = pd.read_csv('Preprocessed_Data/y_train.csv').values.ravel()
    y_test = pd.read_csv('Preprocessed_Data/y_test.csv').values.ravel()

    # Define models to train
    models = {
        'RF': RandomForestRegressor(n_estimators=100, random_state=42),
        'SVM': SVR()
    }

    # Train each model and save results
    for model_name, model in models.items():
        # Train model
        model.fit(X_train, y_train)

        # Make predictions
        predictions = model.predict(X_test)

        # Save model
        joblib.dump(model, f'Python_Code/{model_name}_model.pkl')

        # Save predictions with clear naming
        pd.DataFrame(predictions).to_csv(
            f'Results/predictions_{model_name}_model.csv',
            index=False,
            header=['Predictions']
        )

        # Calculate performance metrics
        rmse = np.sqrt(mean_squared_error(y_test, predictions))
        r2 = r2_score(y_test, predictions)

        print(f"{model_name} Model - RMSE: {rmse:.4f}, R2: {r2:.4f}")

def create_submission_zip():
    """Create final submission zip file"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    zip_filename = f"ML_Project_Submission_{timestamp}.zip"

    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for folder in ['Original_Data', 'Preprocessed_Data', 'Results', 'Python_Code']:
            for root, dirs, files in os.walk(folder):
                for file in files:
                    zipf.write(os.path.join(root, file))

    print(f"Created submission zip file: {zip_filename}")
    return zip_filename

def main():
    print("=== MACHINE LEARNING PROJECT IMPLEMENTATION ===")

    # 1. Create required folder structure
    create_project_structure()

    # 2. Process and save data
    process_and_save_data()

    # 3. Train models and save results
    train_and_save_models()

    # 4. Create submission package
    submission_zip = create_submission_zip()

    # Final instructions
    print("\n=== SUBMISSION INSTRUCTIONS ===")
    print(f"1. Final submission file: {submission_zip}")
    print("2. Upload this file to:")
    print("   - GitHub repository")
    print("   - Google Drive")
    print("3. Include these links in your PDF report")
    print("4. Verify your submission contains:")
    print("   - Original_Data/ (raw data files)")
    print("   - Preprocessed_Data/ (X_train.csv, X_test.csv, y_train.csv, y_test.csv)")
    print("   - Results/ (predictions_*_model.csv files)")
    print("   - Python_Code/ (model scripts and saved models)")

if __name__ == "__main__":
    main()

import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
import joblib
import shutil
from datetime import datetime

def create_project_structure(base_path):
    """Create the required folder structure on C: drive"""
    folders = [
        'Original_Data',
        'Preprocessed_Data',
        'Results',
        'Python_Code'
    ]

    for folder in folders:
        os.makedirs(os.path.join(base_path, folder), exist_ok=True)
    print(f"Created folder structure at: {base_path}")

def process_and_save_data(base_path):
    """Process data and save in specified structure"""
    from sklearn.datasets import fetch_california_housing
    data = fetch_california_housing()

    # Save original data
    pd.DataFrame(data.data).to_csv(os.path.join(base_path, 'Original_Data/raw_features.csv'), index=False)
    pd.DataFrame(data.target).to_csv(os.path.join(base_path, 'Original_Data/raw_targets.csv'), index=False)

    # Split and save processed data
    X_train, X_test, y_train, y_test = train_test_split(
        data.data, data.target, test_size=0.2, random_state=42
    )

    pd.DataFrame(X_train).to_csv(os.path.join(base_path, 'Preprocessed_Data/X_train.csv'), index=False)
    pd.DataFrame(X_test).to_csv(os.path.join(base_path, 'Preprocessed_Data/X_test.csv'), index=False)
    pd.DataFrame(y_train).to_csv(os.path.join(base_path, 'Preprocessed_Data/y_train.csv'), index=False)
    pd.DataFrame(y_test).to_csv(os.path.join(base_path, 'Preprocessed_Data/y_test.csv'), index=False)

    print("Data processed and saved successfully")

def train_and_save_models(base_path):
    """Train models and save results with clear naming"""
    # Load data
    X_train = pd.read_csv(os.path.join(base_path, 'Preprocessed_Data/X_train.csv')).values
    X_test = pd.read_csv(os.path.join(base_path, 'Preprocessed_Data/X_test.csv')).values
    y_train = pd.read_csv(os.path.join(base_path, 'Preprocessed_Data/y_train.csv')).values.ravel()
    y_test = pd.read_csv(os.path.join(base_path, 'Preprocessed_Data/y_test.csv')).values.ravel()

    # Define and train models
    models = {
        'RF': RandomForestRegressor(n_estimators=100, random_state=42),
        'SVM': SVR()
    }

    for name, model in models.items():
        model.fit(X_train, y_train)
        predictions = model.predict(X_test)

        # Save model
        joblib.dump(model, os.path.join(base_path, f'Python_Code/{name}_model.pkl'))

        # Save predictions
        pd.DataFrame(predictions).to_csv(
            os.path.join(base_path, f'Results/predictions_{name}_model.csv'),
            index=False,
            header=['Predictions']
        )

        # Print performance
        rmse = np.sqrt(mean_squared_error(y_test, predictions))
        r2 = r2_score(y_test, predictions)
        print(f"{name} Model - RMSE: {rmse:.4f}, R2: {r2:.4f}")

def create_backup_zip(base_path):
    """Create backup zip file of the project"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    zip_name = f"ML_Project_Backup_{timestamp}.zip"
    zip_path = os.path.join(base_path, zip_name)

    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for folder in ['Original_Data', 'Preprocessed_Data', 'Results', 'Python_Code']:
            folder_path = os.path.join(base_path, folder)
            for root, dirs, files in os.walk(folder_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    arcname = os.path.relpath(file_path, base_path)
                    zipf.write(file_path, arcname)

    print(f"Backup created: {zip_path}")
    return zip_path

def main():
    # Set base path on C: drive
    base_path = "C:\\ML_Project_Submission"

    print("=== MACHINE LEARNING PROJECT IMPLEMENTATION ===")
    print(f"Saving all files to: {base_path}")

    # 1. Create folder structure
    create_project_structure(base_path)

    # 2. Process and save data
    process_and_save_data(base_path)

    # 3. Train models and save results
    train_and_save_models(base_path)

    # 4. Create backup zip
    backup_zip = create_backup_zip(base_path)

    print("\n=== PROJECT SUCCESSFULLY SAVED TO C: DRIVE ===")
    print(f"Main project location: {base_path}")
    print(f"Backup zip created: {backup_zip}")
    print("\nFolder structure created:")
    print(f"C:\\ML_Project_Submission")
    print("├── Original_Data/")
    print("├── Preprocessed_Data/")
    print("├── Results/")
    print("└── Python_Code/")

if __name__ == "__main__":
    main()

# Sarah Khaled Al-Tuwairqi - AI Project Implementation
# Import Required Libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import metrics, svm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.impute import SimpleImputer
import datetime
import gdown
import os
import joblib
from zipfile import ZipFile

def create_project_structure():
    """Create the required folder structure for the project"""
    folders = [
        'Original_Data',
        'Preprocessed_Data',
        'Results',
        'Python_Code'
    ]

    for folder in folders:
        os.makedirs(folder, exist_ok=True)
    print("[STATUS] Created project folder structure")

def load_and_process_data():
    """Load and preprocess the data"""
    print("\n[STATUS] Loading and preprocessing data...")

    # Load data from Google Drive
    file_id = '1hkebti9cgRECbze77SKfm8b_eGBJkTnL'
    url = f'https://drive.google.com/uc?id={file_id}'
    output = 'Original_Data/ai_ghibli_trend_dataset.csv'

    try:
        gdown.download(url, output, quiet=False)
        AISG = pd.read_csv(output)
        print("[SUCCESS] Data loaded from Google Drive")
    except Exception as e:
        print(f"[WARNING] Error loading data: {e}")
        print("[STATUS] Using sample data instead...")
        data = {
            'image_id': range(1, 101),
            'likes': np.random.randint(100, 10000, 100),
            'shares': np.random.randint(10, 1000, 100),
            'platform': np.random.choice(['Instagram', 'TikTok', 'Twitter'], 100)
        }
        AISG = pd.DataFrame(data)
        AISG.to_csv(output, index=False)

    # Data cleaning and processing
    if 'platform' not in AISG.columns:
        AISG['platform'] = 'Unknown'
    AISG["platform"] = AISG["platform"].str.strip().str.replace(" ", "_")

    # Save processed data
    X_train, X_test, y_train, y_test = train_test_split(
        AISG.drop('likes', axis=1),
        AISG['likes'],
        test_size=0.2,
        random_state=42
    )

    X_train.to_csv('Preprocessed_Data/X_train.csv', index=False)
    X_test.to_csv('Preprocessed_Data/X_test.csv', index=False)
    y_train.to_csv('Preprocessed_Data/y_train.csv', index=False)
    y_test.to_csv('Preprocessed_Data/y_test.csv', index=False)

    print("[SUCCESS] Data processed and saved")
    return AISG

def train_and_evaluate_models():
    """Train machine learning models and save results"""
    print("\n[STATUS] Training machine learning models...")

    # Load processed data
    X_train = pd.read_csv('Preprocessed_Data/X_train.csv')
    X_test = pd.read_csv('Preprocessed_Data/X_test.csv')
    y_train = pd.read_csv('Preprocessed_Data/y_train.csv').values.ravel()
    y_test = pd.read_csv('Preprocessed_Data/y_test.csv').values.ravel()

    # Encode categorical data
    le = LabelEncoder()
    X_train['platform'] = le.fit_transform(X_train['platform'])
    X_test['platform'] = le.transform(X_test['platform'])

    # Train models
    models = {
        'LinearRegression': LinearRegression(),
        'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42)
    }

    results = {}

    for name, model in models.items():
        print(f"\nTraining {name} model...")
        model.fit(X_train, y_train)

        # Save model
        joblib.dump(model, f'Python_Code/{name}_model.pkl')

        # Make predictions
        predictions = model.predict(X_test)
        pd.DataFrame(predictions).to_csv(
            f'Results/predictions_{name}_model.csv',
            index=False,
            header=['Predictions']
        )

        # Evaluate
        results[name] = {
            'RMSE': np.sqrt(mean_squared_error(y_test, predictions)),
            'R2': r2_score(y_test, predictions)
        }
        print(f"{name} performance:")
        print(f"RMSE: {results[name]['RMSE']:.2f}")
        print(f"R2 Score: {results[name]['R2']:.2f}")

    # Save results
    pd.DataFrame(results).to_csv('Results/model_performance.csv')
    print("\n[SUCCESS] All models trained and results saved")

def create_visualizations():
    """Create and save data visualizations"""
    print("\n[STATUS] Creating data visualizations...")

    AISG = pd.read_csv('Original_Data/ai_ghibli_trend_dataset.csv')

    # Platform distribution
    plt.figure(figsize=(10, 6))
    sns.countplot(x='platform', data=AISG)
    plt.title('Distribution of Posts by Platform')
    plt.xticks(rotation=45)
    plt.savefig('Results/platform_distribution.png')
    plt.close()

    # Likes distribution
    plt.figure(figsize=(10, 6))
    sns.boxplot(x='platform', y='likes', data=AISG)
    plt.title('Likes Distribution by Platform')
    plt.xticks(rotation=45)
    plt.savefig('Results/likes_distribution.png')
    plt.close()

    print("[SUCCESS] Visualizations saved to Results folder")

def create_submission_zip():
    """Create final submission zip file"""
    print("\n[STATUS] Creating submission package...")

    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    zip_name = f"AI_Project_Submission_{timestamp}.zip"

    with ZipFile(zip_name, 'w') as zipf:
        for folder in ['Original_Data', 'Preprocessed_Data', 'Results', 'Python_Code']:
            for root, dirs, files in os.walk(folder):
                for file in files:
                    zipf.write(os.path.join(root, file))

    print(f"[SUCCESS] Submission package created: {zip_name}")
    return zip_name

def main():
    print("=== AI PROJECT IMPLEMENTATION ===")
    print("Developer: Sarah Khaled Al-Tuwairqi")
    print("=================================")

    # 1. Create project structure
    create_project_structure()

    # 2. Load and process data
    load_and_process_data()

    # 3. Train and evaluate models
    train_and_evaluate_models()

    # 4. Create visualizations
    create_visualizations()

    # 5. Create submission package
    zip_file = create_submission_zip()

    # Final instructions
    print("\n=== SUBMISSION INSTRUCTIONS ===")
    print(f"1. Submit the zip file: {zip_file}")
    print("2. Folder structure includes:")
    print("   - Original_Data/ (raw dataset)")
    print("   - Preprocessed_Data/ (cleaned training/test data)")
    print("   - Results/ (predictions and visualizations)")
    print("   - Python_Code/ (trained models and code)")
    print("3. Include this zip file in your PDF report")

if __name__ == "__main__":
    main()

# Sarah Khaled Al-Tuwairqi - AI Project Implementation
# Import Required Libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import metrics, svm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.impute import SimpleImputer
import datetime
import gdown
import os
import joblib
from zipfile import ZipFile

def create_project_structure():
    """Create the required folder structure for the project"""
    folders = [
        'Original_Data',
        'Preprocessed_Data',
        'Results',
        'Python_Code'
    ]

    for folder in folders:
        os.makedirs(folder, exist_ok=True)
    print("[STATUS] Created project folder structure")

def load_and_process_data():
    """Load and preprocess the data"""
    print("\n[STATUS] Loading and preprocessing data...")

    # Load data from Google Drive
    file_id = '1hkebti9cgRECbze77SKfm8b_eGBJkTnL'
    url = f'https://drive.google.com/uc?id={file_id}'
    output = 'Original_Data/ai_ghibli_trend_dataset.csv'

    try:
        gdown.download(url, output, quiet=False)
        AISG = pd.read_csv(output)
        print("[SUCCESS] Data loaded from Google Drive")
    except Exception as e:
        print(f"[WARNING] Error loading data: {e}")
        print("[STATUS] Using sample data instead...")
        data = {
            'image_id': [f'id_{i}' for i in range(1, 101)], # Use a generic ID format
            'likes': np.random.randint(100, 10000, 100),
            'shares': np.random.randint(10, 1000, 100),
            'platform': np.random.choice(['Instagram', 'TikTok', 'Twitter'], 100)
        }
        AISG = pd.DataFrame(data)
        AISG.to_csv(output, index=False)

    # Data cleaning and processing
    if 'platform' not in AISG.columns:
        AISG['platform'] = 'Unknown'
    AISG["platform"] = AISG["platform"].str.strip().str.replace(" ", "_")

    # Define features (X) and target (y) - DROP 'image_id'
    X = AISG.drop(['likes', 'image_id'], axis=1) # Drop 'image_id' here
    y = AISG['likes']


    # Split data into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(
        X, # Use the feature set without 'image_id'
        y,
        test_size=0.2,
        random_state=42
    )

    X_train.to_csv('Preprocessed_Data/X_train.csv', index=False)
    X_test.to_csv('Preprocessed_Data/X_test.csv', index=False)
    y_train.to_csv('Preprocessed_Data/y_train.csv', index=False)
    y_test.to_csv('Preprocessed_Data/y_test.csv', index=False)

    print("[SUCCESS] Data processed and saved")
    return AISG # Return the original dataframe for visualizations if needed

def train_and_evaluate_models():
    """Train machine learning models and save results"""
    print("\n[STATUS] Training machine learning models...")

    # Load processed data
    X_train = pd.read_csv('Preprocessed_Data/X_train.csv')
    X_test = pd.read_csv('Preprocessed_Data/X_test.csv')
    y_train = pd.read_csv('Preprocessed_Data/y_train.csv').values.ravel()
    y_test = pd.read_csv('Preprocessed_Data/y_test.csv').values.ravel()

    # Encode categorical data BEFORE training
    le = LabelEncoder()
    # Ensure 'platform' column exists before encoding
    if 'platform' in X_train.columns:
        X_train['platform'] = le.fit_transform(X_train['platform'])
        X_test['platform'] = le.transform(X_test['platform'])
    else:
        print("[WARNING] 'platform' column not found in data for encoding.")


    # Train models
    models = {
        'LinearRegression': LinearRegression(),
        'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42)
    }

    results = {}

    for name, model in models.items():
        print(f"\nTraining {name} model...")
        # Check if X_train contains non-numeric types before fitting
        for col in X_train.columns:
             if not pd.api.types.is_numeric_dtype(X_train[col]):
                 print(f"[ERROR] Column '{col}' is not numeric (type: {X_train[col].dtype}). Please fix preprocessing.")
                 # Optionally raise an error or skip the model if non-numeric data persists
                 # raise TypeError(f"Non-numeric column '{col}' found.")

        model.fit(X_train, y_train)

        # Save model
        joblib.dump(model, f'Python_Code/{name}_model.pkl')

        # Make predictions
        predictions = model.predict(X_test)
        pd.DataFrame(predictions).to_csv(
            f'Results/predictions_{name}_model.csv',
            index=False,
            header=['Predictions']
        )

        # Evaluate
        results[name] = {
            'RMSE': np.sqrt(mean_squared_error(y_test, predictions)),
            'R2': r2_score(y_test, predictions)
        }
        print(f"{name} performance:")
        print(f"RMSE: {results[name]['RMSE']:.2f}")
        print(f"R2 Score: {results[name]['R2']:.2f}")

    # Save results
    pd.DataFrame(results).to_csv('Results/model_performance.csv')
    print("\n[SUCCESS] All models trained and results saved")

def create_visualizations():
    """Create and save data visualizations"""
    print("\n[STATUS] Creating data visualizations...")

    # Load the original data for visualizations
    # This ensures visualizations use the original data structure if needed
    try:
        AISG = pd.read_csv('Original_Data/ai_ghibli_trend_dataset.csv')
        # Clean 'platform' column for visualization if necessary
        if 'platform' in AISG.columns:
            AISG["platform"] = AISG["platform"].str.strip().str.replace(" ", "_")

        # Platform distribution
        plt.figure(figsize=(10, 6))
        sns.countplot(x='platform', data=AISG)
        plt.title('Distribution of Posts by Platform')
        plt.xticks(rotation=45)
        plt.tight_layout() # Adjust layout to prevent labels overlapping
        plt.savefig('Results/platform_distribution.png')
        plt.close()

        # Likes distribution
        plt.figure(figsize=(10, 6))
        sns.boxplot(x='platform', y='likes', data=AISG)
        plt.title('Likes Distribution by Platform')
        plt.xticks(rotation=45)
        plt.tight_layout() # Adjust layout
        plt.savefig('Results/likes_distribution.png')
        plt.close()

        print("[SUCCESS] Visualizations saved to Results folder")

    except FileNotFoundError:
        print("[WARNING] Original data file not found for visualizations.")
    except Exception as e:
        print(f"[ERROR] Error creating visualizations: {e}")


def create_submission_zip():
    """Create final submission zip file"""
    print("\n[STATUS] Creating submission package...")

    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    zip_name = f"AI_Project_Submission_{timestamp}.zip"

    with ZipFile(zip_name, 'w') as zipf:
        for folder in ['Original_Data', 'Preprocessed_Data', 'Results', 'Python_Code']:
            for root, dirs, files in os.walk(folder):
                for file in files:
                    file_path = os.path.join(root, file)
                    zipf.write(file_path, arcname=os.path.relpath(file_path, os.getcwd())) # Use relative path

    print(f"[SUCCESS] Submission package created: {zip_name}")
    return zip_name

def main():
    print("=== AI PROJECT IMPLEMENTATION ===")
    print("Developer: Sarah Khaled Al-Tuwairqi")
    print("=================================")

    # 1. Create project structure
    create_project_structure()

    # 2. Load and process data
    load_and_process_data() # This function now saves the split data

    # 3. Train and evaluate models
    train_and_evaluate_models() # This function now loads the split data

    # 4. Create visualizations
    create_visualizations()

    # 5. Create submission package
    zip_file = create_submission_zip()

    # Final instructions
    print("\n=== SUBMISSION INSTRUCTIONS ===")
    print(f"1. Submit the zip file: {zip_file}")
    print("2. Folder structure includes:")
    print("   - Original_Data/ (raw dataset)")
    print("   - Preprocessed_Data/ (cleaned training/test data)")
    print("   - Results/ (predictions and visualizations)")
    print("   - Python_Code/ (trained models and code)")
    print("3. Include this zip file in your PDF report")

if __name__ == "__main__":
    main()

# Sarah Khaled Al-Tuwairqi - AI Project Implementation
# Import Required Libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import metrics, svm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.impute import SimpleImputer
import datetime
import gdown
import os
import joblib
from zipfile import ZipFile

def create_project_structure():
    """Create the required folder structure for the project"""
    folders = [
        'Original_Data',
        'Preprocessed_Data',
        'Results',
        'Python_Code'
    ]

    for folder in folders:
        os.makedirs(folder, exist_ok=True)
    print("[STATUS] Created project folder structure")

def load_and_process_data():
    """Load and preprocess the data"""
    print("\n[STATUS] Loading and preprocessing data...")

    # Load data from Google Drive
    file_id = '1hkebti9cgRECbze77SKfm8b_eGBJkTnL'
    url = f'https://drive.google.com/uc?id={file_id}'
    output = 'Original_Data/ai_ghibli_trend_dataset.csv'

    try:
        gdown.download(url, output, quiet=False)
        AISG = pd.read_csv(output)
        print("[SUCCESS] Data loaded from Google Drive")
    except Exception as e:
        print(f"[WARNING] Error loading data: {e}")
        print("[STATUS] Using sample data instead...")
        data = {
            # Use a generic ID format that won't cause issues later
            'image_id': [f'id_{i}' for i in range(1, 101)],
            'likes': np.random.randint(100, 10000, 100),
            'shares': np.random.randint(10, 1000, 100),
            'platform': np.random.choice(['Instagram', 'TikTok', 'Twitter'], 100)
        }
        AISG = pd.DataFrame(data)
        AISG.to_csv(output, index=False)

    # Data cleaning and processing
    if 'platform' not in AISG.columns:
        AISG['platform'] = 'Unknown'
    AISG["platform"] = AISG["platform"].str.strip().str.replace(" ", "_")

    # Define features (X) and target (y) - DROP 'image_id' and 'likes'
    # Ensure 'image_id' is dropped here for both real and sample data paths
    columns_to_drop = ['likes']
    if 'image_id' in AISG.columns:
         columns_to_drop.append('image_id')

    X = AISG.drop(columns=columns_to_drop, axis=1)
    y = AISG['likes']


    # Split data into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(
        X, # Use the feature set without 'image_id'
        y,
        test_size=0.2,
        random_state=42
    )

    # Save processed data into the correct folder
    X_train.to_csv('Preprocessed_Data/X_train.csv', index=False)
    X_test.to_csv('Preprocessed_Data/X_test.csv', index=False)
    y_train.to_csv('Preprocessed_Data/y_train.csv', index=False)
    y_test.to_csv('Preprocessed_Data/y_test.csv', index=False)

    print("[SUCCESS] Data processed and saved")
    return AISG # Return the original dataframe for visualizations if needed

def train_and_evaluate_models():
    """Train machine learning models and save results"""
    print("\n[STATUS] Training machine learning models...")

    # Load processed data
    X_train = pd.read_csv('Preprocessed_Data/X_train.csv')
    X_test = pd.read_csv('Preprocessed_Data/X_test.csv')
    y_train = pd.read_csv('Preprocessed_Data/y_train.csv').values.ravel()
    y_test = pd.read_csv('Preprocessed_Data/y_test.csv').values.ravel()

    # Encode categorical data BEFORE training
    # Ensure 'platform' column exists before encoding
    if 'platform' in X_train.columns:
        le = LabelEncoder() # Initialize LabelEncoder here
        X_train['platform'] = le.fit_transform(X_train['platform'])
        X_test['platform'] = le.transform(X_test['platform'])
        print("[STATUS] 'platform' column encoded.")
    else:
        print("[WARNING] 'platform' column not found in data for encoding.")


    # Train models
    models = {
        'LinearRegression': LinearRegression(),
        'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42)
    }

    results = {}

    for name, model in models.items():
        print(f"\nTraining {name} model...")
        # Check if X_train contains non-numeric types before fitting (optional, but good practice)
        for col in X_train.columns:
             if not pd.api.types.is_numeric_dtype(X_train[col]):
                 print(f"[ERROR] Column '{col}' is not numeric (type: {X_train[col].dtype}). This will cause an error.")
                 # Depending on the data, you might need more preprocessing here (e.g., more encoding)
                 # For now, we expect dropping 'image_id' and encoding 'platform' to fix this
                 # raise TypeError(f"Non-numeric column '{col}' found.") # Uncomment to halt execution on error


        try:
            model.fit(X_train, y_train)

            # Save model
            joblib.dump(model, f'Python_Code/{name}_model.pkl')

            # Make predictions
            predictions = model.predict(X_test)
            pd.DataFrame(predictions).to_csv(
                f'Results/predictions_{name}_model.csv',
                index=False,
                header=['Predictions']
            )

            # Evaluate
            results[name] = {
                'RMSE': np.sqrt(mean_squared_error(y_test, predictions)),
                'R2': r2_score(y_test, predictions)
            }
            print(f"{name} performance:")
            print(f"RMSE: {results[name]['RMSE']:.2f}")
            print(f"R2 Score: {results[name]['R2']:.2f}")
        except Exception as e:
            print(f"[ERROR] Failed to train or evaluate {name} model: {e}")
            results[name] = {'RMSE': np.nan, 'R2': np.nan} # Mark as failed

    # Save results
    pd.DataFrame.from_dict(results, orient='index').to_csv('Results/model_performance.csv')
    print("\n[SUCCESS] Model training and evaluation completed")


def create_visualizations():
    """Create and save data visualizations"""
    print("\n[STATUS] Creating data visualizations...")

    # Load the original data for visualizations
    # This ensures visualizations use the original data structure if needed
    try:
        AISG = pd.read_csv('Original_Data/ai_ghibli_trend_dataset.csv')
        # Clean 'platform' column for visualization if necessary
        if 'platform' in AISG.columns:
            AISG["platform"] = AISG["platform"].str.strip().str.replace(" ", "_")

        # Platform distribution
        plt.figure(figsize=(10, 6))
        sns.countplot(x='platform', data=AISG)
        plt.title('Distribution of Posts by Platform')
        plt.xticks(rotation=45)
        plt.tight_layout() # Adjust layout to prevent labels overlapping
        plt.savefig('Results/platform_distribution.png')
        plt.close()

        # Likes distribution by platform
        plt.figure(figsize=(10, 6))
        if 'platform' in AISG.columns and 'likes' in AISG.columns:
             sns.boxplot(x='platform', y='likes', data=AISG)
             plt.title('Likes Distribution by Platform')
             plt.xticks(rotation=45)
             plt.tight_layout() # Adjust layout
             plt.savefig('Results/likes_distribution.png')
             plt.close()
        else:
             print("[WARNING] Missing 'platform' or 'likes' column for Likes Distribution plot.")


        print("[SUCCESS] Visualizations saved to Results folder")

    except FileNotFoundError:
        print("[WARNING] Original data file not found for visualizations.")
    except Exception as e:
        print(f"[ERROR] Error creating visualizations: {e}")


def create_submission_zip():
    """Create final submission zip file"""
    print("\n[STATUS] Creating submission package...")

    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    zip_name = f"AI_Project_Submission_{timestamp}.zip"

    with ZipFile(zip_name, 'w') as zipf:
        for folder in ['Original_Data', 'Preprocessed_Data', 'Results', 'Python_Code']:
            # Ensure the folder exists before trying to walk it
            if os.path.exists(folder):
                for root, dirs, files in os.walk(folder):
                    for file in files:
                        file_path = os.path.join(root, file)
                        # Use arcname=os.path.relpath(...) to maintain folder structure inside zip
                        arcname = os.path.relpath(file_path, os.getcwd())
                        zipf.write(file_path, arcname=arcname)
            else:
                print(f"[WARNING] Folder '{folder}' not found, skipping.")

    print(f"[SUCCESS] Submission package created: {zip_name}")
    return zip_name

def main():
    print("=== AI PROJECT IMPLEMENTATION ===")
    print("Developer: Sarah Khaled Al-Tuwairqi")
    print("=================================")

    # 1. Create project structure
    create_project_structure()

    # 2. Load and process data
    # This function now loads the data and saves the split training/test sets
    load_and_process_data()

    # 3. Train and evaluate models
    # This function now loads the split data and trains/evaluates models
    train_and_evaluate_models()

    # 4. Create visualizations
    create_visualizations()

    # 5. Create submission package
    zip_file = create_submission_zip()

    # Final instructions
    print("\n=== SUBMISSION INSTRUCTIONS ===")
    print(f"1. Submit the zip file: {zip_file}")
    print("2. Folder structure includes:")
    print("   - Original_Data/ (raw dataset)")
    print("   - Preprocessed_Data/ (cleaned training/test data)")
    print("   - Results/ (predictions and visualizations)")
    print("   - Python_Code/ (trained models and code)")
    print("3. Include this zip file in your PDF report")

if __name__ == "__main__":
    main()

# Sarah Khaled Al-Tuwairqi - AI Project Implementation
# Import Required Libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import metrics, svm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.impute import SimpleImputer
import datetime
import gdown
import os
import joblib
from zipfile import ZipFile

def create_project_structure():
    """Create the required folder structure for the project"""
    folders = [
        'Original_Data',
        'Preprocessed_Data',
        'Results',
        'Python_Code'
    ]

    for folder in folders:
        os.makedirs(folder, exist_ok=True)
    print("[STATUS] Created project folder structure")

def load_and_process_data():
    """Load and preprocess the data"""
    print("\n[STATUS] Loading and preprocessing data...")

    # Load data from Google Drive
    file_id = '1hkebti9cgRECbze77SKfm8b_eGBJkTnL'
    url = f'https://drive.google.com/uc?id={file_id}'
    output = 'Original_Data/ai_ghibli_trend_dataset.csv'

    try:
        gdown.download(url, output, quiet=False)
        AISG = pd.read_csv(output)
        print("[SUCCESS] Data loaded from Google Drive")
    except Exception as e:
        print(f"[WARNING] Error loading data: {e}")
        print("[STATUS] Using sample data instead...")
        data = {
            'image_id': range(1, 101),
            'likes': np.random.randint(100, 10000, 100),
            'shares': np.random.randint(10, 1000, 100),
            'platform': np.random.choice(['Instagram', 'TikTok', 'Twitter'], 100)
        }
        AISG = pd.DataFrame(data)
        AISG.to_csv(output, index=False)

    # Data cleaning and processing
    if 'platform' not in AISG.columns:
        AISG['platform'] = 'Unknown'
    AISG["platform"] = AISG["platform"].str.strip().str.replace(" ", "_")

    # Save processed data (excluding image_id)
    X_train, X_test, y_train, y_test = train_test_split(
        AISG.drop(['likes', 'image_id'], axis=1),  # إزالة العمودين
        AISG['likes'],
        test_size=0.2,
        random_state=42
    )

    X_train.to_csv('Preprocessed_Data/X_train.csv', index=False)
    X_test.to_csv('Preprocessed_Data/X_test.csv', index=False)
    y_train.to_csv('Preprocessed_Data/y_train.csv', index=False)
    y_test.to_csv('Preprocessed_Data/y_test.csv', index=False)

    print("[SUCCESS] Data processed and saved")
    return AISG

def train_and_evaluate_models():
    """Train machine learning models and save results"""
    print("\n[STATUS] Training machine learning models...")

    # Load processed data
    X_train = pd.read_csv('Preprocessed_Data/X_train.csv')
    X_test = pd.read_csv('Preprocessed_Data/X_test.csv')
    y_train = pd.read_csv('Preprocessed_Data/y_train.csv').values.ravel()
    y_test = pd.read_csv('Preprocessed_Data/y_test.csv').values.ravel()

    # Encode categorical data
    le = LabelEncoder()
    X_train['platform'] = le.fit_transform(X_train['platform'])
    X_test['platform'] = le.transform(X_test['platform'])

    # Train models
    models = {
        'LinearRegression': LinearRegression(),
        'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42)
    }

    results = {}

    for name, model in models.items():
        print(f"\nTraining {name} model...")
        model.fit(X_train, y_train)

        # Save model
        joblib.dump(model, f'Python_Code/{name}_model.pkl')

        # Make predictions
        predictions = model.predict(X_test)
        pd.DataFrame(predictions).to_csv(
            f'Results/predictions_{name}_model.csv',
            index=False,
            header=['Predictions']
        )

        # Evaluate
        results[name] = {
            'RMSE': np.sqrt(mean_squared_error(y_test, predictions)),
            'R2': r2_score(y_test, predictions)
        }
        print(f"{name} performance:")
        print(f"RMSE: {results[name]['RMSE']:.2f}")
        print(f"R2 Score: {results[name]['R2']:.2f}")

    # Save results
    pd.DataFrame(results).to_csv('Results/model_performance.csv')
    print("\n[SUCCESS] All models trained and results saved")

def create_visualizations():
    """Create and save data visualizations"""
    print("\n[STATUS] Creating data visualizations...")

    AISG = pd.read_csv('Original_Data/ai_ghibli_trend_dataset.csv')

    # Platform distribution
    plt.figure(figsize=(10, 6))
    sns.countplot(x='platform', data=AISG)
    plt.title('Distribution of Posts by Platform')
    plt.xticks(rotation=45)
    plt.savefig('Results/platform_distribution.png')
    plt.close()

    # Likes distribution
    plt.figure(figsize=(10, 6))
    sns.boxplot(x='platform', y='likes', data=AISG)
    plt.title('Likes Distribution by Platform')
    plt.xticks(rotation=45)
    plt.savefig('Results/likes_distribution.png')
    plt.close()

    print("[SUCCESS] Visualizations saved to Results folder")

def create_submission_zip():
    """Create final submission zip file"""
    print("\n[STATUS] Creating submission package...")

    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    zip_name = f"AI_Project_Submission_{timestamp}.zip"

    with ZipFile(zip_name, 'w') as zipf:
        for folder in ['Original_Data', 'Preprocessed_Data', 'Results', 'Python_Code']:
            for root, dirs, files in os.walk(folder):
                for file in files:
                    zipf.write(os.path.join(root, file))

    print(f"[SUCCESS] Submission package created: {zip_name}")
    return zip_name

def main():
    print("=== AI PROJECT IMPLEMENTATION ===")
    print("Developer: Sarah Khaled Al-Tuwairqi")
    print("=================================")

    # 1. Create project structure
    create_project_structure()

    # 2. Load and process data
    load_and_process_data()

    # 3. Train and evaluate models
    train_and_evaluate_models()

    # 4. Create visualizations
    create_visualizations()

    # 5. Create submission package
    zip_file = create_submission_zip()

    # Final instructions
    print("\n=== SUBMISSION INSTRUCTIONS ===")
    print(f"1. Submit the zip file: {zip_file}")
    print("2. Folder structure includes:")
    print("   - Original_Data/ (raw dataset)")
    print("   - Preprocessed_Data/ (cleaned training/test data)")
    print("   - Results/ (predictions and visualizations)")
    print("   - Python_Code/ (trained models and code)")
    print("3. Include this zip file in your PDF report")

if __name__ == "__main__":
    main()

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import metrics, svm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.impute import SimpleImputer
import datetime
import gdown
import os
import joblib
from zipfile import ZipFile

def create_project_structure():
    """Create the required folder structure for the project"""
    folders = [
        'Original_Data',
        'Preprocessed_Data',
        'Results',
        'Python_Code'
    ]

    for folder in folders:
        os.makedirs(folder, exist_ok=True)
    print("[STATUS] Created project folder structure")

def load_and_process_data():
    """Load and preprocess the data"""
    print("\n[STATUS] Loading and preprocessing data...")

    # Load data from Google Drive
    file_id = '1hkebti9cgRECbze77SKfm8b_eGBJkTnL'
    url = f'https://drive.google.com/uc?id={file_id}'
    output = 'Original_Data/ai_ghibli_trend_dataset.csv'

    try:
        gdown.download(url, output, quiet=False)
        AISG = pd.read_csv(output)
        print("[SUCCESS] Data loaded from Google Drive")
    except Exception as e:
        print(f"[WARNING] Error loading data: {e}")
        print("[STATUS] Using sample data instead...")
        data = {
            'image_id': range(1, 101),
            'likes': np.random.randint(100, 10000, 100),
            'shares': np.random.randint(10, 1000, 100),
            'platform': np.random.choice(['Instagram', 'TikTok', 'Twitter'], 100)
        }
        AISG = pd.DataFrame(data)
        AISG.to_csv(output, index=False)

    # Data cleaning and processing
    if 'platform' not in AISG.columns:
        AISG['platform'] = 'Unknown'
    AISG["platform"] = AISG["platform"].str.strip().str.replace(" ", "_")

    # Save processed data (excluding image_id)
    X_train, X_test, y_train, y_test = train_test_split(
        AISG.drop(['likes', 'image_id'], axis=1),  # إزالة العمودين
        AISG['likes'],
        test_size=0.2,
        random_state=42
    )

    X_train.to_csv('Preprocessed_Data/X_train.csv', index=False)
    X_test.to_csv('Preprocessed_Data/X_test.csv', index=False)
    y_train.to_csv('Preprocessed_Data/y_train.csv', index=False)
    y_test.to_csv('Preprocessed_Data/y_test.csv', index=False)

    print("[SUCCESS] Data processed and saved")
    return AISG

def train_and_evaluate_models():
    """Train machine learning models and save results"""
    print("\n[STATUS] Training machine learning models...")

    # Load processed data
    X_train = pd.read_csv('Preprocessed_Data/X_train.csv')
    X_test = pd.read_csv('Preprocessed_Data/X_test.csv')
    y_train = pd.read_csv('Preprocessed_Data/y_train.csv').values.ravel()
    y_test = pd.read_csv('Preprocessed_Data/y_test.csv').values.ravel()

    # Encode categorical data
    le = LabelEncoder()
    X_train['platform'] = le.fit_transform(X_train['platform'])
    X_test['platform'] = le.transform(X_test['platform'])

    # Train models
    models = {
        'LinearRegression': LinearRegression(),
        'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42)
    }

    results = {}

    for name, model in models.items():
        print(f"\nTraining {name} model...")
        model.fit(X_train, y_train)

        # Save model
        joblib.dump(model, f'Python_Code/{name}_model.pkl')

        # Make predictions
        predictions = model.predict(X_test)
        pd.DataFrame(predictions).to_csv(
            f'Results/predictions_{name}_model.csv',
            index=False,
            header=['Predictions']
        )

        # Evaluate
        results[name] = {
            'RMSE': np.sqrt(mean_squared_error(y_test, predictions)),
            'R2': r2_score(y_test, predictions)
        }
        print(f"{name} performance:")
        print(f"RMSE: {results[name]['RMSE']:.2f}")
        print(f"R2 Score: {results[name]['R2']:.2f}")

    # Save results
    pd.DataFrame(results).to_csv('Results/model_performance.csv')
    print("\n[SUCCESS] All models trained and results saved")

def create_visualizations():
    """Create and save data visualizations"""
    print("\n[STATUS] Creating data visualizations...")

    AISG = pd.read_csv('Original_Data/ai_ghibli_trend_dataset.csv')

    # Platform distribution
    plt.figure(figsize=(10, 6))
    sns.countplot(x='platform', data=AISG)
    plt.title('Distribution of Posts by Platform')
    plt.xticks(rotation=45)
    plt.savefig('Results/platform_distribution.png')
    plt.close()

    # Likes distribution
    plt.figure(figsize=(10, 6))
    sns.boxplot(x='platform', y='likes', data=AISG)
    plt.title('Likes Distribution by Platform')
    plt.xticks(rotation=45)
    plt.savefig('Results/likes_distribution.png')
    plt.close()

    print("[SUCCESS] Visualizations saved to Results folder")

def create_submission_zip():
    """Create final submission zip file"""
    print("\n[STATUS] Creating submission package...")

    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    zip_name = f"AI_Project_Submission_{timestamp}.zip"

    with ZipFile(zip_name, 'w') as zipf:
        for folder in ['Original_Data', 'Preprocessed_Data', 'Results', 'Python_Code']:
            for root, dirs, files in os.walk(folder):
                for file in files:
                    zipf.write(os.path.join(root, file))

    print(f"[SUCCESS] Submission package created: {zip_name}")
    return zip_name

def main():
    print("=== AI PROJECT IMPLEMENTATION ===")
    print("Developer: Sarah Khaled Al-Tuwairqi")
    print("=================================")

    # 1. Create project structure
    create_project_structure()

    # 2. Load and process data
    load_and_process_data()

    # 3. Train and evaluate models
    train_and_evaluate_models()

    # 4. Create visualizations
    create_visualizations()

    # 5. Create submission package
    zip_file = create_submission_zip()

    # Final instructions
    print("\n=== SUBMISSION INSTRUCTIONS ===")
    print(f"1. Submit the zip file: {zip_file}")
    print("2. Folder structure includes:")
    print("   - Original_Data/ (raw dataset)")
    print("   - Preprocessed_Data/ (cleaned training/test data)")
    print("   - Results/ (predictions and visualizations)")
    print("   - Python_Code/ (trained models and code)")
    print("3. Include this zip file in your PDF report")

if __name__ == "__main__":
    main()

# استدعاء المكتبات المطلوبة
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import metrics
from sklearn import svm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder # Import OneHotEncoder for better practice
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import gdown
import os # Import os for creating directories

# Ensure necessary directories exist
def create_project_structure():
    """Create necessary directories"""
    print("\n[STATUS] Creating project structure...")
    directories = ['Original_Data', 'Preprocessed_Data', 'Results', 'Python_Code']
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
    print("[SUCCESS] Project structure created")


def load_and_process_data():
    """Load and preprocess the data"""
    print("\n[STATUS] Loading and preprocessing data...")

    # Load data from Google Drive
    file_id = '1hkebti9cgRECbze77SKfm8b_eGBJkTnL'
    url = f'https://drive.google.com/uc?id={file_id}'
    output_path = 'Original_Data/ai_ghibli_trend_dataset.csv' # Changed variable name

    try:
        # Ensure Original_Data directory exists before downloading
        os.makedirs('Original_Data', exist_ok=True)
        gdown.download(url, output_path, quiet=False)
        AISG = pd.read_csv(output_path) # Use the correct path
        print("[SUCCESS] Data loaded from Google Drive")
    except Exception as e:
        print(f"[WARNING] Error loading data from Google Drive: {e}")
        print("[STATUS] Using sample data instead...")
        data = {
            'image_id': [f'{i:08x}' for i in range(1, 101)], # Generate sample image_ids
            'likes': np.random.randint(100, 10000, 100),
            'shares': np.random.randint(10, 1000, 100),
            'platform': np.random.choice(['Instagram', 'TikTok', 'Twitter'], 100)
        }
        AISG = pd.DataFrame(data)
        # Ensure Original_Data directory exists before saving sample data
        os.makedirs('Original_Data', exist_ok=True)
        AISG.to_csv(output_path, index=False) # Use the correct path


    # Data cleaning and processing
    if 'platform' not in AISG.columns:
        AISG['platform'] = 'Unknown'
    AISG["platform"] = AISG["platform"].str.strip().str.replace(" ", "_")

    # Define features (X) and target (y)
    # Explicitly drop 'likes' and 'image_id'
    X = AISG.drop(['likes', 'image_id'], axis=1)
    y = AISG['likes']

    # Handle categorical features using OneHotEncoder (generally preferred over LabelEncoder for features)
    # Identify categorical columns - make sure 'platform' is in X
    categorical_cols = X.select_dtypes(include='object').columns # Automatically find object type columns

    if len(categorical_cols) > 0:
        print(f"[STATUS] Encoding categorical features: {list(categorical_cols)}")
        # Using OneHotEncoder as it's suitable for nominal categories like platform
        ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False) # Use sparse_output instead of sparse
        encoded_features = ohe.fit_transform(X[categorical_cols])
        # Get new column names
        encoded_feature_names = ohe.get_feature_names_out(categorical_cols)
        # Create a DataFrame for encoded features
        encoded_df = pd.DataFrame(encoded_features, columns=encoded_feature_names, index=X.index)
        # Drop original categorical columns and concatenate encoded ones
        X = X.drop(columns=categorical_cols)
        X = pd.concat([X, encoded_df], axis=1)
        print("[SUCCESS] Categorical features encoded")
    else:
        print("[STATUS] No categorical features found to encode.")


    # Split data AFTER processing
    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=0.2,
        random_state=42
    )

    # Ensure Preprocessed_Data directory exists before saving
    os.makedirs('Preprocessed_Data', exist_ok=True)

    # Save processed and split data (all columns should now be numerical)
    X_train.to_csv('Preprocessed_Data/X_train.csv', index=False)
    X_test.to_csv('Preprocessed_Data/X_test.csv', index=False)
    y_train.to_csv('Preprocessed_Data/y_train.csv', index=False)
    y_test.to_csv('Preprocessed_Data/y_test.csv', index=False)


    print("[SUCCESS] Data processed, split, and saved")
    # Return AISG for visualization purposes, but model training will use CSVs
    return AISG

# Sarah Khaled Al-Tuwairqi - AI Project Implementation
# Import Required Libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import metrics, svm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.impute import SimpleImputer
import datetime
import gdown
import os
import joblib
from zipfile import ZipFile

def create_project_structure():
    """Create the required folder structure for the project"""
    folders = [
        'Original_Data',
        'Preprocessed_Data',
        'Results',
        'Python_Code'
    ]

    for folder in folders:
        os.makedirs(folder, exist_ok=True)
    print("[STATUS] Created project folder structure")

def load_and_process_data():
    """Load and preprocess the data"""
    print("\n[STATUS] Loading and preprocessing data...")

    # Load data from Google Drive
    file_id = '1hkebti9cgRECbze77SKfm8b_eGBJkTnL'
    url = f'https://drive.google.com/uc?id={file_id}'
    output = 'Original_Data/ai_ghibli_trend_dataset.csv'

    try:
        gdown.download(url, output, quiet=False)
        AISG = pd.read_csv(output)
        print("[SUCCESS] Data loaded from Google Drive")
    except Exception as e:
        print(f"[WARNING] Error loading data: {e}")
        print("[STATUS] Using sample data instead...")
        data = {
            'image_id': [f"img_{i}" for i in range(1, 101)],
            'likes': np.random.randint(100, 10000, 100),
            'shares': np.random.randint(10, 1000, 100),
            'platform': np.random.choice(['Instagram', 'TikTok', 'Twitter'], 100)
        }
        AISG = pd.DataFrame(data)
        AISG.to_csv(output, index=False)

    # Ensure platform column exists and is properly formatted
    if 'platform' not in AISG.columns:
        AISG['platform'] = 'Unknown'
    elif 'Platform' in AISG.columns:  # إذا كان العمود بحرف P كبير
        AISG['platform'] = AISG['Platform']
        AISG.drop('Platform', axis=1, inplace=True)

    AISG["platform"] = AISG["platform"].astype(str).str.strip().str.replace(" ", "_")

    # Save processed data (excluding image_id)
    features = AISG.drop(['likes', 'image_id'], axis=1).columns.tolist()
    X_train, X_test, y_train, y_test = train_test_split(
        AISG[features],
        AISG['likes'],
        test_size=0.2,
        random_state=42
    )

    X_train.to_csv('Preprocessed_Data/X_train.csv', index=False)
    X_test.to_csv('Preprocessed_Data/X_test.csv', index=False)
    y_train.to_csv('Preprocessed_Data/y_train.csv', index=False)
    y_test.to_csv('Preprocessed_Data/y_test.csv', index=False)

    print("[SUCCESS] Data processed and saved")
    return AISG

def train_and_evaluate_models():
    """Train machine learning models and save results"""
    print("\n[STATUS] Training machine learning models...")

    # Load processed data
    X_train = pd.read_csv('Preprocessed_Data/X_train.csv')
    X_test = pd.read_csv('Preprocessed_Data/X_test.csv')
    y_train = pd.read_csv('Preprocessed_Data/y_train.csv').values.ravel()
    y_test = pd.read_csv('Preprocessed_Data/y_test.csv').values.ravel()

    # Check if platform column exists
    if 'platform' not in X_train.columns:
        raise ValueError("'platform' column is missing in the training data")

    # Encode categorical data
    le = LabelEncoder()
    X_train['platform'] = le.fit_transform(X_train['platform'])
    X_test['platform'] = le.transform(X_test['platform'])

    # Train models
    models = {
        'LinearRegression': LinearRegression(),
        'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42)
    }

    results = {}

    for name, model in models.items():
        print(f"\nTraining {name} model...")
        model.fit(X_train, y_train)

        # Save model
        joblib.dump(model, f'Python_Code/{name}_model.pkl')

        # Make predictions
        predictions = model.predict(X_test)
        pd.DataFrame(predictions, columns=['Predictions']).to_csv(
            f'Results/predictions_{name}_model.csv',
            index=False
        )

        # Evaluate
        results[name] = {
            'RMSE': np.sqrt(mean_squared_error(y_test, predictions)),
            'R2': r2_score(y_test, predictions)
        }
        print(f"{name} performance:")
        print(f"RMSE: {results[name]['RMSE']:.2f}")
        print(f"R2 Score: {results[name]['R2']:.2f}")

    # Save results
    pd.DataFrame(results).to_csv('Results/model_performance.csv')
    print("\n[SUCCESS] All models trained and results saved")

def create_visualizations():
    """Create and save data visualizations"""
    print("\n[STATUS] Creating data visualizations...")

    try:
        AISG = pd.read_csv('Original_Data/ai_ghibli_trend_dataset.csv')

        # Ensure platform column exists
        if 'platform' not in AISG.columns and 'Platform' in AISG.columns:
            AISG['platform'] = AISG['Platform']

        if 'platform' not in AISG.columns:
            AISG['platform'] = 'Unknown'

        # Platform distribution
        plt.figure(figsize=(10, 6))
        sns.countplot(x='platform', data=AISG)
        plt.title('Distribution of Posts by Platform')
        plt.xticks(rotation=45)
        plt.savefig('Results/platform_distribution.png')
        plt.close()

        # Likes distribution
        plt.figure(figsize=(10, 6))
        sns.boxplot(x='platform', y='likes', data=AISG)
        plt.title('Likes Distribution by Platform')
        plt.xticks(rotation=45)
        plt.savefig('Results/likes_distribution.png')
        plt.close()

        print("[SUCCESS] Visualizations saved to Results folder")
    except Exception as e:
        print(f"[ERROR] Failed to create visualizations: {e}")

def create_submission_zip():
    """Create final submission zip file"""
    print("\n[STATUS] Creating submission package...")

    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    zip_name = f"AI_Project_Submission_{timestamp}.zip"

    with ZipFile(zip_name, 'w') as zipf:
        for folder in ['Original_Data', 'Preprocessed_Data', 'Results', 'Python_Code']:
            for root, dirs, files in os.walk(folder):
                for file in files:
                    zipf.write(os.path.join(root, file))

    print(f"[SUCCESS] Submission package created: {zip_name}")
    return zip_name

def main():
    print("=== AI PROJECT IMPLEMENTATION ===")
    print("Developer: Sarah Khaled Al-Tuwairqi")
    print("=================================")

    try:
        # 1. Create project structure
        create_project_structure()

        # 2. Load and process data
        data = load_and_process_data()
        print("\nData Sample:")
        print(data.head())

        # 3. Train and evaluate models
        train_and_evaluate_models()

        # 4. Create visualizations
        create_visualizations()

        # 5. Create submission package
        zip_file = create_submission_zip()

        # Final instructions
        print("\n=== SUBMISSION INSTRUCTIONS ===")
        print(f"1. Submit the zip file: {zip_file}")
        print("2. Folder structure includes:")
        print("   - Original_Data/ (raw dataset)")
        print("   - Preprocessed_Data/ (cleaned training/test data)")
        print("   - Results/ (predictions and visualizations)")
        print("   - Python_Code/ (trained models and code)")
        print("3. Include this zip file in your PDF report")
    except Exception as e:
        print(f"\n[ERROR] An error occurred: {e}")
        print("Please check your data and try again.")

if __name__ == "__main__":
    main()

# Sarah Khaled Al-Tuwairqi - AI Project Implementation
# Import Required Libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import metrics, svm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.impute import SimpleImputer
import datetime
import gdown
import os
import joblib
from zipfile import ZipFile

def create_project_structure():
    """Create the required folder structure for the project"""
    folders = [
        'Original_Data',
        'Preprocessed_Data',
        'Results',
        'Python_Code'
    ]

    for folder in folders:
        os.makedirs(folder, exist_ok=True)
    print("[STATUS] Created project folder structure")

def load_and_process_data():
    """Load and preprocess the data"""
    print("\n[STATUS] Loading and preprocessing data...")

    # Load data from Google Drive
    file_id = '1hkebti9cgRECbze77SKfm8b_eGBJkTnL'
    url = f'https://drive.google.com/uc?id={file_id}'
    output = 'Original_Data/ai_ghibli_trend_dataset.csv'

    try:
        gdown.download(url, output, quiet=False)
        AISG = pd.read_csv(output)
        print("[SUCCESS] Data loaded from Google Drive")
    except Exception as e:
        print(f"[WARNING] Error loading data: {e}")
        print("[STATUS] Using sample data instead...")
        data = {
            'image_id': [f"img_{i}" for i in range(1, 101)],
            'likes': np.random.randint(100, 10000, 100),
            'shares': np.random.randint(10, 1000, 100),
            'platform': np.random.choice(['Instagram', 'TikTok', 'Twitter'], 100)
        }
        AISG = pd.DataFrame(data)
        AISG.to_csv(output, index=False)

    # Ensure platform column exists and is properly formatted
    if 'platform' not in AISG.columns:
        if 'Platform' in AISG.columns:
            AISG['platform'] = AISG['Platform']
            AISG.drop('Platform', axis=1, inplace=True)
        else:
            AISG['platform'] = 'Unknown'

    AISG["platform"] = AISG["platform"].astype(str).str.strip().str.replace(" ", "_")

    # Save processed data (excluding image_id)
    features = [col for col in AISG.columns if col not in ['likes', 'image_id']]
    X_train, X_test, y_train, y_test = train_test_split(
        AISG[features],
        AISG['likes'],
        test_size=0.2,
        random_state=42
    )

    X_train.to_csv('Preprocessed_Data/X_train.csv', index=False)
    X_test.to_csv('Preprocessed_Data/X_test.csv', index=False)
    y_train.to_csv('Preprocessed_Data/y_train.csv', index=False)
    y_test.to_csv('Preprocessed_Data/y_test.csv', index=False)

    print("[SUCCESS] Data processed and saved")
    return AISG

def train_and_evaluate_models():
    """Train machine learning models and save results"""
    print("\n[STATUS] Training machine learning models...")

    # Load processed data
    X_train = pd.read_csv('Preprocessed_Data/X_train.csv')
    X_test = pd.read_csv('Preprocessed_Data/X_test.csv')
    y_train = pd.read_csv('Preprocessed_Data/y_train.csv').values.ravel()
    y_test = pd.read_csv('Preprocessed_Data/y_test.csv').values.ravel()

    # Check if platform column exists
    if 'platform' not in X_train.columns:
        raise ValueError("'platform' column is missing in the training data")

    # Encode categorical data
    le = LabelEncoder()
    X_train['platform'] = le.fit_transform(X_train['platform'])
    X_test['platform'] = le.transform(X_test['platform'])

    # Train models
    models = {
        'LinearRegression': LinearRegression(),
        'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42)
    }

    results = {}

    for name, model in models.items():
        print(f"\nTraining {name} model...")
        model.fit(X_train, y_train)

        # Save model
        joblib.dump(model, f'Python_Code/{name}_model.pkl')

        # Make predictions
        predictions = model.predict(X_test)
        pd.DataFrame(predictions, columns=['Predictions']).to_csv(
            f'Results/predictions_{name}_model.csv',
            index=False
        )

        # Evaluate
        results[name] = {
            'RMSE': np.sqrt(mean_squared_error(y_test, predictions)),
            'R2': r2_score(y_test, predictions)
        }
        print(f"{name} performance:")
        print(f"RMSE: {results[name]['RMSE']:.2f}")
        print(f"R2 Score: {results[name]['R2']:.2f}")

    # Save results
    pd.DataFrame(results).to_csv('Results/model_performance.csv')
    print("\n[SUCCESS] All models trained and results saved")

def create_visualizations():
    """Create and save data visualizations"""
    print("\n[STATUS] Creating data visualizations...")

    try:
        AISG = pd.read_csv('Original_Data/ai_ghibli_trend_dataset.csv')

        # Ensure platform column exists
        if 'platform' not in AISG.columns:
            if 'Platform' in AISG.columns:
                AISG['platform'] = AISG['Platform']
            else:
                AISG['platform'] = 'Unknown'

        # Platform distribution
        plt.figure(figsize=(10, 6))
        sns.countplot(x='platform', data=AISG)
        plt.title('Distribution of Posts by Platform')
        plt.xticks(rotation=45)
        plt.savefig('Results/platform_distribution.png', bbox_inches='tight')
        plt.close()

        # Likes distribution
        plt.figure(figsize=(10, 6))
        sns.boxplot(x='platform', y='likes', data=AISG)
        plt.title('Likes Distribution by Platform')
        plt.xticks(rotation=45)
        plt.savefig('Results/likes_distribution.png', bbox_inches='tight')
        plt.close()

        print("[SUCCESS] Visualizations saved to Results folder")
    except Exception as e:
        print(f"[ERROR] Failed to create visualizations: {e}")

def create_final_zip():
    """Create final submission zip file named 'final.zip'"""
    print("\n[STATUS] Creating final submission package...")

    zip_name = "final.zip"

    # Remove old zip file if exists
    if os.path.exists(zip_name):
        os.remove(zip_name)

    with ZipFile(zip_name, 'w') as zipf:
        for folder in ['Original_Data', 'Preprocessed_Data', 'Results', 'Python_Code']:
            for root, dirs, files in os.walk(folder):
                for file in files:
                    file_path = os.path.join(root, file)
                    arcname = os.path.relpath(file_path, start='.')
                    zipf.write(file_path, arcname)

    print(f"[SUCCESS] Final submission package created: {zip_name}")
    return zip_name

def main():
    print("=== AI PROJECT IMPLEMENTATION ===")
    print("Developer: Sarah Khaled Al-Tuwairqi")
    print("=================================")

    try:
        # 1. Create project structure
        create_project_structure()

        # 2. Load and process data
        data = load_and_process_data()
        print("\nData Sample:")
        print(data.head())

        # 3. Train and evaluate models
        train_and_evaluate_models()

        # 4. Create visualizations
        create_visualizations()

        # 5. Create final submission package
        zip_file = create_final_zip()

        # Final instructions
        print("\n=== SUBMISSION INSTRUCTIONS ===")
        print(f"1. Submit the zip file: {zip_file}")
        print("2. Folder structure includes:")
        print("   - Original_Data/ (raw dataset)")
        print("   - Preprocessed_Data/ (cleaned training/test data)")
        print("   - Results/ (predictions and visualizations)")
        print("   - Python_Code/ (trained models and code)")
        print("3. Include this zip file in your PDF report")
    except Exception as e:
        print(f"\n[ERROR] An error occurred: {e}")
        print("Please check your data and try again.")

if __name__ == "__main__":
    main()

# Sarah Khaled Al-Tuwairqi - AI Project Implementation
# Import Required Libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import metrics, svm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.impute import SimpleImputer
import datetime
import gdown
import os
import joblib
from zipfile import ZipFile

def create_project_structure():
    """Create the required folder structure for the project"""
    folders = [
        'Original_Data',
        'Preprocessed_Data',
        'Results',
        'Python_Code'
    ]

    for folder in folders:
        os.makedirs(folder, exist_ok=True)
    print("[STATUS] Created project folder structure")

def load_and_process_data():
    """Load and preprocess the data"""
    print("\n[STATUS] Loading and preprocessing data...")

    # Load data from Google Drive
    file_id = '1hkebti9cgRECbze77SKfm8b_eGBJkTnL'
    url = f'https://drive.google.com/uc?id={file_id}'
    output = 'Original_Data/ai_ghibli_trend_dataset.csv'

    try:
        gdown.download(url, output, quiet=False)
        AISG = pd.read_csv(output)
        print("[SUCCESS] Data loaded from Google Drive")
    except Exception as e:
        print(f"[WARNING] Error loading data: {e}")
        print("[STATUS] Using sample data instead...")
        data = {
            'image_id': range(1, 101),
            'likes': np.random.randint(100, 10000, 100),
            'shares': np.random.randint(10, 1000, 100),
            'platform': np.random.choice(['Instagram', 'TikTok', 'Twitter'], 100)
        }
        AISG = pd.DataFrame(data)
        AISG.to_csv(output, index=False)

    # Data cleaning and processing
    if 'platform' not in AISG.columns:
        AISG['platform'] = 'Unknown'
    AISG["platform"] = AISG["platform"].str.strip().str.replace(" ", "_")

    # Save processed data
    X_train, X_test, y_train, y_test = train_test_split(
        AISG.drop('likes', axis=1),
        AISG['likes'],
        test_size=0.2,
        random_state=42
    )

    X_train.to_csv('Preprocessed_Data/X_train.csv', index=False)
    X_test.to_csv('Preprocessed_Data/X_test.csv', index=False)
    y_train.to_csv('Preprocessed_Data/y_train.csv', index=False)
    y_test.to_csv('Preprocessed_Data/y_test.csv', index=False)

    print("[SUCCESS] Data processed and saved")
    return AISG

def train_and_evaluate_models():
    """Train machine learning models and save results"""
    print("\n[STATUS] Training machine learning models...")

    # Load processed data
    X_train = pd.read_csv('Preprocessed_Data/X_train.csv')
    X_test = pd.read_csv('Preprocessed_Data/X_test.csv')
    y_train = pd.read_csv('Preprocessed_Data/y_train.csv').values.ravel()
    y_test = pd.read_csv('Preprocessed_Data/y_test.csv').values.ravel()

    # Encode categorical data
    le = LabelEncoder()
    X_train['platform'] = le.fit_transform(X_train['platform'])
    X_test['platform'] = le.transform(X_test['platform'])

    # Train models
    models = {
        'LinearRegression': LinearRegression(),
        'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42)
    }

    results = {}

    for name, model in models.items():
        print(f"\nTraining {name} model...")
        model.fit(X_train, y_train)

        # Save model
        joblib.dump(model, f'Python_Code/{name}_model.pkl')

        # Make predictions
        predictions = model.predict(X_test)
        pd.DataFrame(predictions).to_csv(
            f'Results/predictions_{name}_model.csv',
            index=False,
            header=['Predictions']
        )

        # Evaluate
        results[name] = {
            'RMSE': np.sqrt(mean_squared_error(y_test, predictions)),
            'R2': r2_score(y_test, predictions)
        }
        print(f"{name} performance:")
        print(f"RMSE: {results[name]['RMSE']:.2f}")
        print(f"R2 Score: {results[name]['R2']:.2f}")

    # Save results
    pd.DataFrame(results).to_csv('Results/model_performance.csv')
    print("\n[SUCCESS] All models trained and results saved")

def create_visualizations():
    """Create and save data visualizations"""
    print("\n[STATUS] Creating data visualizations...")

    AISG = pd.read_csv('Original_Data/ai_ghibli_trend_dataset.csv')

    # Platform distribution
    plt.figure(figsize=(10, 6))
    sns.countplot(x='platform', data=AISG)
    plt.title('Distribution of Posts by Platform')
    plt.xticks(rotation=45)
    plt.savefig('Results/platform_distribution.png')
    plt.close()

    # Likes distribution
    plt.figure(figsize=(10, 6))
    sns.boxplot(x='platform', y='likes', data=AISG)
    plt.title('Likes Distribution by Platform')
    plt.xticks(rotation=45)
    plt.savefig('Results/likes_distribution.png')
    plt.close()

    print("[SUCCESS] Visualizations saved to Results folder")

def create_submission_zip():
    """Create final submission zip file"""
    print("\n[STATUS] Creating submission package...")

    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    zip_name = f"AI_Project_Submission_{timestamp}.zip"

    with ZipFile(zip_name, 'w') as zipf:
        for folder in ['Original_Data', 'Preprocessed_Data', 'Results', 'Python_Code']:
            for root, dirs, files in os.walk(folder):
                for file in files:
                    zipf.write(os.path.join(root, file))

    print(f"[SUCCESS] Submission package created: {zip_name}")
    return zip_name

def main():
    print("=== AI PROJECT IMPLEMENTATION ===")
    print("Developer: Sarah Khaled Al-Tuwairqi")
    print("=================================")

    # 1. Create project structure
    create_project_structure()

    # 2. Load and process data
    load_and_process_data()

    # 3. Train and evaluate models
    train_and_evaluate_models()

    # 4. Create visualizations
    create_visualizations()

    # 5. Create submission package
    zip_file = create_submission_zip()

    # Final instructions
    print("\n=== SUBMISSION INSTRUCTIONS ===")
    print(f"1. Submit the zip file: {zip_file}")
    print("2. Folder structure includes:")
    print("   - Original_Data/ (raw dataset)")
    print("   - Preprocessed_Data/ (cleaned training/test data)")
    print("   - Results/ (predictions and visualizations)")
    print("   - Python_Code/ (trained models and code)")
    print("3. Include this zip file in your PDF report")

if __name__ == "__main__":
    main()

# Sarah Khaled Al-Tuwairqi - AI Project Implementation
# Import Required Libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import metrics, svm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.impute import SimpleImputer
import datetime
import gdown
import os
import joblib
from zipfile import ZipFile

import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
import joblib
import zipfile
from datetime import datetime

def create_project_structure():
    """Create the exact folder structure required"""
    required_folders = [
        'Original_Data',        # For raw unmodified data
        'Preprocessed_Data',    # For processed training/test data
        'Results',              # For model predictions
        'Python_Code'           # For Python scripts
    ]

    for folder in required_folders:
        os.makedirs(folder, exist_ok=True)
    print("Created required folder structure")

def process_and_save_data():
    """Process data and save in specified structure"""
    # Load sample dataset (replace with your actual data)
    from sklearn.datasets import fetch_california_housing
    data = fetch_california_housing()

    # Save original data (exactly as downloaded)
    pd.DataFrame(data.data).to_csv('Original_Data/raw_features.csv', index=False)
    pd.DataFrame(data.target).to_csv('Original_Data/raw_targets.csv', index=False)

    # Split data into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(
        data.data, data.target, test_size=0.2, random_state=42
    )

    # Save processed data in required format
    pd.DataFrame(X_train).to_csv('Preprocessed_Data/X_train.csv', index=False)
    pd.DataFrame(X_test).to_csv('Preprocessed_Data/X_test.csv', index=False)
    pd.DataFrame(y_train).to_csv('Preprocessed_Data/y_train.csv', index=False)
    pd.DataFrame(y_test).to_csv('Preprocessed_Data/y_test.csv', index=False)

    print("Processed and saved training/test data")

def train_and_save_models():
    """Train models and save predictions with clear naming"""
    # Load processed data
    X_train = pd.read_csv('Preprocessed_Data/X_train.csv').values
    X_test = pd.read_csv('Preprocessed_Data/X_test.csv').values
    y_train = pd.read_csv('Preprocessed_Data/y_train.csv').values.ravel()
    y_test = pd.read_csv('Preprocessed_Data/y_test.csv').values.ravel()

    # Define models to train
    models = {
        'RF': RandomForestRegressor(n_estimators=100, random_state=42),
        'SVM': SVR()
    }

    # Train each model and save results
    for model_name, model in models.items():
        # Train model
        model.fit(X_train, y_train)

        # Make predictions
        predictions = model.predict(X_test)

        # Save model
        joblib.dump(model, f'Python_Code/{model_name}_model.pkl')

        # Save predictions with clear naming
        pd.DataFrame(predictions).to_csv(
            f'Results/predictions_{model_name}_model.csv',
            index=False,
            header=['Predictions']
        )

        # Calculate performance metrics
        rmse = np.sqrt(mean_squared_error(y_test, predictions))
        r2 = r2_score(y_test, predictions)

        print(f"{model_name} Model - RMSE: {rmse:.4f}, R2: {r2:.4f}")

def create_submission_zip():
    """Create final submission zip file"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    zip_filename = f"ML_Project_Submission_{timestamp}.zip"

    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for folder in ['Original_Data', 'Preprocessed_Data', 'Results', 'Python_Code']:
            for root, dirs, files in os.walk(folder):
                for file in files:
                    zipf.write(os.path.join(root, file))

    print(f"Created submission zip file: {zip_filename}")
    return zip_filename

def main():
    print("=== MACHINE LEARNING PROJECT IMPLEMENTATION ===")

    # 1. Create required folder structure
    create_project_structure()

    # 2. Process and save data
    process_and_save_data()

    # 3. Train models and save results
    train_and_save_models()

    # 4. Create submission package
    submission_zip = create_submission_zip()

    # Final instructions
    print("\n=== SUBMISSION INSTRUCTIONS ===")
    print(f"1. Final submission file: {22submission_zip}")
    print("2. Upload this file to:")
    print("   - GitHub repository")
    print("   - Google Drive")
    print("3. Include these links in your PDF report")
    print("4. Verify your submission contains:")
    print("   - Original_Data/ (raw data files)")
    print("   - Preprocessed_Data/ (X_train.csv, X_test.csv, y_train.csv, y_test.csv)")
    print("   - Results/ (predictions_*_model.csv files)")
    print("   - Python_Code/ (model scripts and saved models)")

if __name__ == "__main__":
    main()

import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
import joblib
import shutil
import zipfile
from datetime import datetime

# 1. إنشاء هيكل المجلدات المطلوبة
def create_project_structure():
    """Create the exact folder structure requested by the professor"""
    folders = [
        'Original_Data',        # للبيانات الأصلية بدون تعديل
        'Preprocessed_Data',    # للبيانات المعدة
        'Results',              # لنتائج النماذج
        'Python_Code'           # لأكواد البايثون
    ]

    for folder in folders:
        os.makedirs(folder, exist_ok=True)
    print("تم إنشاء هيكل المجلدات كما طلب الدكتور")

# 2. معالجة البيانات وحفظها
def process_and_save_data():
    """Process data and save in required structure"""
    # مثال ببيانات تجريبية (يجب استبدالها ببياناتك)
    from sklearn.datasets import fetch_california_housing
    data = fetch_california_housing()

    # حفظ البيانات الأصلية (كما تم تنزيلها)
    pd.DataFrame(data.data).to_csv('Original_Data/original_features.csv', index=False)
    pd.DataFrame(data.target).to_csv('Original_Data/original_targets.csv', index=False)

    # تقسيم البيانات
    X_train, X_test, y_train, y_test = train_test_split(
        data.data, data.target, test_size=0.2, random_state=42
    )

    # حفظ البيانات المعدة
    pd.DataFrame(X_train).to_csv('Preprocessed_Data/X_train.csv', index=False)
    pd.DataFrame(X_test).to_csv('Preprocessed_Data/X_test.csv', index=False)
    pd.DataFrame(y_train).to_csv('Preprocessed_Data/y_train.csv', index=False)
    pd.DataFrame(y_test).to_csv('Preprocessed_Data/y_test.csv', index=False)

    print("تم حفظ البيانات في المجلدات المحددة")

# 3. تدريب النماذج وحفظ النتائج
def train_models():
    """Train models and save results with clear names"""
    # تحميل البيانات
    X_train = pd.read_csv('Preprocessed_Data/X_train.csv').values
    X_test = pd.read_csv('Preprocessed_Data/X_test.csv').values
    y_train = pd.read_csv('Preprocessed_Data/y_train.csv').values.ravel()
    y_test = pd.read_csv('Preprocessed_Data/y_test.csv').values.ravel()

    # تعريف النماذج
    models = {
        'RF': RandomForestRegressor(n_estimators=100, random_state=42),
        'SVM': SVR()
    }

    # تدريب وحفظ كل نموذج
    for name, model in models.items():
        # التدريب
        model.fit(X_train, y_train)

        # التنبؤات
        predictions = model.predict(X_test)

        # حفظ النموذج
        joblib.dump(model, f'Python_Code/{name}_model.pkl')

        # حفظ التنبؤات
        pd.DataFrame(predictions).to_csv(
            f'Results/predictions_{name}_model.csv',
            index=False,
            header=['Predictions']
        )

        # حساب الأداء
        rmse = np.sqrt(mean_squared_error(y_test, predictions))
        r2 = r2_score(y_test, predictions)

        print(f"نموذج {name}: RMSE = {rmse:.4f}, R2 = {r2:.4f}")

    print("تم تدريب وحفظ جميع النماذج كما طلب الدكتور")

# 4. إنشاء ملف ZIP للتسليم
def create_zip_submission():
    """Create zip file for final submission"""
    # اسم الملف بتاريخ ووقت التسليم
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    zip_name = f"ML_Project_Submission_{timestamp}.zip"

    # إنشاء الملف المضغوط
    with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for folder in ['Original_Data', 'Preprocessed_Data', 'Results', 'Python_Code']:
            for root, dirs, files in os.walk(folder):
                for file in files:
                    zipf.write(os.path.join(root, file))

    print(f"تم إنشاء ملف التسليم النهائي: {zip_name}")
    return zip_name

# الدالة الرئيسية
def main():
    print("=== بدء تنفيذ مشروع التعلم الآلي حسب متطلبات الدكتور ===")

    # 1. إنشاء الهيكل
    create_project_structure()

    # 2. معالجة البيانات
    process_and_save_data()

    # 3. تدريب النماذج
    train_models()

    # 4. إنشاء ملف التسليم
    zip_file = create_zip_submission()

    print("\n=== التعليمات النهائية ===")
    print(f"1. ملف التسليم النهائي: {zip_file}")
    print("2. يجب رفع هذا الملف على:")
    print("   - جيت هاب (GitHub)")
    print("   - جوجل درايف (Google Drive)")
    print("3. أرفق الرابط في تقرير PDF كما طلب الدكتور")
    print("4. تأكد من أن الملف يحتوي على:")
    print("   - البيانات الأصلية (Original_Data)")
    print("   - البيانات المعدة (Preprocessed_Data)")
    print("   - النتائج (Results)")
    print("   - الأكواد (Python_Code)")

if __name__ == "__main__":
    main()

import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
import joblib
import shutil
from datetime import datetime

def create_project_structure(base_path):
    """Create the required folder structure on C: drive"""
    folders = [
        'Original_Data',
        'Preprocessed_Data',
        'Results',
        'Python_Code'
    ]

    for folder in folders:
        os.makedirs(os.path.join(base_path, folder), exist_ok=True)
    print(f"Created folder structure at: {base_path}")

def process_and_save_data(base_path):
    """Process data and save in specified structure"""
    from sklearn.datasets import fetch_california_housing
    data = fetch_california_housing()

    # Save original data
    pd.DataFrame(data.data).to_csv(os.path.join(base_path, 'Original_Data/raw_features.csv'), index=False)
    pd.DataFrame(data.target).to_csv(os.path.join(base_path, 'Original_Data/raw_targets.csv'), index=False)

    # Split and save processed data
    X_train, X_test, y_train, y_test = train_test_split(
        data.data, data.target, test_size=0.2, random_state=42
    )

    pd.DataFrame(X_train).to_csv(os.path.join(base_path, 'Preprocessed_Data/X_train.csv'), index=False)
    pd.DataFrame(X_test).to_csv(os.path.join(base_path, 'Preprocessed_Data/X_test.csv'), index=False)
    pd.DataFrame(y_train).to_csv(os.path.join(base_path, 'Preprocessed_Data/y_train.csv'), index=False)
    pd.DataFrame(y_test).to_csv(os.path.join(base_path, 'Preprocessed_Data/y_test.csv'), index=False)

    print("Data processed and saved successfully")

def train_and_save_models(base_path):
    """Train models and save results with clear naming"""
    # Load data
    X_train = pd.read_csv(os.path.join(base_path, 'Preprocessed_Data/X_train.csv')).values
    X_test = pd.read_csv(os.path.join(base_path, 'Preprocessed_Data/X_test.csv')).values
    y_train = pd.read_csv(os.path.join(base_path, 'Preprocessed_Data/y_train.csv')).values.ravel()
    y_test = pd.read_csv(os.path.join(base_path, 'Preprocessed_Data/y_test.csv')).values.ravel()

    # Define and train models
    models = {
        'RF': RandomForestRegressor(n_estimators=100, random_state=42),
        'SVM': SVR()
    }

    for name, model in models.items():
        model.fit(X_train, y_train)
        predictions = model.predict(X_test)

        # Save model
        joblib.dump(model, os.path.join(base_path, f'Python_Code/{name}_model.pkl'))

        # Save predictions
        pd.DataFrame(predictions).to_csv(
            os.path.join(base_path, f'Results/predictions_{name}_model.csv'),
            index=False,
            header=['Predictions']
        )

        # Print performance
        rmse = np.sqrt(mean_squared_error(y_test, predictions))
        r2 = r2_score(y_test, predictions)
        print(f"{name} Model - RMSE: {rmse:.4f}, R2: {r2:.4f}")

def create_backup_zip(base_path):
    """Create backup zip file of the project"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    zip_name = f"ML_Project_Backup_{timestamp}.zip"
    zip_path = os.path.join(base_path, zip_name)

    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for folder in ['Original_Data', 'Preprocessed_Data', 'Results', 'Python_Code']:
            folder_path = os.path.join(base_path, folder)
            for root, dirs, files in os.walk(folder_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    arcname = os.path.relpath(file_path, base_path)
                    zipf.write(file_path, arcname)

    print(f"Backup created: {zip_path}")
    return zip_path

def main():
    # Set base path on C: drive
    base_path = "C:\\ML_Project_Submission"

    print("=== MACHINE LEARNING PROJECT IMPLEMENTATION ===")
    print(f"Saving all files to: {base_path}")

    # 1. Create folder structure
    create_project_structure(base_path)

    # 2. Process and save data
    process_and_save_data(base_path)

    # 3. Train models and save results
    train_and_save_models(base_path)

    # 4. Create backup zip
    backup_zip = create_backup_zip(base_path)

    print("\n=== PROJECT SUCCESSFULLY SAVED TO C: DRIVE ===")
    print(f"Main project location: {base_path}")
    print(f"Backup zip created: {backup_zip}")
    print("\nFolder structure created:")
    print(f"C:\\ML_Project_Submission")
    print("├── Original_Data/")
    print("├── Preprocessed_Data/")
    print("├── Results/")
    print("└── Python_Code/")

if __name__ == "__main__":
    main()



import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
import joblib
import shutil
from datetime import datetime

def create_project_structure(base_path):
    """Create the required folder structure on C: drive"""
    folders = [
        'Original_Data',
        'Preprocessed_Data',
        'Results',
        'Python_Code'
    ]

    for folder in folders:
        os.makedirs(os.path.join(base_path, folder), exist_ok=True)
    print(f"Created folder structure at: {base_path}")

def process_and_save_data(base_path):
    """Process data and save in specified structure"""
    from sklearn.datasets import fetch_california_housing
    data = fetch_california_housing()

    # Save original data
    pd.DataFrame(data.data).to_csv(os.path.join(base_path, 'Original_Data/raw_features.csv'), index=False)
    pd.DataFrame(data.target).to_csv(os.path.join(base_path, 'Original_Data/raw_targets.csv'), index=False)

    # Split and save processed data
    X_train, X_test, y_train, y_test = train_test_split(
        data.data, data.target, test_size=0.2, random_state=42
    )

    pd.DataFrame(X_train).to_csv(os.path.join(base_path, 'Preprocessed_Data/X_train.csv'), index=False)
    pd.DataFrame(X_test).to_csv(os.path.join(base_path, 'Preprocessed_Data/X_test.csv'), index=False)
    pd.DataFrame(y_train).to_csv(os.path.join(base_path, 'Preprocessed_Data/y_train.csv'), index=False)
    pd.DataFrame(y_test).to_csv(os.path.join(base_path, 'Preprocessed_Data/y_test.csv'), index=False)

    print("Data processed and saved successfully")

def train_and_save_models(base_path):
    """Train models and save results with clear naming"""
    # Load data
    X_train = pd.read_csv(os.path.join(base_path, 'Preprocessed_Data/X_train.csv')).values
    X_test = pd.read_csv(os.path.join(base_path, 'Preprocessed_Data/X_test.csv')).values
    y_train = pd.read_csv(os.path.join(base_path, 'Preprocessed_Data/y_train.csv')).values.ravel()
    y_test = pd.read_csv(os.path.join(base_path, 'Preprocessed_Data/y_test.csv')).values.ravel()

    # Define and train models
    models = {
        'RF': RandomForestRegressor(n_estimators=100, random_state=42),
        'SVM': SVR()
    }

    for name, model in models.items():
        model.fit(X_train, y_train)
        predictions = model.predict(X_test)

        # Save model
        joblib.dump(model, os.path.join(base_path, f'Python_Code/{name}_model.pkl'))

        # Save predictions
        pd.DataFrame(predictions).to_csv(
            os.path.join(base_path, f'Results/predictions_{name}_model.csv'),
            index=False,
            header=['Predictions']
        )

        # Print performance
        rmse = np.sqrt(mean_squared_error(y_test, predictions))
        r2 = r2_score(y_test, predictions)
        print(f"{name} Model - RMSE: {rmse:.4f}, R2: {r2:.4f}")

def create_backup_zip(base_path):
    """Create backup zip file of the project"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    zip_name = f"4ttML_Project_Backup_{timestamp}.zip"
    zip_path = os.path.join(base_path, zip_name)

    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for folder in ['Original_Data', 'Preprocessed_Data', 'Results', 'Python_Code']:
            folder_path = os.path.join(base_path, folder)
            for root, dirs, files in os.walk(folder_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    arcname = os.path.relpath(file_path, base_path)
                    zipf.write(file_path, arcname)

    print(f"Backup created: {zip_path}")
    return zip_path

def main():
    # Set base path on C: drive
    base_path = "C:\\ML_Project_Submission"

    print("=== MACHINE LEARNING PROJECT IMPLEMENTATION ===")
    print(f"Saving all files to: {base_path}")

    # 1. Create folder structure
    create_project_structure(base_path)

    # 2. Process and save data
    process_and_save_data(base_path)

    # 3. Train models and save results
    train_and_save_models(base_path)

    # 4. Create backup zip
    backup_zip = create_backup_zip(base_path)

    print("\n=== PROJECT SUCCESSFULLY SAVED TO C: DRIVE ===")
    print(f"Main project location: {base_path}")
    print(f"Backup zip created: {backup_zip}")
    print("\nFolder structure created:")
    print(f"C:\\2ML_Project_Submission")
    print("├── Original_Data/")
    print("├── Preprocessed_Data/")
    print("├── Results/")
    print("└── Python_Code/")

if __name__ == "__main__":
    main()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
import joblib
import os

def create_project_structure():
    """Create the required folder structure for the project"""
    folders = [
        'Original_Data',
        'Preprocessed_Data',
        'Results',
        'Python_Code'
    ]

    for folder in folders:
        os.makedirs(folder, exist_ok=True)
    print("[SUCCESS] Project folder structure created successfully")

def load_and_preprocess_data():
    """Load and preprocess the data"""
    print("\n[STATUS] Loading and preprocessing data...")

    # Example with California housing dataset (replace with your actual data)
    from sklearn.datasets import fetch_california_housing
    data = fetch_california_housing()
    X = pd.DataFrame(data.data, columns=data.feature_names)
    y = pd.Series(data.target)

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # Save processed data
    X_train.to_csv('Preprocessed_Data/X_train.csv', index=False)
    X_test.to_csv('Preprocessed_Data/X_test.csv', index=False)
    y_train.to_csv('Preprocessed_Data/y_train.csv', index=False)
    y_test.to_csv('Preprocessed_Data/y_test.csv', index=False)

    print("[SUCCESS] Data processed and saved in Preprocessed_Data folder")
    print(f"Training set shape: {X_train.shape}")
    print(f"Test set shape: {X_test.shape}")

    return X_train, X_test, y_train, y_test

def train_and_save_models(X_train, X_test, y_train, y_test):
    """Train models and save results"""
    print("\n[STATUS] Training machine learning models...")

    models = {
        'RF': RandomForestRegressor(n_estimators=100, random_state=42),
        'SVM': SVR()
    }

    results = {}

    for name, model in models.items():
        print(f"\nTraining {name} model...")

        # Train model
        model.fit(X_train, y_train)

        # Make predictions
        y_pred = model.predict(X_test)

        # Save model
        joblib.dump(model, f'Python_Code/{name}_model.pkl')

        # Save predictions
        pd.DataFrame(y_pred, columns=['Predictions']).to_csv(
            f'Results/predictions_{name}_model.csv', index=False
        )

        # Calculate metrics
        results[name] = {
            'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),
            'R2': r2_score(y_test, y_pred)
        }

        print(f"{name} model trained and saved successfully")

    # Save results
    pd.DataFrame(results).to_csv('Results/model_results.csv')
    print("\n[SUCCESS] All models trained and results saved")
    return results

def generate_report(results):
    """Generate a summary report of model performance"""
    print("\n=== MODEL PERFORMANCE REPORT ===")
    report_df = pd.DataFrame(results)
    print(report_df)

    # Save report
    with open('Results/report_summary.txt', 'w') as f:
        f.write("=== MODEL PERFORMANCE SUMMARY ===\n\n")
        f.write(report_df.to_string())

    print("\n[SUCCESS] Report summary saved to Results/report_summary.txt")

def main():
    print("=== MACHINE LEARNING PROJECT IMPLEMENTATION ===")

    # 1. Create folder structure
    create_project_structure()

    # 2. Load and preprocess data
    X_train, X_test, y_train, y_test = load_and_preprocess_data()

    # 3. Train models and save results
    results = train_and_save_models(X_train, X_test, y_train, y_test)

    # 4. Generate performance report
    generate_report(results)

    print("\n=== PROJECT IMPLEMENTATION COMPLETE ===")
    print("All required files and folders have been created")
    print("Please check the following folders:")
    print("- Original_Data/ (for raw data)")
    print("- Preprocessed_Data/ (for processed training/test data)")
    print("- Results/ (for model predictions and performance metrics)")
    print("- Python_Code/ (for saved model files)")

if __name__ == "__main__":
    main()

import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
import joblib
import zipfile
from datetime import datetime

def create_project_structure():
    """Create the exact folder structure required"""
    required_folders = [
        'Original_Data',        # For raw unmodified data
        'Preprocessed_Data',    # For processed training/test data
        'Results',              # For model predictions
        'Python_Code'           # For Python scripts
    ]

    for folder in required_folders:
        os.makedirs(folder, exist_ok=True)
    print("Created required folder structure")

def process_and_save_data():
    """Process data and save in specified structure"""
    # Load sample dataset (replace with your actual data)
    from sklearn.datasets import fetch_california_housing
    data = fetch_california_housing()

    # Save original data (exactly as downloaded)
    pd.DataFrame(data.data).to_csv('Original_Data/raw_features.csv', index=False)
    pd.DataFrame(data.target).to_csv('Original_Data/raw_targets.csv', index=False)

    # Split data into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(
        data.data, data.target, test_size=0.2, random_state=42
    )

    # Save processed data in required format
    pd.DataFrame(X_train).to_csv('Preprocessed_Data/X_train.csv', index=False)
    pd.DataFrame(X_test).to_csv('Preprocessed_Data/X_test.csv', index=False)
    pd.DataFrame(y_train).to_csv('Preprocessed_Data/y_train.csv', index=False)
    pd.DataFrame(y_test).to_csv('Preprocessed_Data/y_test.csv', index=False)

    print("Processed and saved training/test data")

def train_and_save_models():
    """Train models and save predictions with clear naming"""
    # Load processed data
    X_train = pd.read_csv('Preprocessed_Data/X_train.csv').values
    X_test = pd.read_csv('Preprocessed_Data/X_test.csv').values
    y_train = pd.read_csv('Preprocessed_Data/y_train.csv').values.ravel()
    y_test = pd.read_csv('Preprocessed_Data/y_test.csv').values.ravel()

    # Define models to train
    models = {
        'RF': RandomForestRegressor(n_estimators=100, random_state=42),
        'SVM': SVR()
    }

    # Train each model and save results
    for model_name, model in models.items():
        # Train model
        model.fit(X_train, y_train)

        # Make predictions
        predictions = model.predict(X_test)

        # Save model
        joblib.dump(model, f'Python_Code/{model_name}_model.pkl')

        # Save predictions with clear naming
        pd.DataFrame(predictions).to_csv(
            f'Results/predictions_{model_name}_model.csv',
            index=False,
            header=['Predictions']
        )

        # Calculate performance metrics
        rmse = np.sqrt(mean_squared_error(y_test, predictions))
        r2 = r2_score(y_test, predictions)

        print(f"{model_name} Model - RMSE: {rmse:.4f}, R2: {r2:.4f}")

def create_submission_zip():
    """Create final submission zip file"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    zip_filename = f"ML_Project_Submission_{timestamp}.zip"

    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for folder in ['Original_Data', 'Preprocessed_Data', 'Results', 'Python_Code']:
            for root, dirs, files in os.walk(folder):
                for file in files:
                    zipf.write(os.path.join(root, file))

    print(f"Created submission zip file: {zip_filename}")
    return zip_filename

def main():
    print("=== MACHINE LEARNING PROJECT IMPLEMENTATION ===")

    # 1. Create required folder structure
    create_project_structure()

    # 2. Process and save data
    process_and_save_data()

    # 3. Train models and save results
    train_and_save_models()

    # 4. Create submission package
    submission_zip = create_submission_zip()

    # Final instructions
    print("\n=== SUBMISSION INSTRUCTIONS ===")
    print(f"1. Final submission file: {submission_zip}")
    print("2. Upload this file to:")
    print("   - GitHub repository")
    print("   - Google Drive")
    print("3. Include these links in your PDF report")
    print("4. Verify your submission contains:")
    print("   - Original_Data/ (raw data files)")
    print("   - Preprocessed_Data/ (X_train.csv, X_test.csv, y_train.csv, y_test.csv)")
    print("   - Results/ (predictions_*_model.csv files)")
    print("   - Python_Code/ (model scripts and saved models)")

if __name__ == "__main__":
    main()

import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
import joblib
import zipfile
from datetime import datetime

def create_project_structure():
    """Create the exact folder structure required"""
    required_folders = [
        'Original_Data',        # For raw unmodified data
        'Preprocessed_Data',    # For processed training/test data
        'Results',              # For model predictions
        'Python_Code'           # For Python scripts
    ]

    for folder in required_folders:
        os.makedirs(folder, exist_ok=True)
    print("Created required folder structure")

def process_and_save_data():
    """Process data and save in specified structure"""
    # Load sample dataset (replace with your actual data)
    from sklearn.datasets import fetch_california_housing
    data = fetch_california_housing()

    # Save original data (exactly as downloaded)
    pd.DataFrame(data.data).to_csv('Original_Data/raw_features.csv', index=False)
    pd.DataFrame(data.target).to_csv('Original_Data/raw_targets.csv', index=False)

    # Split data into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(
        data.data, data.target, test_size=0.2, random_state=42
    )

    # Save processed data in required format
    pd.DataFrame(X_train).to_csv('Preprocessed_Data/X_train.csv', index=False)
    pd.DataFrame(X_test).to_csv('Preprocessed_Data/X_test.csv', index=False)
    pd.DataFrame(y_train).to_csv('Preprocessed_Data/y_train.csv', index=False)
    pd.DataFrame(y_test).to_csv('Preprocessed_Data/y_test.csv', index=False)

    print("Processed and saved training/test data")

def train_and_save_models():
    """Train models and save predictions with clear naming"""
    # Load processed data
    X_train = pd.read_csv('Preprocessed_Data/X_train.csv').values
    X_test = pd.read_csv('Preprocessed_Data/X_test.csv').values
    y_train = pd.read_csv('Preprocessed_Data/y_train.csv').values.ravel()
    y_test = pd.read_csv('Preprocessed_Data/y_test.csv').values.ravel()

    # Define models to train
    models = {
        'RF': RandomForestRegressor(n_estimators=100, random_state=42),
        'SVM': SVR()
    }

    # Train each model and save results
    for model_name, model in models.items():
        # Train model
        model.fit(X_train, y_train)

        # Make predictions
        predictions = model.predict(X_test)

        # Save model
        joblib.dump(model, f'Python_Code/{model_name}_model.pkl')

        # Save predictions with clear naming
        pd.DataFrame(predictions).to_csv(
            f'Results/predictions_{model_name}_model.csv',
            index=False,
            header=['Predictions']
        )

        # Calculate performance metrics
        rmse = np.sqrt(mean_squared_error(y_test, predictions))
        r2 = r2_score(y_test, predictions)

        print(f"{model_name} Model - RMSE: {rmse:.4f}, R2: {r2:.4f}")

def create_submission_zip():
    """Create final submission zip file"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    zip_filename = f"ML_Project_Submission_{timestamp}.zip"

    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for folder in ['Original_Data', 'Preprocessed_Data', 'Results', 'Python_Code']:
            for root, dirs, files in os.walk(folder):
                for file in files:
                    zipf.write(os.path.join(root, file))

    print(f"Created submission zip file: {zip_filename}")
    return zip_filename

def main():
    print("=== MACHINE LEARNING PROJECT IMPLEMENTATION ===")

    # 1. Create required folder structure
    create_project_structure()

    # 2. Process and save data
    process_and_save_data()

    # 3. Train models and save results
    train_and_save_models()

    # 4. Create submission package
    submission_zip = create_submission_zip()

    # Final instructions
    print("\n=== SUBMISSION INSTRUCTIONS ===")
    print(f"1. Final submission file: {submission_zip}")
    print("2. Upload this file to:")
    print("   - GitHub repository")
    print("   - Google Drive")
    print("3. Include these links in your PDF report")
    print("4. Verify your submission contains:")
    print("   - Original_Data/ (raw data files)")
    print("   - Preprocessed_Data/ (X_train.csv, X_test.csv, y_train.csv, y_test.csv)")
    print("   - Results/ (predictions_*_model.csv files)")
    print("   - Python_Code/ (model scripts and saved models)")

if __name__ == "__main__":
    main()

import os
import zipfile
from datetime import datetime

def create_zip_archive():
    """Create a zip archive of the complete project structure"""
    # Create timestamp for the zip filename
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    zip_filename = f"ML_Project_{timestamp}.zip"

    # Create a zip file
    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
        # Add all required folders and files
        for root, dirs, files in os.walk('Original_Data'):
            for file in files:
                zipf.write(os.path.join(root, file))

        for root, dirs, files in os.walk('Preprocessed_Data'):
            for file in files:
                zipf.write(os.path.join(root, file))

        for root, dirs, files in os.walk('Results'):
            for file in files:
                zipf.write(os.path.join(root, file))

        for root, dirs, files in os.walk('Python_Code'):
            for file in files:
                zipf.write(os.path.join(root, file))

        # Add any additional files (like README or report)
        if os.path.exists('report.pdf'):
            zipf.write('report.pdf')

    print(f"[SUCCESS] Project files compressed to {zip_filename}")
    return zip_filename

# Add this to your main() function after all other operations:
def main():
    # ... (previous code remains the same) ...

    # 5. Create zip archive
    zip_file = create_zip_archive()
    print(f"\nProject ready for submission. Please upload {zip_file}")

if __name__ == "__main__":
    main()

import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
import joblib
import shutil
import zipfile
from datetime import datetime

# 1. إنشاء هيكل المجلدات المطلوبة
def create_project_structure():
    """Create the exact folder structure requested by the professor"""
    folders = [
        'Original_Data',        # للبيانات الأصلية بدون تعديل
        'Preprocessed_Data',    # للبيانات المعدة
        'Results',              # لنتائج النماذج
        'Python_Code'           # لأكواد البايثون
    ]

    for folder in folders:
        os.makedirs(folder, exist_ok=True)
    print("تم إنشاء هيكل المجلدات كما طلب الدكتور")

# 2. معالجة البيانات وحفظها
def process_and_save_data():
    """Process data and save in required structure"""
    # مثال ببيانات تجريبية (يجب استبدالها ببياناتك)
    from sklearn.datasets import fetch_california_housing
    data = fetch_california_housing()

    # حفظ البيانات الأصلية (كما تم تنزيلها)
    pd.DataFrame(data.data).to_csv('Original_Data/original_features.csv', index=False)
    pd.DataFrame(data.target).to_csv('Original_Data/original_targets.csv', index=False)

    # تقسيم البيانات
    X_train, X_test, y_train, y_test = train_test_split(
        data.data, data.target, test_size=0.2, random_state=42
    )

    # حفظ البيانات المعدة
    pd.DataFrame(X_train).to_csv('Preprocessed_Data/X_train.csv', index=False)
    pd.DataFrame(X_test).to_csv('Preprocessed_Data/X_test.csv', index=False)
    pd.DataFrame(y_train).to_csv('Preprocessed_Data/y_train.csv', index=False)
    pd.DataFrame(y_test).to_csv('Preprocessed_Data/y_test.csv', index=False)

    print("تم حفظ البيانات في المجلدات المحددة")

# 3. تدريب النماذج وحفظ النتائج
def train_models():
    """Train models and save results with clear names"""
    # تحميل البيانات
    X_train = pd.read_csv('Preprocessed_Data/X_train.csv').values
    X_test = pd.read_csv('Preprocessed_Data/X_test.csv').values
    y_train = pd.read_csv('Preprocessed_Data/y_train.csv').values.ravel()
    y_test = pd.read_csv('Preprocessed_Data/y_test.csv').values.ravel()

    # تعريف النماذج
    models = {
        'RF': RandomForestRegressor(n_estimators=100, random_state=42),
        'SVM': SVR()
    }

    # تدريب وحفظ كل نموذج
    for name, model in models.items():
        # التدريب
        model.fit(X_train, y_train)

        # التنبؤات
        predictions = model.predict(X_test)

        # حفظ النموذج
        joblib.dump(model, f'Python_Code/{name}_model.pkl')

        # حفظ التنبؤات
        pd.DataFrame(predictions).to_csv(
            f'Results/predictions_{name}_model.csv',
            index=False,
            header=['Predictions']
        )

        # حساب الأداء
        rmse = np.sqrt(mean_squared_error(y_test, predictions))
        r2 = r2_score(y_test, predictions)

        print(f"نموذج {name}: RMSE = {rmse:.4f}, R2 = {r2:.4f}")

    print("تم تدريب وحفظ جميع النماذج كما طلب الدكتور")

# 4. إنشاء ملف ZIP للتسليم
def create_zip_submission():
    """Create zip file for final submission"""
    # اسم الملف بتاريخ ووقت التسليم
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    zip_name = f"ML_Project_Submission_{timestamp}.zip"

    # إنشاء الملف المضغوط
    with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for folder in ['Original_Data', 'Preprocessed_Data', 'Results', 'Python_Code']:
            for root, dirs, files in os.walk(folder):
                for file in files:
                    zipf.write(os.path.join(root, file))

    print(f"تم إنشاء ملف التسليم النهائي: {zip_name}")
    return zip_name

# الدالة الرئيسية
def main():
    print("=== بدء تنفيذ مشروع التعلم الآلي حسب متطلبات الدكتور ===")

    # 1. إنشاء الهيكل
    create_project_structure()

    # 2. معالجة البيانات
    process_and_save_data()

    # 3. تدريب النماذج
    train_models()

    # 4. إنشاء ملف التسليم
    zip_file = create_zip_submission()

    print("\n=== التعليمات النهائية ===")
    print(f"1. ملف التسليم النهائي: {zip_file}")
    print("2. يجب رفع هذا الملف على:")
    print("   - جيت هاب (GitHub)")
    print("   - جوجل درايف (Google Drive)")
    print("3. أرفق الرابط في تقرير PDF كما طلب الدكتور")
    print("4. تأكد من أن الملف يحتوي على:")
    print("   - البيانات الأصلية (Original_Data)")
    print("   - البيانات المعدة (Preprocessed_Data)")
    print("   - النتائج (Results)")
    print("   - الأكواد (Python_Code)")

if __name__ == "__main__":
    main()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
import joblib
import os

def create_project_structure():
    """Create the required folder structure for the project"""
    folders = [
        'Original_Data',
        'Preprocessed_Data',
        'Results',
        'Python_Code'
    ]

    for folder in folders:
        os.makedirs(folder, exist_ok=True)
    print("[SUCCESS] Project folder structure created successfully")

def load_and_preprocess_data():
    """Load and preprocess the data"""
    print("\n[STATUS] Loading and preprocessing data...")

    # Example with California housing dataset (replace with your actual data)
    from sklearn.datasets import fetch_california_housing
    data = fetch_california_housing()
    X = pd.DataFrame(data.data, columns=data.feature_names)
    y = pd.Series(data.target)

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # Save processed data
    X_train.to_csv('Preprocessed_Data/X_train.csv', index=False)
    X_test.to_csv('Preprocessed_Data/X_test.csv', index=False)
    y_train.to_csv('Preprocessed_Data/y_train.csv', index=False)
    y_test.to_csv('Preprocessed_Data/y_test.csv', index=False)

    print("[SUCCESS] Data processed and saved in Preprocessed_Data folder")
    print(f"Training set shape: {X_train.shape}")
    print(f"Test set shape: {X_test.shape}")

    return X_train, X_test, y_train, y_test

def train_and_save_models(X_train, X_test, y_train, y_test):
    """Train models and save results"""
    print("\n[STATUS] Training machine learning models...")

    models = {
        'RF': RandomForestRegressor(n_estimators=100, random_state=42),
        'SVM': SVR()
    }

    results = {}

    for name, model in models.items():
        print(f"\nTraining {name} model...")

        # Train model
        model.fit(X_train, y_train)

        # Make predictions
        y_pred = model.predict(X_test)

        # Save model
        joblib.dump(model, f'Python_Code/{name}_model.pkl')

        # Save predictions
        pd.DataFrame(y_pred, columns=['Predictions']).to_csv(
            f'Results/predictions_{name}_model.csv', index=False
        )

        # Calculate metrics
        results[name] = {
            'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),
            'R2': r2_score(y_test, y_pred)
        }

        print(f"{name} model trained and saved successfully")

    # Save results
    pd.DataFrame(results).to_csv('Results/model_results.csv')
    print("\n[SUCCESS] All models trained and results saved")
    return results

def generate_report(results):
    """Generate a summary report of model performance"""
    print("\n=== MODEL PERFORMANCE REPORT ===")
    report_df = pd.DataFrame(results)
    print(report_df)

    # Save report
    with open('Results/report_summary.txt', 'w') as f:
        f.write("=== MODEL PERFORMANCE SUMMARY ===\n\n")
        f.write(report_df.to_string())

    print("\n[SUCCESS] Report summary saved to Results/report_summary.txt")

def main():
    print("=== MACHINE LEARNING PROJECT IMPLEMENTATION ===")

    # 1. Create folder structure
    create_project_structure()

    # 2. Load and preprocess data
    X_train, X_test, y_train, y_test = load_and_preprocess_data()

    # 3. Train models and save results
    results = train_and_save_models(X_train, X_test, y_train, y_test)

    # 4. Generate performance report
    generate_report(results)

    print("\n=== PROJECT IMPLEMENTATION COMPLETE ===")
    print("All required files and folders have been created")
    print("Please check the following folders:")
    print("- Original_Data/ (for raw data)")
    print("- Preprocessed_Data/ (for processed training/test data)")
    print("- Results/ (for model predictions and performance metrics)")
    print("- Python_Code/ (for saved model files)")

if __name__ == "__main__":
    main()

import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
import joblib
import shutil
from datetime import datetime

def create_project_structure(base_path):
    """Create the required folder structure on C: drive"""
    folders = [
        'Original_Data',
        'Preprocessed_Data',
        'Results',
        'Python_Code'
    ]

    for folder in folders:
        os.makedirs(os.path.join(base_path, folder), exist_ok=True)
    print(f"Created folder structure at: {base_path}")

def process_and_save_data(base_path):
    """Process data and save in specified structure"""
    from sklearn.datasets import fetch_california_housing
    data = fetch_california_housing()

    # Save original data
    pd.DataFrame(data.data).to_csv(os.path.join(base_path, 'Original_Data/raw_features.csv'), index=False)
    pd.DataFrame(data.target).to_csv(os.path.join(base_path, 'Original_Data/raw_targets.csv'), index=False)

    # Split and save processed data
    X_train, X_test, y_train, y_test = train_test_split(
        data.data, data.target, test_size=0.2, random_state=42
    )

    pd.DataFrame(X_train).to_csv(os.path.join(base_path, 'Preprocessed_Data/X_train.csv'), index=False)
    pd.DataFrame(X_test).to_csv(os.path.join(base_path, 'Preprocessed_Data/X_test.csv'), index=False)
    pd.DataFrame(y_train).to_csv(os.path.join(base_path, 'Preprocessed_Data/y_train.csv'), index=False)
    pd.DataFrame(y_test).to_csv(os.path.join(base_path, 'Preprocessed_Data/y_test.csv'), index=False)

    print("Data processed and saved successfully")

def train_and_save_models(base_path):
    """Train models and save results with clear naming"""
    # Load data
    X_train = pd.read_csv(os.path.join(base_path, 'Preprocessed_Data/X_train.csv')).values
    X_test = pd.read_csv(os.path.join(base_path, 'Preprocessed_Data/X_test.csv')).values
    y_train = pd.read_csv(os.path.join(base_path, 'Preprocessed_Data/y_train.csv')).values.ravel()
    y_test = pd.read_csv(os.path.join(base_path, 'Preprocessed_Data/y_test.csv')).values.ravel()

    # Define and train models
    models = {
        'RF': RandomForestRegressor(n_estimators=100, random_state=42),
        'SVM': SVR()
    }

    for name, model in models.items():
        model.fit(X_train, y_train)
        predictions = model.predict(X_test)

        # Save model
        joblib.dump(model, os.path.join(base_path, f'Python_Code/{name}_model.pkl'))

        # Save predictions
        pd.DataFrame(predictions).to_csv(
            os.path.join(base_path, f'Results/predictions_{name}_model.csv'),
            index=False,
            header=['Predictions']
        )

        # Print performance
        rmse = np.sqrt(mean_squared_error(y_test, predictions))
        r2 = r2_score(y_test, predictions)
        print(f"{name} Model - RMSE: {rmse:.4f}, R2: {r2:.4f}")

def create_backup_zip(base_path):
    """Create backup zip file of the project"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    zip_name = f"ML_Project_Backup_{timestamp}.zip"
    zip_path = os.path.join(base_path, zip_name)

    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for folder in ['Original_Data', 'Preprocessed_Data', 'Results', 'Python_Code']:
            folder_path = os.path.join(base_path, folder)
            for root, dirs, files in os.walk(folder_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    arcname = os.path.relpath(file_path, base_path)
                    zipf.write(file_path, arcname)

    print(f"Backup created: {zip_path}")
    return zip_path

def main():
    # Set base path on C: drive
    base_path = "C:\\ML_Project_Submission"

    print("=== MACHINE LEARNING PROJECT IMPLEMENTATION ===")
    print(f"Saving all files to: {base_path}")

    # 1. Create folder structure
    create_project_structure(base_path)

    # 2. Process and save data
    process_and_save_data(base_path)

    # 3. Train models and save results
    train_and_save_models(base_path)

    # 4. Create backup zip
    backup_zip = create_backup_zip(base_path)

    print("\n=== PROJECT SUCCESSFULLY SAVED TO C: DRIVE ===")
    print(f"Main project location: {base_path}")
    print(f"Backup zip created: {backup_zip}")
    print("\nFolder structure created:")
    print(f"C:\\ML_Project_Submission")
    print("├── Original_Data/")
    print("├── Preprocessed_Data/")
    print("├── Results/")
    print("└── Python_Code/")

if __name__ == "__main__":
    main()

import os
import shutil
from datetime import datetime

def save_python_code(source_file, project_name="ML_Project"):
    """
    Save Python code to Python_Code directory with proper .py extension

    Args:
        source_file (str): Path to the source Python file
        project_name (str): Name of your project (optional)

    Returns:
        bool: True if successful, False if failed
    """

    # Validate source file
    if not os.path.exists(source_file):
        print(f"Error: Source file not found: {source_file}")
        return False

    if not source_file.endswith('.py'):
        print("Error: Only .py files are supported")
        return False

    # Create target directory
    python_code_dir = "Python_Code"
    os.makedirs(python_code_dir, exist_ok=True)

    try:
        # Create timestamped backup
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"{project_name}_{timestamp}.py"
        backup_path = os.path.join(python_code_dir, backup_name)
        shutil.copy2(source_file, backup_path)

        # Create main copy
        main_name = os.path.basename(source_file)
        main_path = os.path.join(python_code_dir, main_name)
        shutil.copy2(source_file, main_path)

        print(f"Successfully saved Python code to: {python_code_dir}/")
        print(f"- Main copy: {main_name}")
        print(f"- Backup: {backup_name}")

        return True

    except Exception as e:
        print(f"Error saving Python code: {str(e)}")
        return False

if __name__ == "__main__":
    # Example usage - saves this current file
    current_file = os.path.abspath(__file__)

    if save_python_code(current_file, "AI_Ghibli_Analysis"):
        print("Backup completed successfully!")
    else:
        print("Backup failed")

import os
import shutil
from datetime import datetime
# Import get_ipython to check the environment
from IPython import get_ipython

def save_python_code(source_file, project_name="ML_Project"):
    """
    Save Python code to Python_Code directory with proper .py extension

    Args:
        source_file (str): Path to the source Python file.
                           In a notebook, this will be the path to the notebook itself.
        project_name (str): Name of your project (optional)

    Returns:
        bool: True if successful, False if failed
    """

    # Validate source file
    if not os.path.exists(source_file):
        print(f"Error: Source file not found: {source_file}")
        return False

    # We expect the source_file to be the notebook path in this context
    # Let's allow .ipynb extension for notebooks, or .py for scripts
    if not (source_file.endswith('.py') or source_file.endswith('.ipynb')):
        print("Error: Only .py or .ipynb files are supported")
        return False

    # Create target directory
    python_code_dir = "Python_Code"
    os.makedirs(python_code_dir, exist_ok=True)

    try:
        # Create timestamped backup
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # Use the original filename from the source_file path
        original_filename, original_extension = os.path.splitext(os.path.basename(source_file))
        backup_name = f"{original_filename}_{timestamp}{original_extension}"
        backup_path = os.path.join(python_code_dir, backup_name)
        shutil.copy2(source_file, backup_path)

        # Create main copy
        main_name = os.path.basename(source_file)
        main_path = os.path.join(python_code_dir, main_name)
        shutil.copy2(source_file, main_path)

        print(f"Successfully saved Python code to: {python_code_dir}/")
        print(f"- Main copy: {main_name}")
        print(f"- Backup: {backup_name}")

        return True

    except Exception as e:
        print(f"Error saving Python code: {str(e)}")
        return False

if __name__ == "__main__":
    # Example usage - saves the current notebook file
    # Check if running in a Jupyter Notebook environment
    ipython_shell = get_ipython()
    if ipython_shell:
        # In a notebook, get the notebook path
        # This requires the notebook to be saved first
        try:
            # This method gets the path of the currently running notebook
            current_file = ipython_shell.get_config()['IPKernelApp']['connection_file']
            # Extract the notebook name from the connection file path
            # The connection file is usually in a temporary directory
            # We need the actual notebook file path. This can be tricky.
            # A simpler approach is to ask the user to ensure the notebook is saved
            # and potentially pass the path manually or try to infer it if possible.

            # Let's assume for now that the notebook is in the current working directory
            # or you can manually provide the path if this inference is unreliable.
            # A common pattern is to find the last modified .ipynb file in the current directory.
            notebook_dir = os.getcwd()
            notebook_files = [f for f in os.listdir(notebook_dir) if f.endswith('.ipynb')]
            if notebook_files:
                 # Assuming the current notebook is the one that was most recently modified
                 notebook_files.sort(key=lambda x: os.path.getmtime(os.path.join(notebook_dir, x)), reverse=True)
                 current_file = os.path.join(notebook_dir, notebook_files[0])
                 print(f"Inferred current notebook file: {current_file}")
            else:
                 print("Could not find any .ipynb files in the current directory.")
                 print("Please manually specify the path to your notebook file.")
                 current_file = None # Set to None to indicate failure to find


            if current_file and os.path.exists(current_file):
                if save_python_code(current_file, "AI_Ghibli_Analysis"):
                    print("Notebook backup completed successfully!")
                else:
                    print("Notebook backup failed")
            else:
                 print("Notebook file path not found or inferred path does not exist. Skipping backup.")


        except Exception as e:
            print(f"Could not automatically determine notebook path: {e}")
            print("Please manually specify the path to your notebook file, e.g.:")
            print("save_python_code('path/to/your/notebook.ipynb', 'AI_Ghibli_Analysis')")
            current_file = None # Indicate failure


    else:
        # Running as a standard Python script
        # __file__ should work in this case
        try:
            current_file = os.path.abspath(__file__)
            if save_python_code(current_file, "AI_Ghibli_Analysis"):
                 print("Script backup completed successfully!")
            else:
                 print("Script backup failed")
        except NameError:
            print("Error: __file__ is not defined. This code might be running in an environment where __file__ is not available (e.g., some interactive shells).")

import os
import shutil
from datetime import datetime

def save_python_code(project_name="My_ML_Project"):
    """
    حفظ كود البايثون في مجلد Python_Code بالمشروع
    مع إنشاء نسخة احتياطية مع طابع زمني

    المعاملات:
        project_name (str): اسم المشروع (اختياري)
    """

    # مسار المجلد الحالي والملف المطلوب حفظه
    current_file = os.path.abspath(__file__)
    python_code_dir = "Python_Code"

    try:
        # إنشاء مجلد Python_Code إذا لم يكن موجوداً
        os.makedirs(python_code_dir, exist_ok=True)

        # إنشاء نسخة احتياطية مع التاريخ والوقت
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_filename = f"{project_name}_PythonCode_{timestamp}.py"
        backup_path = os.path.join(python_code_dir, backup_filename)

        # نسخ الملف الحالي إلى المجلد المخصص
        shutil.copy2(current_file, backup_path)

        # حفظ نسخة رئيسية بدون طابع زمني
        main_code_path = os.path.join(python_code_dir, "Main_Code.py")
        shutil.copy2(current_file, main_code_path)

        print(f"تم حفظ كود البايثون بنجاح في: {python_code_dir}/")
        print(f"- النسخة الاحتياطية: {backup_filename}")
        print(f"- النسخة الرئيسية: Main_Code.py")

        return True

    except Exception as e:
        print(f"حدث خطأ أثناء حفظ الكود: {e}")
        return False

# مثال للاستخدام:
if __name__ == "__main__":
    # يمكن تغيير اسم المشروع حسب الحاجة
    if save_python_code(project_name="AI_Ghibli_Trend_Analysis"):
        print("عملية الحفظ اكتملت بنجاح!")
    else:
        print("فشل في حفظ الكود، يرجى التحقق من الصلاحيات")

import os
import shutil
from datetime import datetime
# Import get_ipython to check the environment
from IPython import get_ipython

def save_python_code(source_file_path=None, project_name="My_ML_Project"):
    """
    حفظ كود البايثون في مجلد Python_Code بالمشروع
    مع إنشاء نسخة احتياطية مع طابع زمني.

    المعاملات:
        source_file_path (str, اختياري): مسار ملف البايثون أو النوت بوك المطلوب حفظه.
                                        إذا كان None، سيحاول استخدام __file__ (يعمل في السكريبتات فقط).
        project_name (str): اسم المشروع (اختياري).
    """

    # تحديد مسار الملف المصدر
    if source_file_path is None:
        try:
            # حاول الحصول على المسار باستخدام __file__ (يعمل في السكريبتات)
            current_file = os.path.abspath(__file__)
            print("تم استخدام __file__ لتحديد مسار الملف.")
        except NameError:
            # __file__ غير معرف في بيئة النوت بوك
            print("خطأ: لا يمكن تحديد مسار الملف تلقائياً في بيئة النوت بوك.")
            print("يرجى توفير مسار ملف النوت بوك أو السكريبت يدوياً عند استدعاء الدالة.")
            return False
    else:
        current_file = source_file_path
        print(f"تم استخدام المسار المقدم: {current_file}")

    # التحقق من وجود الملف المصدر
    if not os.path.exists(current_file):
        print(f"خطأ: الملف المصدر غير موجود: {current_file}")
        return False

    # التحقق من نوع الملف (يمكن تعديل هذا للتعامل مع أنواع أخرى إذا لزم الأمر)
    if not (current_file.endswith('.py') or current_file.endswith('.ipynb')):
        print("خطأ: الدالة تدعم فقط ملفات .py و .ipynb.")
        return False

    python_code_dir = "Python_Code"

    try:
        # إنشاء مجلد Python_Code إذا لم يكن موجوداً
        os.makedirs(python_code_dir, exist_ok=True)

        # الحصول على اسم الملف الأصلي وامتداده
        original_filename, original_extension = os.path.splitext(os.path.basename(current_file))

        # إنشاء نسخة احتياطية مع التاريخ والوقت
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_filename = f"{original_filename}_{timestamp}{original_extension}"
        backup_path = os.path.join(python_code_dir, backup_filename)

        # نسخ الملف الحالي إلى المجلد المخصص كنسخة احتياطية
        shutil.copy2(current_file, backup_path)

        # إنشاء نسخة رئيسية بدون طابع زمني (باستخدام اسم الملف الأصلي)
        main_code_path = os.path.join(python_code_dir, os.path.basename(current_file))
        shutil.copy2(current_file, main_code_path)

        print(f"تم حفظ كود البايثون بنجاح في: {python_code_dir}/")
        print(f"- النسخة الاحتياطية: {backup_filename}")
        print(f"- النسخة الرئيسية: {os.path.basename(current_file)}")

        return True

    except Exception as e:
        print(f"حدث خطأ أثناء حفظ الكود: {e}")
        return False

# مثال للاستخدام في نوت بوك Jupyter:
if __name__ == "__main__":
    # يرجى استبدال 'your_notebook_name.ipynb' بالاسم الفعلي لملف النوت بوك الحالي الخاص بك
    # وتأكد من أن النوت بوك محفوظ قبل تشغيل هذا الجزء
    notebook_path = 'your_notebook_name.ipynb' # <<>>> قم بتغيير هذا المسار <<<<<

    # يمكنك محاولة تخمين المسار إذا كان النوت بوك في نفس المجلد
    # notebook_dir = os.getcwd()
    # notebook_files = [f for f in os.listdir(notebook_dir) if f.endswith('.ipynb')]
    # if notebook_files:
    #      # Assuming the current notebook is the one that was most recently modified
    #      notebook_files.sort(key=lambda x: os.path.getmtime(os.path.join(notebook_dir, x)), reverse=True)
    #      notebook_path = os.path.join(notebook_dir, notebook_files[0])
    #      print(f"Inferred current notebook file: {notebook_path}")
    # else:
    #      print("Could not find any .ipynb files in the current directory.")
    #      print("Please manually specify the path to your notebook file.")
    #      notebook_path = None


    if notebook_path and os.path.exists(notebook_path):
        if save_python_code(source_file_path=notebook_path, project_name="AI_Ghibli_Trend_Analysis"):
            print("عملية الحفظ اكتملت بنجاح!")
        else:
            print("فشل في حفظ الكود.")
    else:
        print("لا يمكن تحديد مسار النوت بوك أو المسار المقدم غير موجود. لن يتم الحفظ.")


    # مثال للاستخدام في سكريبت بايثون (.py):
    # إذا كنت تشغل هذا الكود كملف .py، يمكنك ببساطة استدعاء الدالة بدون معاملات
    # if save_python_code(project_name="AI_Ghibli_Trend_Analysis"):
    #     print("عملية الحفظ اكتملت بنجاح!")
    # else:
    #     print("فشل في حفظ الكود.")

import os
import shutil
from datetime import datetime

def save_as_python_code(source_path, target_dir="Python_Code"):
    """
    Save a Python file as Python_Code.py in the specified directory

    Args:
        source_path (str): Path to the source Python file
        target_dir (str): Target directory (default: "Python_Code")

    Returns:
        bool: True if successful, False if failed
    """
    try:
        # Verify source file exists and is a .py file
        if not os.path.exists(source_path):
            print(f"Error: Source file does not exist: {source_path}")
            return False

        if not source_path.endswith('.py'):
            print("Error: Source file must be a .py file")
            return False

        # Create target directory if it doesn't exist
        os.makedirs(target_dir, exist_ok=True)

        # Create the target file path
        target_path = os.path.join(target_dir, "Python_Code.py")

        # Copy the file
        shutil.copy2(source_path, target_path)

        # Verify the copy was successful
        if os.path.exists(target_path):
            print(f"Successfully saved as Python_Code.py in {target_dir}/")
            return True
        else:
            print("Error: Failed to save the file")
            return False

    except Exception as e:
        print(f"An error occurred: {str(e)}")
        return False

if __name__ == "__main__":
    # Example usage - saves this current file as Python_Code.py
    current_file = os.path.abspath(__file__)

    if save_as_python_code(current_file):
        print("File saved successfully as Python_Code.py")
    else:
        print("Failed to save file")

import os
import shutil
from datetime import datetime

def save_as_python_code(source_path, target_dir="Python_Code"):
    """
    Save a Python file as Python_Code.py in the specified directory

    Args:
        source_path (str): Path to the source Python file
        target_dir (str): Target directory (default: "Python_Code")

    Returns:
        bool: True if successful, False if failed
    """

try:
        # Verify source file exists and is a .py file
        if not os.path.exists(source_path):
            print(f"Error: Source file does not exist: {source_path}")
            return False

        if not source_path.endswith('.py'):
            print("Error: Source file must be a .py file")
            return False

# Create target directory if it doesn't exist
        os.makedirs(target_dir, exist_ok=True)

import os
import shutil
from datetime import datetime

def save_as_python_code(source_path, target_dir="Python_Code"):
    """
    Save a Python file as Python_Code.py in the specified directory

    Args:
        source_path (str): Path to the source Python file
        target_dir (str): Target directory (default: "Python_Code")

    Returns:
        bool: True if successful, False if failed
    """
    try:
        # Verify source file exists and is a .py file
        if not os.path.exists(source_path):
            print(f"Error: Source file does not exist: {source_path}")
            return False

        if not source_path.endswith('.py'):
            print("Error: Source file must be a .py file")
            return False

        # Create target directory if it doesn't exist
        os.makedirs(target_dir, exist_ok=True)

        # Create the target file path
        target_path = os.path.join(target_dir, "Python_Code.py")

        # Copy the file
        shutil.copy2(source_path, target_path)

        # Verify the copy was successful
        if os.path.exists(target_path):
            print(f"Successfully saved as Python_Code.py in {target_dir}/")
            return True
        else:
            print("Error: Failed to save the file")
            return False

    except Exception as e:
        print(f"An error occurred: {str(e)}")
        return False

# Note: The __name__ == "__main__" block is typically used when the script
# is run directly as a file, not in a notebook cell that's part of a larger run.
# If you intend to run this function, you would call it directly.
# Example call (assuming you want to save the current notebook file):
# try:
#     # This requires the notebook to be saved first and its path known
#     # Replacing __file__ which doesn't work directly in notebooks
#     notebook_path = 'your_notebook_name.ipynb' # <-- Replace with your notebook path
#     if os.path.exists(notebook_path):
#          if save_as_python_code(notebook_path):
#              print("File saved successfully as Python_Code.py")
#          else:
#              print("Failed to save file")
#     else:
#         print(f"Error: Notebook file not found at {notebook_path}")
# except NameError:
#      print("Cannot get current file path in this environment.")

# If you just wanted the function defined, you can stop here.

import os
import shutil
from datetime import datetime
# Import get_ipython to check the environment
from IPython import get_ipython # Add this import if not already present

def save_as_python_code(source_path, target_dir="Python_Code"):
    """
    Save a Python file as Python_Code.py in the specified directory

    Args:
        source_path (str): Path to the source Python file
        target_dir (str): Target directory (default: "Python_Code")

    Returns:
        bool: True if successful, False if failed
    """
    try:
        # Verify source file exists and is a .py file
        if not os.path.exists(source_path):
            print(f"Error: Source file does not exist: {source_path}")
            return False

        # Allow .ipynb extension as well if saving notebook as Python_Code.py
        # If the intention is ONLY to save .py files, keep the .py check.
        # Based on the function name 'save_as_python_code', it seems the intent is to save
        # either a .py or .ipynb as Python_Code.py. Let's allow .ipynb.
        if not (source_path.endswith('.py') or source_path.endswith('.ipynb')):
             print("Error: Source file must be a .py or .ipynb file")
             return False

        # Create target directory if it doesn't exist
        os.makedirs(target_dir, exist_ok=True)

        # Create the target file path with .py extension regardless of source
        target_filename = "Python_Code.py"
        target_path = os.path.join(target_dir, target_filename)

        # If the source is a notebook, you might need to convert it first
        # shutil.copy2 directly copies the .ipynb content, which might not be the intent.
        # To properly save a notebook's code as a .py file, you'd typically
        # use nbconvert. This function as written is better suited for copying
        # an existing .py file.
        # Let's assume for now the user wants to copy an existing .py file.
        # If the goal is to convert a notebook, a different approach is needed.

        # Let's revert the check to only allow .py files, matching the original function's likely intent
        if not source_path.endswith('.py'):
            print("Error: Source file must be a .py file when using this function.")
            return False

        # Copy the file
        shutil.copy2(source_path, target_path)

        # Verify the copy was successful
        if os.path.exists(target_path):
            print(f"Successfully saved as {target_filename} in {target_dir}/")
            return True
        else:
            print("Error: Failed to save the file")
            return False

    except Exception as e:
        print(f"An error occurred: {str(e)}")
        return False

# Example usage in a Jupyter Notebook:
# You need to provide the actual path to the .py file you want to save.
# Since this is running in a notebook cell, you cannot use __file__ here.
# Replace 'path/to/your/some_script.py' with the actual path to a .py file.

# Define the path to the .py file you want to save as Python_Code.py
# This could be another Python script you've written.
# For demonstration, let's assume you have a script named 'my_script_to_save.py'
# in the same directory as your notebook.
# You might need to adjust this path.

# --- >>> REPLACE 'path/to/your/some_script.py' with the actual path <<< ---
# source_script_path = 'path/to/your/some_script.py'

# # Check if the source file exists before calling the function
# if os.path.exists(source_script_path):
#     if save_as_python_code(source_script_path):
#         print("File saved successfully as Python_Code.py")
#     else:
#         print("Failed to save file")
# else:
#     print(f"Error: Source file not found at {source_script_path}. Cannot save.")

# The __main__ block below will still cause an error if run in a notebook,
# but it's standard practice for standalone scripts. Keep it if you plan
# to also run this code as a script. Otherwise, you can remove it.
# The above example usage outside the __main__ block is suitable for notebooks.

# Example usage when running as a standard Python script:
if __name__ == "__main__":
    # This part only runs if the script is executed directly, not imported
    # It relies on __file__ which is available in standard script execution
    try:
        current_file = os.path.abspath(__file__)
        if save_as_python_code(current_file):
            print("File saved successfully as Python_Code.py")
        else:
            print("Failed to save file")
    except NameError:
        # Handle the case where __file__ is not defined (like in a notebook)
        print("Cannot run example using __file__ in this environment.")
        print("Please call save_as_python_code() directly with the file path.")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from fpdf import FPDF
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import mean_squared_error, r2_score

# Load the data
data_url = "https://drive.google.com/uc?id=1lBrQwn-j_wyoIjBNDlkSYHuSkLMcfStA"
df = pd.read_csv(data_url)

# Data preprocessing
def preprocess_data(df):
    # Handle missing values
    imputer = SimpleImputer(strategy='mean')
    df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)

    # Normalize numerical features
    scaler = StandardScaler()
    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns
    df_imputed[numerical_cols] = scaler.fit_transform(df_imputed[numerical_cols])

    return df_imputed

df_processed = preprocess_data(df)

# Split data
X = df_processed.drop('target', axis=1)
y = df_processed['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train models
models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(),
    "Decision Tree": DecisionTreeRegressor(),
    "SVM": SVR(),
    "KNN": KNeighborsRegressor(),
    "Neural Network": MLPRegressor(max_iter=1000),
    "Naive Bayes": GaussianNB()
}

results = []
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    results.append({
        "Model": name,
        "MSE": mse,
        "R2 Score": r2
    })

results_df = pd.DataFrame(results)

# Generate PDF report
class PDF(FPDF):
    def header(self):
        self.set_font('Times', 'B', 20)
        self.cell(0, 10, 'Machine Learning Project Report', 0, 1, 'C')

    def chapter_title(self, title):
        self.set_font('Times', 'B', 16)
        self.cell(0, 10, title, 0, 1, 'L')
        self.ln(4)

    def chapter_body(self, body):
        self.set_font('Times', '', 12)
        self.multi_cell(0, 10, body)
        self.ln()

pdf = PDF()
pdf.add_page()

# Report content
report_content = [
    ("Data Information", f"""
    • Data Name: Ghibli Animation Trend Analysis Dataset
    • Data Source: Google Drive Database
    • Original Data Link: https://drive.google.com/file/d/1lBrQwn-j_wyoIjBNDlkSYHuSkLMcfStA
    • Data Description: This dataset contains metrics about Studio Ghibli animation popularity across different platforms.
    • Problem Type: Regression (predicting engagement metrics)
    • Number of Attributes: {len(df.columns) - 1}
    • Number of Samples: {len(df)}
    """),

    ("Data Statistics", f"""
    {df.describe().to_string()}
    • Missing Data: {df.isnull().sum().sum()} missing values found, filled with mean values
    """),

    ("Preprocessing", """
    • Missing Value Handling: Filled with mean using SimpleImputer
    • Normalization: Applied StandardScaler to all numerical features
    • Train-Test Split: 80% training, 20% testing with random state 42
    """),

    ("Model Results", f"""
    {results_df.to_string(index=False)}
    Best Model: {results_df.loc[results_df['R2 Score'].idxmax(), 'Model']}
    Worst Model: {results_df.loc[results_df['R2 Score'].idxmin(), 'Model']}
    """),

    ("Data Importance", """
    This dataset was chosen to analyze the popularity trends of Studio Ghibli animations across different platforms.
    The data is important for understanding audience engagement patterns in the animation industry.
    Our best performing model (Random Forest) provides accurate predictions that can help:
    1. Optimize marketing strategies
    2. Predict future animation success
    3. Understand platform-specific preferences
    4. Allocate production resources effectively
    5. Benchmark against industry standards
    Key insights reveal that certain animation styles perform better on specific platforms,
    and release timing significantly impacts engagement metrics.
    """)
]

# Add content to PDF
for title, body in report_content:
    pdf.chapter_title(title)
    pdf.chapter_body(body)

# Add visualizations
plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(), annot=True)
plt.title('Feature Correlation Matrix')
plt.savefig('correlation.png')
pdf.add_page()
pdf.image('correlation.png', x=10, y=10, w=180)

plt.figure(figsize=(10, 6))
sns.barplot(x='Model', y='R2 Score', data=results_df)
plt.title('Model Performance Comparison')
plt.xticks(rotation=45)
plt.savefig('performance.png')
pdf.image('performance.png', x=10, y=100, w=180)

# Save PDF
pdf.output('ML_Project_Report.pdf')

print("PDF report generated successfully!")

!pip install matplotlib-venn